{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JosephThompson607/dir_vae/blob/main/sepsis_vae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "phRvmO49rUUT"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we prepare the data for training"
      ],
      "metadata": {
        "id": "0SiJSH5EMXTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: Read this from the cloud\n",
        "patients = pd.read_csv(\"/content/unique_patient_dem.csv\")\n",
        "\n",
        "patients.drop(columns=['subject_id'], inplace=True)\n",
        "numeric_cols = patients.select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_cols = patients.select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "# Reorder DataFrame\n",
        "patients = patients[numeric_cols + categorical_cols]\n",
        "#1 hot encoding\n",
        "df_encoded = pd.get_dummies(patients, columns=categorical_cols)\n",
        "\n",
        "#If cuda is available, device is cuda, otherwise cpu\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "scaler = StandardScaler()\n",
        "#Scaling numeric columns\n",
        "df_encoded[numeric_cols] = scaler.fit_transform(df_encoded[numeric_cols])\n",
        "features = df_encoded.astype('float32').values\n",
        "# print(features.columns)\n",
        "# print(features.dtypes)\n",
        "# Get indices for slicing\n",
        "num_indices = list(range(len(numeric_cols)))\n",
        "n_numeric = len(numeric_cols)\n",
        "cat_indices = list(range(len(numeric_cols), len(features)))\n",
        "tensor = torch.tensor(features, dtype=torch.float32)\n",
        "\n",
        "X_train, X_test = train_test_split(tensor, test_size=0.2, random_state=42)\n",
        "\n",
        "train_dataset = TensorDataset(X_train)  # or (X_train, y_train)\n",
        "test_dataset = TensorDataset(X_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "input_size = X_train[0].shape[0] #input size is the number of features going into the network\n",
        "print(input_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4j5hmBSfrZhl",
        "outputId": "707f8005-2bdd-42c3-b977-f186582266c6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we define the model and related functions"
      ],
      "metadata": {
        "id": "HeWHLkFqMPmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ngf = 64\n",
        "ndf = 64\n",
        "nc = 1\n",
        "\n",
        "def prior(K, alpha):\n",
        "    \"\"\"\n",
        "    Prior for the model.\n",
        "    :K: number of categories\n",
        "    :alpha: Hyper param of Dir\n",
        "    :return: mean and variance tensors\n",
        "    \"\"\"\n",
        "    # Approximate to normal distribution using Laplace approximation\n",
        "    a = torch.Tensor(1, K).float().fill_(alpha)\n",
        "    mean = a.log().t() - a.log().mean(1)\n",
        "    var = ((1 - 2.0 / K) * a.reciprocal()).t() + (1.0 / K ** 2) * a.reciprocal().sum(1)\n",
        "    return mean.t(), var.t() # Parameters of prior distribution after approximation\n",
        "\n",
        "class Dir_VAE(nn.Module):\n",
        "    def __init__(self, input_size,n_numeric, latent_size=10,\n",
        "                 hidden_dim = 200, num_weight=1.0, cat_weight=1.0, kld_weight=1.0):\n",
        "        self.num_weight = num_weight\n",
        "        self.cat_weight = cat_weight\n",
        "        self.kld_weight = kld_weight\n",
        "        self.num_numeric_cols = n_numeric\n",
        "        self.latent_size = latent_size\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.input_size = input_size\n",
        "        super(Dir_VAE, self).__init__()\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "          nn.Linear(self.input_size, self.hidden_dim),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(self.hidden_dim, self.hidden_dim),\n",
        "          nn.ReLU(),\n",
        "          # nn.Linear(self.hidden_dim, self.hidden_dim),\n",
        "          # nn.ReLU()\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "          nn.Linear(self.latent_size, self.hidden_dim),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(self.hidden_dim, self.hidden_dim),\n",
        "          nn.ReLU(),\n",
        "          # nn.Linear(self.hidden_dim, self.hidden_dim),\n",
        "          # nn.ReLU(),\n",
        "          nn.Linear(self.hidden_dim, self.hidden_dim),\n",
        "          nn.ReLU(),\n",
        "          # nn.Unflatten(dim=1, unflattened_size=(1, 28, 28)) # This was for image data\n",
        "        )\n",
        "        #self.fc1 = nn.Linear(self.hidden_dim, 512)\n",
        "        self.fc21 = nn.Linear(self.hidden_dim, self.latent_size)\n",
        "        self.fc22 = nn.Linear(self.hidden_dim, self.latent_size)\n",
        "\n",
        "        self.fc3 = nn.Linear(self.hidden_dim, self.input_size)#last layer\n",
        "        #self.fc4 = nn.Linear(512, self.hidden_dim)\n",
        "\n",
        "        self.lrelu = nn.LeakyReLU()\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Dir prior\n",
        "        self.prior_mean, self.prior_var = map(nn.Parameter, prior(self.latent_size, 0.3)) # 0.3 is a hyper param of Dirichlet distribution\n",
        "        self.prior_logvar = nn.Parameter(self.prior_var.log())\n",
        "        self.prior_mean.requires_grad = False\n",
        "        self.prior_var.requires_grad = False\n",
        "        self.prior_logvar.requires_grad = False\n",
        "\n",
        "\n",
        "    def encode(self, x):\n",
        "        encoding = self.encoder(x);\n",
        "        #h1 = self.fc1(encoding)\n",
        "        return self.fc21(encoding), self.fc22(encoding)\n",
        "\n",
        "    def decode(self, gauss_z):\n",
        "        dir_z = F.softmax(gauss_z,dim=1) #Reduntant, already done in forward\n",
        "        # This variable (z) can be treated as a variable that follows a Dirichlet distribution (a variable that can be interpreted as a probability that the sum is 1)\n",
        "        # Use the Softmax function to satisfy the simplex constraint\n",
        "        x_out = self.fc3(self.decoder(dir_z))\n",
        "\n",
        "        # Apply sigmoid to categorical output only\n",
        "        x_out[:, self.num_numeric_cols:] = torch.sigmoid(x_out[:, self.num_numeric_cols:])\n",
        "        return x_out\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps*std\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        gauss_z = self.reparameterize(mu, logvar)\n",
        "        # gause_z is a variable that follows a multivariate normal distribution\n",
        "        # Inputting gause_z into softmax func yields a random variable that follows a Dirichlet distribution (Softmax func are used in decoder)\n",
        "        dir_z = F.softmax(gauss_z,dim=1) # This variable follows a Dirichlet distribution\n",
        "        return self.decode(gauss_z), mu, logvar, gauss_z, dir_z\n",
        "\n",
        "    def reconstruction_loss(self, x_true, x_recon):\n",
        "        # Slice the tensors\n",
        "        x_true_num = x_true[:, :self.num_numeric_cols]\n",
        "        x_true_cat = x_true[:, self.num_numeric_cols:]\n",
        "\n",
        "        x_recon_num = x_recon[:, :self.num_numeric_cols]\n",
        "        x_recon_cat = x_recon[:, self.num_numeric_cols:]\n",
        "\n",
        "        # Compute losses\n",
        "        num_loss = F.mse_loss(x_recon_num, x_true_num)\n",
        "        cat_loss = F.cross_entropy(x_recon_cat, x_true_cat)\n",
        "        #putting weights to device\n",
        "        num_weight = torch.tensor(self.num_weight, device = x_recon.device)\n",
        "        cat_weight = torch.tensor(self.cat_weight, device =x_recon.device )\n",
        "        return num_weight *num_loss + cat_weight *cat_loss\n",
        "\n",
        "    # Reconstruction + KL divergence losses summed over all elements and batch\n",
        "    def loss_function(self, recon_x, x, mu, logvar):\n",
        "        # Apply sigmoid to the input data x to ensure values are between 0 and 1\n",
        "        recon_loss = self.reconstruction_loss(x, recon_x, )\n",
        "        # ディリクレ事前分布と変分事後分布とのKLを計算\n",
        "        # Calculating KL with Dirichlet prior and variational posterior distributions\n",
        "        # Original paper:\"Autoencodeing variational inference for topic model\"-https://arxiv.org/pdf/1703.01488\n",
        "        prior_mean = self.prior_mean.expand_as(mu)\n",
        "        prior_var = self.prior_var.expand_as(logvar)\n",
        "        prior_logvar = self.prior_logvar.expand_as(logvar)\n",
        "        var_division = logvar.exp() / prior_var # Σ_0 / Σ_1\n",
        "        diff = mu - prior_mean # μ_１ - μ_0\n",
        "        diff_term = diff *diff / prior_var # (μ_1 - μ_0)(μ_1 - μ_0)/Σ_1\n",
        "        logvar_division = prior_logvar - logvar # log|Σ_1| - log|Σ_0| = log(|Σ_1|/|Σ_2|)\n",
        "        # KLD\n",
        "        kld_weight = torch.tensor(self.kld_weight, device = recon_x.device)\n",
        "        KLD = 0.5 * ((var_division + diff_term + logvar_division).sum(1) - self.latent_size) * kld_weight\n",
        "        self.last_KLD = torch.mean(KLD) #Used for reporting\n",
        "        self.last_BCE = recon_loss\n",
        "        return recon_loss + KLD"
      ],
      "metadata": {
        "id": "jFoNCRlVMNud"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below are the training and test loops\n"
      ],
      "metadata": {
        "id": "EQ-82vOxMqkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = Dir_VAE(input_size, n_numeric, latent_size=2, hidden_dim=20).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data,) in enumerate(train_loader): # Unpack only one element\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar, gauss_z, dir_z = model(data)\n",
        "\n",
        "        loss = model.loss_function(recon_batch, data, mu, logvar, )\n",
        "        loss = loss.mean()\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 10 == 0:\n",
        "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader)}%)] \\\n",
        "            Loss:{loss.item() / len(data)}\\\n",
        "            R_loss {model.last_BCE}, KLD_loss {model.last_KLD}')\n",
        "\n",
        "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
        "          epoch, train_loss / len(train_loader.dataset)))\n",
        "\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for i, (data,) in enumerate(test_loader): # Unpack only one element\n",
        "            data = data.to(device)\n",
        "            recon_batch, mu, logvar, gauss_z, dir_z = model(data)\n",
        "            loss = model.loss_function(recon_batch, data, mu, logvar)\n",
        "            test_loss += loss.mean()\n",
        "            test_loss.item()\n",
        "\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('====> Test set loss: {:.4f}'.format(test_loss))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    for epoch in range(1, 10 + 1):\n",
        "        train(epoch)\n",
        "        test(epoch)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgOw9CXb2p_l",
        "outputId": "6e41944a-4977-45a5-aad9-a8ef06f56790"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/14166 (0.0%)]             Loss:0.2699800133705139            R_loss 8.454643249511719, KLD_loss 0.18471680581569672\n",
            "Train Epoch: 1 [320/14166 (2.2573363431151243%)]             Loss:0.2574441432952881            R_loss 8.076220512390137, KLD_loss 0.1619911789894104\n",
            "Train Epoch: 1 [640/14166 (4.514672686230249%)]             Loss:0.25799787044525146            R_loss 8.111760139465332, KLD_loss 0.1441720426082611\n",
            "Train Epoch: 1 [960/14166 (6.772009029345372%)]             Loss:0.2566404342651367            R_loss 8.088797569274902, KLD_loss 0.12369564175605774\n",
            "Train Epoch: 1 [1280/14166 (9.029345372460497%)]             Loss:0.25545191764831543            R_loss 8.068413734436035, KLD_loss 0.10604721307754517\n",
            "Train Epoch: 1 [1600/14166 (11.286681715575622%)]             Loss:0.25564733147621155            R_loss 8.092647552490234, KLD_loss 0.08806723356246948\n",
            "Train Epoch: 1 [1920/14166 (13.544018058690744%)]             Loss:0.2458692044019699            R_loss 7.798941612243652, KLD_loss 0.06887292861938477\n",
            "Train Epoch: 1 [2240/14166 (15.801354401805868%)]             Loss:0.25365304946899414            R_loss 8.068004608154297, KLD_loss 0.04889347404241562\n",
            "Train Epoch: 1 [2560/14166 (18.058690744920995%)]             Loss:0.2615559995174408            R_loss 8.338517189025879, KLD_loss 0.03127433732151985\n",
            "Train Epoch: 1 [2880/14166 (20.31602708803612%)]             Loss:0.23062869906425476            R_loss 7.361171722412109, KLD_loss 0.01894654706120491\n",
            "Train Epoch: 1 [3200/14166 (22.573363431151243%)]             Loss:0.24412418901920319            R_loss 7.802920341491699, KLD_loss 0.009053848683834076\n",
            "Train Epoch: 1 [3520/14166 (24.830699774266364%)]             Loss:0.23655156791210175            R_loss 7.5655999183654785, KLD_loss 0.004049960523843765\n",
            "Train Epoch: 1 [3840/14166 (27.08803611738149%)]             Loss:0.23852793872356415            R_loss 7.630381107330322, KLD_loss 0.002513118088245392\n",
            "Train Epoch: 1 [4160/14166 (29.345372460496613%)]             Loss:0.21286965906620026            R_loss 6.810403823852539, KLD_loss 0.0014248639345169067\n",
            "Train Epoch: 1 [4480/14166 (31.602708803611737%)]             Loss:0.2245088517665863            R_loss 7.1823811531066895, KLD_loss 0.001901671290397644\n",
            "Train Epoch: 1 [4800/14166 (33.86004514672686%)]             Loss:0.2122240662574768            R_loss 6.789919853210449, KLD_loss 0.0012504570186138153\n",
            "Train Epoch: 1 [5120/14166 (36.11738148984199%)]             Loss:0.21121202409267426            R_loss 6.758181571960449, KLD_loss 0.00060315802693367\n",
            "Train Epoch: 1 [5440/14166 (38.37471783295711%)]             Loss:0.19859221577644348            R_loss 6.354482650756836, KLD_loss 0.0004686526954174042\n",
            "Train Epoch: 1 [5760/14166 (40.63205417607224%)]             Loss:0.21170580387115479            R_loss 6.774259567260742, KLD_loss 0.0003263205289840698\n",
            "Train Epoch: 1 [6080/14166 (42.88939051918736%)]             Loss:0.21713903546333313            R_loss 6.9479780197143555, KLD_loss 0.0004714466631412506\n",
            "Train Epoch: 1 [6400/14166 (45.146726862302486%)]             Loss:0.21988452970981598            R_loss 7.035901069641113, KLD_loss 0.0004039183259010315\n",
            "Train Epoch: 1 [6720/14166 (47.40406320541761%)]             Loss:0.20489443838596344            R_loss 6.556439399719238, KLD_loss 0.00018239393830299377\n",
            "Train Epoch: 1 [7040/14166 (49.66139954853273%)]             Loss:0.21866317093372345            R_loss 6.996971607208252, KLD_loss 0.0002498440444469452\n",
            "Train Epoch: 1 [7360/14166 (51.918735891647856%)]             Loss:0.21311667561531067            R_loss 6.819514274597168, KLD_loss 0.00021911412477493286\n",
            "Train Epoch: 1 [7680/14166 (54.17607223476298%)]             Loss:0.20491865277290344            R_loss 6.557281494140625, KLD_loss 0.00011544302105903625\n",
            "Train Epoch: 1 [8000/14166 (56.433408577878104%)]             Loss:0.212148517370224            R_loss 6.7885942459106445, KLD_loss 0.00015879422426223755\n",
            "Train Epoch: 1 [8320/14166 (58.690744920993225%)]             Loss:0.20149929821491241            R_loss 6.447815895080566, KLD_loss 0.00016154348850250244\n",
            "Train Epoch: 1 [8640/14166 (60.94808126410835%)]             Loss:0.19808751344680786            R_loss 6.338722229003906, KLD_loss 7.744506001472473e-05\n",
            "Train Epoch: 1 [8960/14166 (63.205417607223474%)]             Loss:0.21401424705982208            R_loss 6.848291397094727, KLD_loss 0.00016446411609649658\n",
            "Train Epoch: 1 [9280/14166 (65.4627539503386%)]             Loss:0.2179475724697113            R_loss 6.974077224731445, KLD_loss 0.0002451278269290924\n",
            "Train Epoch: 1 [9600/14166 (67.72009029345372%)]             Loss:0.20098279416561127            R_loss 6.4311699867248535, KLD_loss 0.0002794824540615082\n",
            "Train Epoch: 1 [9920/14166 (69.97742663656885%)]             Loss:0.20715279877185822            R_loss 6.628711223602295, KLD_loss 0.00017827749252319336\n",
            "Train Epoch: 1 [10240/14166 (72.23476297968398%)]             Loss:0.21430784463882446            R_loss 6.857670307159424, KLD_loss 0.00018110498785972595\n",
            "Train Epoch: 1 [10560/14166 (74.49209932279909%)]             Loss:0.20327278971672058            R_loss 6.504532337188721, KLD_loss 0.0001970827579498291\n",
            "Train Epoch: 1 [10880/14166 (76.74943566591422%)]             Loss:0.2159712314605713            R_loss 6.9108781814575195, KLD_loss 0.00020164623856544495\n",
            "Train Epoch: 1 [11200/14166 (79.00677200902935%)]             Loss:0.22086462378501892            R_loss 7.06746768951416, KLD_loss 0.0002000853419303894\n",
            "Train Epoch: 1 [11520/14166 (81.26410835214448%)]             Loss:0.21451570093631744            R_loss 6.864339351654053, KLD_loss 0.00016294792294502258\n",
            "Train Epoch: 1 [11840/14166 (83.52144469525959%)]             Loss:0.21274955570697784            R_loss 6.807801246643066, KLD_loss 0.00018480420112609863\n",
            "Train Epoch: 1 [12160/14166 (85.77878103837472%)]             Loss:0.2117747664451599            R_loss 6.776615619659424, KLD_loss 0.00017752870917320251\n",
            "Train Epoch: 1 [12480/14166 (88.03611738148985%)]             Loss:0.20085103809833527            R_loss 6.427030563354492, KLD_loss 0.00020252540707588196\n",
            "Train Epoch: 1 [12800/14166 (90.29345372460497%)]             Loss:0.21318572759628296            R_loss 6.821646690368652, KLD_loss 0.0002964884042739868\n",
            "Train Epoch: 1 [13120/14166 (92.55079006772009%)]             Loss:0.19998742640018463            R_loss 6.39933443069458, KLD_loss 0.0002625957131385803\n",
            "Train Epoch: 1 [13440/14166 (94.80812641083521%)]             Loss:0.2064930945634842            R_loss 6.607579231262207, KLD_loss 0.00020050257444381714\n",
            "Train Epoch: 1 [13760/14166 (97.06546275395034%)]             Loss:0.20496557652950287            R_loss 6.558581352233887, KLD_loss 0.00031674280762672424\n",
            "Train Epoch: 1 [14080/14166 (99.32279909706546%)]             Loss:0.20627610385417938            R_loss 6.6005859375, KLD_loss 0.00024902448058128357\n",
            "====> Epoch: 1 Average loss: 0.2233\n",
            "====> Test set loss: 0.2112\n",
            "Train Epoch: 2 [0/14166 (0.0%)]             Loss:0.2136361002922058            R_loss 6.836113929748535, KLD_loss 0.00024079903960227966\n",
            "Train Epoch: 2 [320/14166 (2.2573363431151243%)]             Loss:0.21836408972740173            R_loss 6.9874420166015625, KLD_loss 0.000209122896194458\n",
            "Train Epoch: 2 [640/14166 (4.514672686230249%)]             Loss:0.2214830368757248            R_loss 7.087247848510742, KLD_loss 0.0002097487449645996\n",
            "Train Epoch: 2 [960/14166 (6.772009029345372%)]             Loss:0.21597081422805786            R_loss 6.910792350769043, KLD_loss 0.0002737604081630707\n",
            "Train Epoch: 2 [1280/14166 (9.029345372460497%)]             Loss:0.20470750331878662            R_loss 6.55033540725708, KLD_loss 0.0003045983612537384\n",
            "Train Epoch: 2 [1600/14166 (11.286681715575622%)]             Loss:0.21326246857643127            R_loss 6.824145317077637, KLD_loss 0.0002536289393901825\n",
            "Train Epoch: 2 [1920/14166 (13.544018058690744%)]             Loss:0.20184576511383057            R_loss 6.458885192871094, KLD_loss 0.00017892569303512573\n",
            "Train Epoch: 2 [2240/14166 (15.801354401805868%)]             Loss:0.20089659094810486            R_loss 6.4285054206848145, KLD_loss 0.00018544867634773254\n",
            "Train Epoch: 2 [2560/14166 (18.058690744920995%)]             Loss:0.20996513962745667            R_loss 6.718631267547607, KLD_loss 0.0002534054219722748\n",
            "Train Epoch: 2 [2880/14166 (20.31602708803612%)]             Loss:0.2098514288663864            R_loss 6.7150397300720215, KLD_loss 0.0002065412700176239\n",
            "Train Epoch: 2 [3200/14166 (22.573363431151243%)]             Loss:0.21035143733024597            R_loss 6.731039047241211, KLD_loss 0.0002066679298877716\n",
            "Train Epoch: 2 [3520/14166 (24.830699774266364%)]             Loss:0.22419385612010956            R_loss 7.173977851867676, KLD_loss 0.0002254769206047058\n",
            "Train Epoch: 2 [3840/14166 (27.08803611738149%)]             Loss:0.20716968178749084            R_loss 6.6292219161987305, KLD_loss 0.00020761415362358093\n",
            "Train Epoch: 2 [4160/14166 (29.345372460496613%)]             Loss:0.2295307219028473            R_loss 7.344645977020264, KLD_loss 0.000337015837430954\n",
            "Train Epoch: 2 [4480/14166 (31.602708803611737%)]             Loss:0.19787636399269104            R_loss 6.331867218017578, KLD_loss 0.0001765042543411255\n",
            "Train Epoch: 2 [4800/14166 (33.86004514672686%)]             Loss:0.2052748054265976            R_loss 6.5685272216796875, KLD_loss 0.00026640668511390686\n",
            "Train Epoch: 2 [5120/14166 (36.11738148984199%)]             Loss:0.2370142936706543            R_loss 7.584060192108154, KLD_loss 0.0003968067467212677\n",
            "Train Epoch: 2 [5440/14166 (38.37471783295711%)]             Loss:0.22308114171028137            R_loss 7.138182640075684, KLD_loss 0.0004137866199016571\n",
            "Train Epoch: 2 [5760/14166 (40.63205417607224%)]             Loss:0.2152884304523468            R_loss 6.888915061950684, KLD_loss 0.00031454116106033325\n",
            "Train Epoch: 2 [6080/14166 (42.88939051918736%)]             Loss:0.21863901615142822            R_loss 6.996135234832764, KLD_loss 0.0003127679228782654\n",
            "Train Epoch: 2 [6400/14166 (45.146726862302486%)]             Loss:0.2005636841058731            R_loss 6.4178361892700195, KLD_loss 0.00020150840282440186\n",
            "Train Epoch: 2 [6720/14166 (47.40406320541761%)]             Loss:0.21506910026073456            R_loss 6.881777286529541, KLD_loss 0.00043409690260887146\n",
            "Train Epoch: 2 [7040/14166 (49.66139954853273%)]             Loss:0.21773704886436462            R_loss 6.967275619506836, KLD_loss 0.00031016767024993896\n",
            "Train Epoch: 2 [7360/14166 (51.918735891647856%)]             Loss:0.23218348622322083            R_loss 7.42952823638916, KLD_loss 0.00034371018409729004\n",
            "Train Epoch: 2 [7680/14166 (54.17607223476298%)]             Loss:0.21201589703559875            R_loss 6.78426456451416, KLD_loss 0.00024393945932388306\n",
            "Train Epoch: 2 [8000/14166 (56.433408577878104%)]             Loss:0.21564945578575134            R_loss 6.900567531585693, KLD_loss 0.00021498650312423706\n",
            "Train Epoch: 2 [8320/14166 (58.690744920993225%)]             Loss:0.20405419170856476            R_loss 6.529572010040283, KLD_loss 0.00016227364540100098\n",
            "Train Epoch: 2 [8640/14166 (60.94808126410835%)]             Loss:0.2032264769077301            R_loss 6.503098487854004, KLD_loss 0.00014849752187728882\n",
            "Train Epoch: 2 [8960/14166 (63.205417607223474%)]             Loss:0.21076138317584991            R_loss 6.744164943695068, KLD_loss 0.00019896402955055237\n",
            "Train Epoch: 2 [9280/14166 (65.4627539503386%)]             Loss:0.2159349024295807            R_loss 6.909705638885498, KLD_loss 0.00021091103553771973\n",
            "Train Epoch: 2 [9600/14166 (67.72009029345372%)]             Loss:0.20146209001541138            R_loss 6.446653842926025, KLD_loss 0.00013281777501106262\n",
            "Train Epoch: 2 [9920/14166 (69.97742663656885%)]             Loss:0.22485269606113434            R_loss 7.195037841796875, KLD_loss 0.00024834275245666504\n",
            "Train Epoch: 2 [10240/14166 (72.23476297968398%)]             Loss:0.2049320787191391            R_loss 6.557621002197266, KLD_loss 0.0002057105302810669\n",
            "Train Epoch: 2 [10560/14166 (74.49209932279909%)]             Loss:0.19861555099487305            R_loss 6.355485439300537, KLD_loss 0.00021248310804367065\n",
            "Train Epoch: 2 [10880/14166 (76.74943566591422%)]             Loss:0.2119937390089035            R_loss 6.783514976501465, KLD_loss 0.0002849847078323364\n",
            "Train Epoch: 2 [11200/14166 (79.00677200902935%)]             Loss:0.21786466240882874            R_loss 6.971371650695801, KLD_loss 0.0002975836396217346\n",
            "Train Epoch: 2 [11520/14166 (81.26410835214448%)]             Loss:0.21427688002586365            R_loss 6.8564605712890625, KLD_loss 0.0003992505371570587\n",
            "Train Epoch: 2 [11840/14166 (83.52144469525959%)]             Loss:0.20940908789634705            R_loss 6.700736045837402, KLD_loss 0.0003544837236404419\n",
            "Train Epoch: 2 [12160/14166 (85.77878103837472%)]             Loss:0.20723316073417664            R_loss 6.631056308746338, KLD_loss 0.0004058033227920532\n",
            "Train Epoch: 2 [12480/14166 (88.03611738148985%)]             Loss:0.2018664926290512            R_loss 6.459385871887207, KLD_loss 0.0003423765301704407\n",
            "Train Epoch: 2 [12800/14166 (90.29345372460497%)]             Loss:0.20870143175125122            R_loss 6.6781206130981445, KLD_loss 0.00032483041286468506\n",
            "Train Epoch: 2 [13120/14166 (92.55079006772009%)]             Loss:0.2061089426279068            R_loss 6.595187187194824, KLD_loss 0.0002986043691635132\n",
            "Train Epoch: 2 [13440/14166 (94.80812641083521%)]             Loss:0.21238094568252563            R_loss 6.795823097229004, KLD_loss 0.000366751104593277\n",
            "Train Epoch: 2 [13760/14166 (97.06546275395034%)]             Loss:0.21723705530166626            R_loss 6.951234817504883, KLD_loss 0.0003511086106300354\n",
            "Train Epoch: 2 [14080/14166 (99.32279909706546%)]             Loss:0.20708808302879333            R_loss 6.626513481140137, KLD_loss 0.0003054998815059662\n",
            "====> Epoch: 2 Average loss: 0.2113\n",
            "====> Test set loss: 0.2110\n",
            "Train Epoch: 3 [0/14166 (0.0%)]             Loss:0.2193237841129303            R_loss 7.017941474914551, KLD_loss 0.0004195757210254669\n",
            "Train Epoch: 3 [320/14166 (2.2573363431151243%)]             Loss:0.20918186008930206            R_loss 6.693446636199951, KLD_loss 0.00037257373332977295\n",
            "Train Epoch: 3 [640/14166 (4.514672686230249%)]             Loss:0.21598714590072632            R_loss 6.911102294921875, KLD_loss 0.00048612430691719055\n",
            "Train Epoch: 3 [960/14166 (6.772009029345372%)]             Loss:0.21324267983436584            R_loss 6.823273658752441, KLD_loss 0.0004922077059745789\n",
            "Train Epoch: 3 [1280/14166 (9.029345372460497%)]             Loss:0.20579135417938232            R_loss 6.584791660308838, KLD_loss 0.0005316585302352905\n",
            "Train Epoch: 3 [1600/14166 (11.286681715575622%)]             Loss:0.2021385133266449            R_loss 6.467980861663818, KLD_loss 0.00045175477862358093\n",
            "Train Epoch: 3 [1920/14166 (13.544018058690744%)]             Loss:0.20726335048675537            R_loss 6.632050514221191, KLD_loss 0.0003768056631088257\n",
            "Train Epoch: 3 [2240/14166 (15.801354401805868%)]             Loss:0.21390745043754578            R_loss 6.84459114074707, KLD_loss 0.000447295606136322\n",
            "Train Epoch: 3 [2560/14166 (18.058690744920995%)]             Loss:0.20749416947364807            R_loss 6.639374732971191, KLD_loss 0.00043866410851478577\n",
            "Train Epoch: 3 [2880/14166 (20.31602708803612%)]             Loss:0.21659326553344727            R_loss 6.930466175079346, KLD_loss 0.0005180314183235168\n",
            "Train Epoch: 3 [3200/14166 (22.573363431151243%)]             Loss:0.21015945076942444            R_loss 6.724735736846924, KLD_loss 0.00036627426743507385\n",
            "Train Epoch: 3 [3520/14166 (24.830699774266364%)]             Loss:0.21849147975444794            R_loss 6.991343021392822, KLD_loss 0.0003843829035758972\n",
            "Train Epoch: 3 [3840/14166 (27.08803611738149%)]             Loss:0.2073015570640564            R_loss 6.633372783660889, KLD_loss 0.0002772584557533264\n",
            "Train Epoch: 3 [4160/14166 (29.345372460496613%)]             Loss:0.22517463564872742            R_loss 7.2050275802612305, KLD_loss 0.0005608797073364258\n",
            "Train Epoch: 3 [4480/14166 (31.602708803611737%)]             Loss:0.20753926038742065            R_loss 6.640726566314697, KLD_loss 0.0005294531583786011\n",
            "Train Epoch: 3 [4800/14166 (33.86004514672686%)]             Loss:0.20752808451652527            R_loss 6.640495777130127, KLD_loss 0.00040310248732566833\n",
            "Train Epoch: 3 [5120/14166 (36.11738148984199%)]             Loss:0.21269643306732178            R_loss 6.805854320526123, KLD_loss 0.0004312098026275635\n",
            "Train Epoch: 3 [5440/14166 (38.37471783295711%)]             Loss:0.21084128320217133            R_loss 6.746543884277344, KLD_loss 0.00037695467472076416\n",
            "Train Epoch: 3 [5760/14166 (40.63205417607224%)]             Loss:0.20861169695854187            R_loss 6.67512845993042, KLD_loss 0.0004459880292415619\n",
            "Train Epoch: 3 [6080/14166 (42.88939051918736%)]             Loss:0.19182220101356506            R_loss 6.138065814971924, KLD_loss 0.00024456530809402466\n",
            "Train Epoch: 3 [6400/14166 (45.146726862302486%)]             Loss:0.21161843836307526            R_loss 6.77129602432251, KLD_loss 0.0004938505589962006\n",
            "Train Epoch: 3 [6720/14166 (47.40406320541761%)]             Loss:0.21705694496631622            R_loss 6.94522762298584, KLD_loss 0.0005947388708591461\n",
            "Train Epoch: 3 [7040/14166 (49.66139954853273%)]             Loss:0.226388081908226            R_loss 7.243739128112793, KLD_loss 0.0006791390478610992\n",
            "Train Epoch: 3 [7360/14166 (51.918735891647856%)]             Loss:0.2215718924999237            R_loss 7.089600563049316, KLD_loss 0.0006997622549533844\n",
            "Train Epoch: 3 [7680/14166 (54.17607223476298%)]             Loss:0.21348871290683746            R_loss 6.831035614013672, KLD_loss 0.0006033554673194885\n",
            "Train Epoch: 3 [8000/14166 (56.433408577878104%)]             Loss:0.20236888527870178            R_loss 6.475375175476074, KLD_loss 0.00042866915464401245\n",
            "Train Epoch: 3 [8320/14166 (58.690744920993225%)]             Loss:0.21525873243808746            R_loss 6.887656211853027, KLD_loss 0.000622924417257309\n",
            "Train Epoch: 3 [8640/14166 (60.94808126410835%)]             Loss:0.20658066868782043            R_loss 6.609921932220459, KLD_loss 0.0006593167781829834\n",
            "Train Epoch: 3 [8960/14166 (63.205417607223474%)]             Loss:0.21842850744724274            R_loss 6.988927841186523, KLD_loss 0.0007840991020202637\n",
            "Train Epoch: 3 [9280/14166 (65.4627539503386%)]             Loss:0.22052136063575745            R_loss 7.055854797363281, KLD_loss 0.0008288249373435974\n",
            "Train Epoch: 3 [9600/14166 (67.72009029345372%)]             Loss:0.22066736221313477            R_loss 7.060662269592285, KLD_loss 0.0006934665143489838\n",
            "Train Epoch: 3 [9920/14166 (69.97742663656885%)]             Loss:0.2145683318376541            R_loss 6.865396499633789, KLD_loss 0.0007903315126895905\n",
            "Train Epoch: 3 [10240/14166 (72.23476297968398%)]             Loss:0.20381630957126617            R_loss 6.52138090133667, KLD_loss 0.0007412731647491455\n",
            "Train Epoch: 3 [10560/14166 (74.49209932279909%)]             Loss:0.21935755014419556            R_loss 7.0185418128967285, KLD_loss 0.0008997321128845215\n",
            "Train Epoch: 3 [10880/14166 (76.74943566591422%)]             Loss:0.2265104502439499            R_loss 7.247289180755615, KLD_loss 0.0010448545217514038\n",
            "Train Epoch: 3 [11200/14166 (79.00677200902935%)]             Loss:0.21426384150981903            R_loss 6.855792045593262, KLD_loss 0.0006513819098472595\n",
            "Train Epoch: 3 [11520/14166 (81.26410835214448%)]             Loss:0.20210647583007812            R_loss 6.466942310333252, KLD_loss 0.0004653632640838623\n",
            "Train Epoch: 3 [11840/14166 (83.52144469525959%)]             Loss:0.21318446099758148            R_loss 6.821118354797363, KLD_loss 0.0007843077182769775\n",
            "Train Epoch: 3 [12160/14166 (85.77878103837472%)]             Loss:0.21365544199943542            R_loss 6.8364129066467285, KLD_loss 0.0005605928599834442\n",
            "Train Epoch: 3 [12480/14166 (88.03611738148985%)]             Loss:0.20971889793872833            R_loss 6.710355758666992, KLD_loss 0.0006486885249614716\n",
            "Train Epoch: 3 [12800/14166 (90.29345372460497%)]             Loss:0.19468559324741364            R_loss 6.22939395904541, KLD_loss 0.0005453899502754211\n",
            "Train Epoch: 3 [13120/14166 (92.55079006772009%)]             Loss:0.21145427227020264            R_loss 6.765676498413086, KLD_loss 0.0008601844310760498\n",
            "Train Epoch: 3 [13440/14166 (94.80812641083521%)]             Loss:0.20975472033023834            R_loss 6.711427688598633, KLD_loss 0.0007232613861560822\n",
            "Train Epoch: 3 [13760/14166 (97.06546275395034%)]             Loss:0.21730908751487732            R_loss 6.953143119812012, KLD_loss 0.00074782595038414\n",
            "Train Epoch: 3 [14080/14166 (99.32279909706546%)]             Loss:0.20061776041984558            R_loss 6.419349670410156, KLD_loss 0.00041947141289711\n",
            "====> Epoch: 3 Average loss: 0.2113\n",
            "====> Test set loss: 0.2114\n",
            "Train Epoch: 4 [0/14166 (0.0%)]             Loss:0.19872628152370453            R_loss 6.3587541580200195, KLD_loss 0.0004871785640716553\n",
            "Train Epoch: 4 [320/14166 (2.2573363431151243%)]             Loss:0.20866510272026062            R_loss 6.676661491394043, KLD_loss 0.0006217546761035919\n",
            "Train Epoch: 4 [640/14166 (4.514672686230249%)]             Loss:0.20586979389190674            R_loss 6.587011814117432, KLD_loss 0.0008212700486183167\n",
            "Train Epoch: 4 [960/14166 (6.772009029345372%)]             Loss:0.21479378640651703            R_loss 6.872400283813477, KLD_loss 0.0010006949305534363\n",
            "Train Epoch: 4 [1280/14166 (9.029345372460497%)]             Loss:0.2156408727169037            R_loss 6.899643421173096, KLD_loss 0.0008649826049804688\n",
            "Train Epoch: 4 [1600/14166 (11.286681715575622%)]             Loss:0.2071680873632431            R_loss 6.6287078857421875, KLD_loss 0.0006704516708850861\n",
            "Train Epoch: 4 [1920/14166 (13.544018058690744%)]             Loss:0.22553718090057373            R_loss 7.216092109680176, KLD_loss 0.0010976679623126984\n",
            "Train Epoch: 4 [2240/14166 (15.801354401805868%)]             Loss:0.2218402624130249            R_loss 7.098027229309082, KLD_loss 0.0008615031838417053\n",
            "Train Epoch: 4 [2560/14166 (18.058690744920995%)]             Loss:0.2093532830476761            R_loss 6.698764324188232, KLD_loss 0.0005406886339187622\n",
            "Train Epoch: 4 [2880/14166 (20.31602708803612%)]             Loss:0.20436477661132812            R_loss 6.539188385009766, KLD_loss 0.00048467889428138733\n",
            "Train Epoch: 4 [3200/14166 (22.573363431151243%)]             Loss:0.2210363894701004            R_loss 7.072321891784668, KLD_loss 0.0008423104882240295\n",
            "Train Epoch: 4 [3520/14166 (24.830699774266364%)]             Loss:0.20625169575214386            R_loss 6.599206924438477, KLD_loss 0.0008473619818687439\n",
            "Train Epoch: 4 [3840/14166 (27.08803611738149%)]             Loss:0.2106870710849762            R_loss 6.7412214279174805, KLD_loss 0.000764802098274231\n",
            "Train Epoch: 4 [4160/14166 (29.345372460496613%)]             Loss:0.20135203003883362            R_loss 6.442646026611328, KLD_loss 0.0006186738610267639\n",
            "Train Epoch: 4 [4480/14166 (31.602708803611737%)]             Loss:0.23359498381614685            R_loss 7.4738664627075195, KLD_loss 0.0011733248829841614\n",
            "Train Epoch: 4 [4800/14166 (33.86004514672686%)]             Loss:0.21611277759075165            R_loss 6.914821147918701, KLD_loss 0.0007881596684455872\n",
            "Train Epoch: 4 [5120/14166 (36.11738148984199%)]             Loss:0.20353104174137115            R_loss 6.512553691864014, KLD_loss 0.000439666211605072\n",
            "Train Epoch: 4 [5440/14166 (38.37471783295711%)]             Loss:0.2053137719631195            R_loss 6.56947135925293, KLD_loss 0.0005696378648281097\n",
            "Train Epoch: 4 [5760/14166 (40.63205417607224%)]             Loss:0.22415828704833984            R_loss 7.172115325927734, KLD_loss 0.0009499490261077881\n",
            "Train Epoch: 4 [6080/14166 (42.88939051918736%)]             Loss:0.20399659872055054            R_loss 6.527191638946533, KLD_loss 0.0006992854177951813\n",
            "Train Epoch: 4 [6400/14166 (45.146726862302486%)]             Loss:0.21900169551372528            R_loss 7.0071282386779785, KLD_loss 0.000925995409488678\n",
            "Train Epoch: 4 [6720/14166 (47.40406320541761%)]             Loss:0.2096436470746994            R_loss 6.70776891708374, KLD_loss 0.0008275918662548065\n",
            "Train Epoch: 4 [7040/14166 (49.66139954853273%)]             Loss:0.22205770015716553            R_loss 7.104881286621094, KLD_loss 0.0009653754532337189\n",
            "Train Epoch: 4 [7360/14166 (51.918735891647856%)]             Loss:0.2042233794927597            R_loss 6.534510135650635, KLD_loss 0.000638030469417572\n",
            "Train Epoch: 4 [7680/14166 (54.17607223476298%)]             Loss:0.2161424309015274            R_loss 6.915640830993652, KLD_loss 0.0009167790412902832\n",
            "Train Epoch: 4 [8000/14166 (56.433408577878104%)]             Loss:0.21264199912548065            R_loss 6.803704738616943, KLD_loss 0.0008392408490180969\n",
            "Train Epoch: 4 [8320/14166 (58.690744920993225%)]             Loss:0.22218607366085052            R_loss 7.1090168952941895, KLD_loss 0.0009378865361213684\n",
            "Train Epoch: 4 [8640/14166 (60.94808126410835%)]             Loss:0.2284993827342987            R_loss 7.310828685760498, KLD_loss 0.0011512376368045807\n",
            "Train Epoch: 4 [8960/14166 (63.205417607223474%)]             Loss:0.2022314965724945            R_loss 6.470741271972656, KLD_loss 0.0006665252149105072\n",
            "Train Epoch: 4 [9280/14166 (65.4627539503386%)]             Loss:0.211511492729187            R_loss 6.767294883728027, KLD_loss 0.0010728053748607635\n",
            "Train Epoch: 4 [9600/14166 (67.72009029345372%)]             Loss:0.2101764976978302            R_loss 6.7247419357299805, KLD_loss 0.0009057074785232544\n",
            "Train Epoch: 4 [9920/14166 (69.97742663656885%)]             Loss:0.20774123072624207            R_loss 6.646507263183594, KLD_loss 0.00121234729886055\n",
            "Train Epoch: 4 [10240/14166 (72.23476297968398%)]             Loss:0.2199760526418686            R_loss 7.037411212921143, KLD_loss 0.001822303980588913\n",
            "Train Epoch: 4 [10560/14166 (74.49209932279909%)]             Loss:0.20724037289619446            R_loss 6.630578517913818, KLD_loss 0.00111306831240654\n",
            "Train Epoch: 4 [10880/14166 (76.74943566591422%)]             Loss:0.21943646669387817            R_loss 7.020491600036621, KLD_loss 0.0014755278825759888\n",
            "Train Epoch: 4 [11200/14166 (79.00677200902935%)]             Loss:0.2174859195947647            R_loss 6.958310127258301, KLD_loss 0.0012397132813930511\n",
            "Train Epoch: 4 [11520/14166 (81.26410835214448%)]             Loss:0.2156556248664856            R_loss 6.8997483253479, KLD_loss 0.0012313537299633026\n",
            "Train Epoch: 4 [11840/14166 (83.52144469525959%)]             Loss:0.22129887342453003            R_loss 7.080202102661133, KLD_loss 0.0013613216578960419\n",
            "Train Epoch: 4 [12160/14166 (85.77878103837472%)]             Loss:0.20550537109375            R_loss 6.575267314910889, KLD_loss 0.000904567539691925\n",
            "Train Epoch: 4 [12480/14166 (88.03611738148985%)]             Loss:0.21045182645320892            R_loss 6.733366012573242, KLD_loss 0.001092623919248581\n",
            "Train Epoch: 4 [12800/14166 (90.29345372460497%)]             Loss:0.2003895789384842            R_loss 6.41146993637085, KLD_loss 0.0009968578815460205\n",
            "Train Epoch: 4 [13120/14166 (92.55079006772009%)]             Loss:0.19770547747612            R_loss 6.325570583343506, KLD_loss 0.0010045282542705536\n",
            "Train Epoch: 4 [13440/14166 (94.80812641083521%)]             Loss:0.20968680083751678            R_loss 6.7081708908081055, KLD_loss 0.0018069818615913391\n",
            "Train Epoch: 4 [13760/14166 (97.06546275395034%)]             Loss:0.22413964569568634            R_loss 7.170184135437012, KLD_loss 0.0022837966680526733\n",
            "Train Epoch: 4 [14080/14166 (99.32279909706546%)]             Loss:0.20776648819446564            R_loss 6.646963119506836, KLD_loss 0.0015643872320652008\n",
            "====> Epoch: 4 Average loss: 0.2113\n",
            "====> Test set loss: 0.2110\n",
            "Train Epoch: 5 [0/14166 (0.0%)]             Loss:0.2102799415588379            R_loss 6.727191925048828, KLD_loss 0.0017663463950157166\n",
            "Train Epoch: 5 [320/14166 (2.2573363431151243%)]             Loss:0.20557034015655518            R_loss 6.576674461364746, KLD_loss 0.0015765763819217682\n",
            "Train Epoch: 5 [640/14166 (4.514672686230249%)]             Loss:0.21191692352294922            R_loss 6.779364109039307, KLD_loss 0.0019774213433265686\n",
            "Train Epoch: 5 [960/14166 (6.772009029345372%)]             Loss:0.21689942479133606            R_loss 6.939239978790283, KLD_loss 0.001541581004858017\n",
            "Train Epoch: 5 [1280/14166 (9.029345372460497%)]             Loss:0.21363180875778198            R_loss 6.834699630737305, KLD_loss 0.0015185438096523285\n",
            "Train Epoch: 5 [1600/14166 (11.286681715575622%)]             Loss:0.2127906233072281            R_loss 6.80777645111084, KLD_loss 0.0015235841274261475\n",
            "Train Epoch: 5 [1920/14166 (13.544018058690744%)]             Loss:0.20891068875789642            R_loss 6.683863162994385, KLD_loss 0.0012794137001037598\n",
            "Train Epoch: 5 [2240/14166 (15.801354401805868%)]             Loss:0.19426630437374115            R_loss 6.215544700622559, KLD_loss 0.0009767040610313416\n",
            "Train Epoch: 5 [2560/14166 (18.058690744920995%)]             Loss:0.20495791733264923            R_loss 6.557305335998535, KLD_loss 0.0013477206230163574\n",
            "Train Epoch: 5 [2880/14166 (20.31602708803612%)]             Loss:0.21393558382987976            R_loss 6.84453010559082, KLD_loss 0.001408405601978302\n",
            "Train Epoch: 5 [3200/14166 (22.573363431151243%)]             Loss:0.22222237288951874            R_loss 7.109187126159668, KLD_loss 0.0019290931522846222\n",
            "Train Epoch: 5 [3520/14166 (24.830699774266364%)]             Loss:0.2118404656648636            R_loss 6.777544021606445, KLD_loss 0.0013509541749954224\n",
            "Train Epoch: 5 [3840/14166 (27.08803611738149%)]             Loss:0.20220603048801422            R_loss 6.469590663909912, KLD_loss 0.0010021068155765533\n",
            "Train Epoch: 5 [4160/14166 (29.345372460496613%)]             Loss:0.19772614538669586            R_loss 6.3263678550720215, KLD_loss 0.0008687078952789307\n",
            "Train Epoch: 5 [4480/14166 (31.602708803611737%)]             Loss:0.23740820586681366            R_loss 7.594761848449707, KLD_loss 0.002300586551427841\n",
            "Train Epoch: 5 [4800/14166 (33.86004514672686%)]             Loss:0.2122412919998169            R_loss 6.789990425109863, KLD_loss 0.0017314143478870392\n",
            "Train Epoch: 5 [5120/14166 (36.11738148984199%)]             Loss:0.20423950254917145            R_loss 6.534577369689941, KLD_loss 0.0010864995419979095\n",
            "Train Epoch: 5 [5440/14166 (38.37471783295711%)]             Loss:0.21642564237117767            R_loss 6.9241251945495605, KLD_loss 0.0014956183731555939\n",
            "Train Epoch: 5 [5760/14166 (40.63205417607224%)]             Loss:0.196516215801239            R_loss 6.2877302169799805, KLD_loss 0.0007882416248321533\n",
            "Train Epoch: 5 [6080/14166 (42.88939051918736%)]             Loss:0.21990364789962769            R_loss 7.035154342651367, KLD_loss 0.0017623603343963623\n",
            "Train Epoch: 5 [6400/14166 (45.146726862302486%)]             Loss:0.2145390659570694            R_loss 6.863818645477295, KLD_loss 0.0014316961169242859\n",
            "Train Epoch: 5 [6720/14166 (47.40406320541761%)]             Loss:0.21623148024082184            R_loss 6.917725086212158, KLD_loss 0.001682426780462265\n",
            "Train Epoch: 5 [7040/14166 (49.66139954853273%)]             Loss:0.20336176455020905            R_loss 6.506382465362549, KLD_loss 0.0011942051351070404\n",
            "Train Epoch: 5 [7360/14166 (51.918735891647856%)]             Loss:0.20844274759292603            R_loss 6.6687092781066895, KLD_loss 0.001458495855331421\n",
            "Train Epoch: 5 [7680/14166 (54.17607223476298%)]             Loss:0.21948537230491638            R_loss 7.021512031555176, KLD_loss 0.002020064741373062\n",
            "Train Epoch: 5 [8000/14166 (56.433408577878104%)]             Loss:0.21259935200214386            R_loss 6.801177978515625, KLD_loss 0.002001233398914337\n",
            "Train Epoch: 5 [8320/14166 (58.690744920993225%)]             Loss:0.20268791913986206            R_loss 6.4847731590271, KLD_loss 0.0012398920953273773\n",
            "Train Epoch: 5 [8640/14166 (60.94808126410835%)]             Loss:0.20536445081233978            R_loss 6.570525169372559, KLD_loss 0.0011370889842510223\n",
            "Train Epoch: 5 [8960/14166 (63.205417607223474%)]             Loss:0.22312520444393158            R_loss 7.1383867263793945, KLD_loss 0.0016196072101593018\n",
            "Train Epoch: 5 [9280/14166 (65.4627539503386%)]             Loss:0.2194814234972            R_loss 7.021872520446777, KLD_loss 0.0015329085290431976\n",
            "Train Epoch: 5 [9600/14166 (67.72009029345372%)]             Loss:0.20458222925662994            R_loss 6.545415878295898, KLD_loss 0.0012147724628448486\n",
            "Train Epoch: 5 [9920/14166 (69.97742663656885%)]             Loss:0.2176133096218109            R_loss 6.961592674255371, KLD_loss 0.002033565193414688\n",
            "Train Epoch: 5 [10240/14166 (72.23476297968398%)]             Loss:0.2058669775724411            R_loss 6.586260795593262, KLD_loss 0.0014827921986579895\n",
            "Train Epoch: 5 [10560/14166 (74.49209932279909%)]             Loss:0.20388038456439972            R_loss 6.522941589355469, KLD_loss 0.0012307055294513702\n",
            "Train Epoch: 5 [10880/14166 (76.74943566591422%)]             Loss:0.21465715765953064            R_loss 6.867245674133301, KLD_loss 0.0017832815647125244\n",
            "Train Epoch: 5 [11200/14166 (79.00677200902935%)]             Loss:0.2073802649974823            R_loss 6.634376525878906, KLD_loss 0.0017924457788467407\n",
            "Train Epoch: 5 [11520/14166 (81.26410835214448%)]             Loss:0.2179221212863922            R_loss 6.97108268737793, KLD_loss 0.0024249516427516937\n",
            "Train Epoch: 5 [11840/14166 (83.52144469525959%)]             Loss:0.22454887628555298            R_loss 7.183074951171875, KLD_loss 0.0024894699454307556\n",
            "Train Epoch: 5 [12160/14166 (85.77878103837472%)]             Loss:0.211897075176239            R_loss 6.778865337371826, KLD_loss 0.001841530203819275\n",
            "Train Epoch: 5 [12480/14166 (88.03611738148985%)]             Loss:0.21296784281730652            R_loss 6.8132476806640625, KLD_loss 0.0017228424549102783\n",
            "Train Epoch: 5 [12800/14166 (90.29345372460497%)]             Loss:0.2027176320552826            R_loss 6.485287189483643, KLD_loss 0.0016777180135250092\n",
            "Train Epoch: 5 [13120/14166 (92.55079006772009%)]             Loss:0.19711533188819885            R_loss 6.306642055511475, KLD_loss 0.001048542559146881\n",
            "Train Epoch: 5 [13440/14166 (94.80812641083521%)]             Loss:0.2027895152568817            R_loss 6.487725257873535, KLD_loss 0.0015389025211334229\n",
            "Train Epoch: 5 [13760/14166 (97.06546275395034%)]             Loss:0.20831769704818726            R_loss 6.6642255783081055, KLD_loss 0.0019409023225307465\n",
            "Train Epoch: 5 [14080/14166 (99.32279909706546%)]             Loss:0.19442497193813324            R_loss 6.2203755378723145, KLD_loss 0.0012235268950462341\n",
            "====> Epoch: 5 Average loss: 0.2113\n",
            "====> Test set loss: 0.2117\n",
            "Train Epoch: 6 [0/14166 (0.0%)]             Loss:0.2087497115135193            R_loss 6.6780195236206055, KLD_loss 0.001971520483493805\n",
            "Train Epoch: 6 [320/14166 (2.2573363431151243%)]             Loss:0.2001950740814209            R_loss 6.404677867889404, KLD_loss 0.0015644319355487823\n",
            "Train Epoch: 6 [640/14166 (4.514672686230249%)]             Loss:0.21679461002349854            R_loss 6.934866905212402, KLD_loss 0.002560541033744812\n",
            "Train Epoch: 6 [960/14166 (6.772009029345372%)]             Loss:0.20145532488822937            R_loss 6.444627285003662, KLD_loss 0.0019431933760643005\n",
            "Train Epoch: 6 [1280/14166 (9.029345372460497%)]             Loss:0.21121475100517273            R_loss 6.756898880004883, KLD_loss 0.001972995698451996\n",
            "Train Epoch: 6 [1600/14166 (11.286681715575622%)]             Loss:0.2111523598432541            R_loss 6.754746437072754, KLD_loss 0.0021285712718963623\n",
            "Train Epoch: 6 [1920/14166 (13.544018058690744%)]             Loss:0.21201243996620178            R_loss 6.781897068023682, KLD_loss 0.0025015659630298615\n",
            "Train Epoch: 6 [2240/14166 (15.801354401805868%)]             Loss:0.2161361128091812            R_loss 6.913629531860352, KLD_loss 0.0027257315814495087\n",
            "Train Epoch: 6 [2560/14166 (18.058690744920995%)]             Loss:0.21107660233974457            R_loss 6.751643657684326, KLD_loss 0.00280781090259552\n",
            "Train Epoch: 6 [2880/14166 (20.31602708803612%)]             Loss:0.2297554910182953            R_loss 7.348666191101074, KLD_loss 0.0035091154277324677\n",
            "Train Epoch: 6 [3200/14166 (22.573363431151243%)]             Loss:0.21490724384784698            R_loss 6.8741536140441895, KLD_loss 0.00287836417555809\n",
            "Train Epoch: 6 [3520/14166 (24.830699774266364%)]             Loss:0.2197481393814087            R_loss 7.0289177894592285, KLD_loss 0.0030221976339817047\n",
            "Train Epoch: 6 [3840/14166 (27.08803611738149%)]             Loss:0.20532603561878204            R_loss 6.568234443664551, KLD_loss 0.0021986067295074463\n",
            "Train Epoch: 6 [4160/14166 (29.345372460496613%)]             Loss:0.21866384148597717            R_loss 6.993307113647461, KLD_loss 0.003935575485229492\n",
            "Train Epoch: 6 [4480/14166 (31.602708803611737%)]             Loss:0.21460244059562683            R_loss 6.86348295211792, KLD_loss 0.003795187920331955\n",
            "Train Epoch: 6 [4800/14166 (33.86004514672686%)]             Loss:0.21262995898723602            R_loss 6.800647735595703, KLD_loss 0.003511197865009308\n",
            "Train Epoch: 6 [5120/14166 (36.11738148984199%)]             Loss:0.19948306679725647            R_loss 6.381485939025879, KLD_loss 0.0019720755517482758\n",
            "Train Epoch: 6 [5440/14166 (38.37471783295711%)]             Loss:0.20031791925430298            R_loss 6.407978534698486, KLD_loss 0.002195395529270172\n",
            "Train Epoch: 6 [5760/14166 (40.63205417607224%)]             Loss:0.21522264182567596            R_loss 6.883313179016113, KLD_loss 0.003810714930295944\n",
            "Train Epoch: 6 [6080/14166 (42.88939051918736%)]             Loss:0.21034148335456848            R_loss 6.727499008178711, KLD_loss 0.0034284256398677826\n",
            "Train Epoch: 6 [6400/14166 (45.146726862302486%)]             Loss:0.21543321013450623            R_loss 6.889632701873779, KLD_loss 0.00422954186797142\n",
            "Train Epoch: 6 [6720/14166 (47.40406320541761%)]             Loss:0.22405724227428436            R_loss 7.165117263793945, KLD_loss 0.004714794456958771\n",
            "Train Epoch: 6 [7040/14166 (49.66139954853273%)]             Loss:0.2105129361152649            R_loss 6.732085704803467, KLD_loss 0.004328358918428421\n",
            "Train Epoch: 6 [7360/14166 (51.918735891647856%)]             Loss:0.21406953036785126            R_loss 6.845582962036133, KLD_loss 0.004641823470592499\n",
            "Train Epoch: 6 [7680/14166 (54.17607223476298%)]             Loss:0.2232680469751358            R_loss 7.1393585205078125, KLD_loss 0.005219154059886932\n",
            "Train Epoch: 6 [8000/14166 (56.433408577878104%)]             Loss:0.21499061584472656            R_loss 6.87412166595459, KLD_loss 0.005578070878982544\n",
            "Train Epoch: 6 [8320/14166 (58.690744920993225%)]             Loss:0.2097286731004715            R_loss 6.706134796142578, KLD_loss 0.005183123052120209\n",
            "Train Epoch: 6 [8640/14166 (60.94808126410835%)]             Loss:0.20893678069114685            R_loss 6.681174278259277, KLD_loss 0.004803106188774109\n",
            "Train Epoch: 6 [8960/14166 (63.205417607223474%)]             Loss:0.21866269409656525            R_loss 6.989876747131348, KLD_loss 0.0073297470808029175\n",
            "Train Epoch: 6 [9280/14166 (65.4627539503386%)]             Loss:0.21457944810390472            R_loss 6.859340667724609, KLD_loss 0.007201801985502243\n",
            "Train Epoch: 6 [9600/14166 (67.72009029345372%)]             Loss:0.21102160215377808            R_loss 6.746674537658691, KLD_loss 0.00601644441485405\n",
            "Train Epoch: 6 [9920/14166 (69.97742663656885%)]             Loss:0.21830394864082336            R_loss 6.9785614013671875, KLD_loss 0.007165446877479553\n",
            "Train Epoch: 6 [10240/14166 (72.23476297968398%)]             Loss:0.20682023465633392            R_loss 6.612607955932617, KLD_loss 0.005639266222715378\n",
            "Train Epoch: 6 [10560/14166 (74.49209932279909%)]             Loss:0.2092970907688141            R_loss 6.691177845001221, KLD_loss 0.00632915273308754\n",
            "Train Epoch: 6 [10880/14166 (76.74943566591422%)]             Loss:0.19973573088645935            R_loss 6.387864589691162, KLD_loss 0.0036785826086997986\n",
            "Train Epoch: 6 [11200/14166 (79.00677200902935%)]             Loss:0.20511528849601746            R_loss 6.558279991149902, KLD_loss 0.005409028381109238\n",
            "Train Epoch: 6 [11520/14166 (81.26410835214448%)]             Loss:0.21138142049312592            R_loss 6.756944179534912, KLD_loss 0.0072607919573783875\n",
            "Train Epoch: 6 [11840/14166 (83.52144469525959%)]             Loss:0.20802569389343262            R_loss 6.64968729019165, KLD_loss 0.007135171443223953\n",
            "Train Epoch: 6 [12160/14166 (85.77878103837472%)]             Loss:0.21443931758403778            R_loss 6.852697849273682, KLD_loss 0.009360797703266144\n",
            "Train Epoch: 6 [12480/14166 (88.03611738148985%)]             Loss:0.21292604506015778            R_loss 6.803727149963379, KLD_loss 0.009906027466058731\n",
            "Train Epoch: 6 [12800/14166 (90.29345372460497%)]             Loss:0.20785591006278992            R_loss 6.64316987991333, KLD_loss 0.008219800889492035\n",
            "Train Epoch: 6 [13120/14166 (92.55079006772009%)]             Loss:0.198164701461792            R_loss 6.335320949554443, KLD_loss 0.005949225276708603\n",
            "Train Epoch: 6 [13440/14166 (94.80812641083521%)]             Loss:0.21295057237148285            R_loss 6.804916858673096, KLD_loss 0.009501557797193527\n",
            "Train Epoch: 6 [13760/14166 (97.06546275395034%)]             Loss:0.19908997416496277            R_loss 6.362555027008057, KLD_loss 0.0083245187997818\n",
            "Train Epoch: 6 [14080/14166 (99.32279909706546%)]             Loss:0.21211029589176178            R_loss 6.776185035705566, KLD_loss 0.011344656348228455\n",
            "====> Epoch: 6 Average loss: 0.2112\n",
            "====> Test set loss: 0.2110\n",
            "Train Epoch: 7 [0/14166 (0.0%)]             Loss:0.2069416344165802            R_loss 6.612361431121826, KLD_loss 0.009770587086677551\n",
            "Train Epoch: 7 [320/14166 (2.2573363431151243%)]             Loss:0.21240279078483582            R_loss 6.7826313972473145, KLD_loss 0.014258671551942825\n",
            "Train Epoch: 7 [640/14166 (4.514672686230249%)]             Loss:0.201996847987175            R_loss 6.454063415527344, KLD_loss 0.009835749864578247\n",
            "Train Epoch: 7 [960/14166 (6.772009029345372%)]             Loss:0.20290261507034302            R_loss 6.482057571411133, KLD_loss 0.010825775563716888\n",
            "Train Epoch: 7 [1280/14166 (9.029345372460497%)]             Loss:0.20952516794204712            R_loss 6.686478137969971, KLD_loss 0.018326904624700546\n",
            "Train Epoch: 7 [1600/14166 (11.286681715575622%)]             Loss:0.21843090653419495            R_loss 6.967800140380859, KLD_loss 0.02198842540383339\n",
            "Train Epoch: 7 [1920/14166 (13.544018058690744%)]             Loss:0.20452913641929626            R_loss 6.526632785797119, KLD_loss 0.01829926297068596\n",
            "Train Epoch: 7 [2240/14166 (15.801354401805868%)]             Loss:0.21501943469047546            R_loss 6.854917526245117, KLD_loss 0.025705058127641678\n",
            "Train Epoch: 7 [2560/14166 (18.058690744920995%)]             Loss:0.2185152918100357            R_loss 6.962693214416504, KLD_loss 0.029795873910188675\n",
            "Train Epoch: 7 [2880/14166 (20.31602708803612%)]             Loss:0.2039743959903717            R_loss 6.506834030151367, KLD_loss 0.02034633979201317\n",
            "Train Epoch: 7 [3200/14166 (22.573363431151243%)]             Loss:0.20440831780433655            R_loss 6.517279624938965, KLD_loss 0.023786425590515137\n",
            "Train Epoch: 7 [3520/14166 (24.830699774266364%)]             Loss:0.21994417905807495            R_loss 7.000193119049072, KLD_loss 0.03802071139216423\n",
            "Train Epoch: 7 [3840/14166 (27.08803611738149%)]             Loss:0.20668603479862213            R_loss 6.576074123382568, KLD_loss 0.037878770381212234\n",
            "Train Epoch: 7 [4160/14166 (29.345372460496613%)]             Loss:0.2028309851884842            R_loss 6.459286689758301, KLD_loss 0.031304772943258286\n",
            "Train Epoch: 7 [4480/14166 (31.602708803611737%)]             Loss:0.21148681640625            R_loss 6.723837852478027, KLD_loss 0.04374029114842415\n",
            "Train Epoch: 7 [4800/14166 (33.86004514672686%)]             Loss:0.2058936506509781            R_loss 6.535478591918945, KLD_loss 0.05311848223209381\n",
            "Train Epoch: 7 [5120/14166 (36.11738148984199%)]             Loss:0.21139542758464813            R_loss 6.69625997543335, KLD_loss 0.06839334964752197\n",
            "Train Epoch: 7 [5440/14166 (38.37471783295711%)]             Loss:0.2086801528930664            R_loss 6.6179070472717285, KLD_loss 0.059858597815036774\n",
            "Train Epoch: 7 [5760/14166 (40.63205417607224%)]             Loss:0.19829560816287994            R_loss 6.281924247741699, KLD_loss 0.06353525817394257\n",
            "Train Epoch: 7 [6080/14166 (42.88939051918736%)]             Loss:0.19714902341365814            R_loss 6.212038040161133, KLD_loss 0.09673109650611877\n",
            "Train Epoch: 7 [6400/14166 (45.146726862302486%)]             Loss:0.21167314052581787            R_loss 6.635590553283691, KLD_loss 0.13795006275177002\n",
            "Train Epoch: 7 [6720/14166 (47.40406320541761%)]             Loss:0.19312477111816406            R_loss 6.093583583831787, KLD_loss 0.08640922605991364\n",
            "Train Epoch: 7 [7040/14166 (49.66139954853273%)]             Loss:0.20016901195049286            R_loss 6.249705791473389, KLD_loss 0.155702605843544\n",
            "Train Epoch: 7 [7360/14166 (51.918735891647856%)]             Loss:0.21495383977890015            R_loss 6.7083234786987305, KLD_loss 0.17020002007484436\n",
            "Train Epoch: 7 [7680/14166 (54.17607223476298%)]             Loss:0.2038431316614151            R_loss 6.353414535522461, KLD_loss 0.1695658266544342\n",
            "Train Epoch: 7 [8000/14166 (56.433408577878104%)]             Loss:0.1977882832288742            R_loss 6.200318336486816, KLD_loss 0.12890726327896118\n",
            "Train Epoch: 7 [8320/14166 (58.690744920993225%)]             Loss:0.2113906741142273            R_loss 6.500485420227051, KLD_loss 0.264016330242157\n",
            "Train Epoch: 7 [8640/14166 (60.94808126410835%)]             Loss:0.19889236986637115            R_loss 6.11669921875, KLD_loss 0.24785657227039337\n",
            "Train Epoch: 7 [8960/14166 (63.205417607223474%)]             Loss:0.20771737396717072            R_loss 6.431623458862305, KLD_loss 0.21533240377902985\n",
            "Train Epoch: 7 [9280/14166 (65.4627539503386%)]             Loss:0.20490016043186188            R_loss 6.358331203460693, KLD_loss 0.19847428798675537\n",
            "Train Epoch: 7 [9600/14166 (67.72009029345372%)]             Loss:0.2125745564699173            R_loss 6.4868364334106445, KLD_loss 0.3155496418476105\n",
            "Train Epoch: 7 [9920/14166 (69.97742663656885%)]             Loss:0.20867982506752014            R_loss 6.396066188812256, KLD_loss 0.28168824315071106\n",
            "Train Epoch: 7 [10240/14166 (72.23476297968398%)]             Loss:0.2029733657836914            R_loss 6.244106292724609, KLD_loss 0.25104108452796936\n",
            "Train Epoch: 7 [10560/14166 (74.49209932279909%)]             Loss:0.20609277486801147            R_loss 6.2991228103637695, KLD_loss 0.2958458364009857\n",
            "Train Epoch: 7 [10880/14166 (76.74943566591422%)]             Loss:0.20585185289382935            R_loss 6.254720211029053, KLD_loss 0.3325392007827759\n",
            "Train Epoch: 7 [11200/14166 (79.00677200902935%)]             Loss:0.20206396281719208            R_loss 6.182773590087891, KLD_loss 0.2832728624343872\n",
            "Train Epoch: 7 [11520/14166 (81.26410835214448%)]             Loss:0.2042660266160965            R_loss 6.285229206085205, KLD_loss 0.2512833774089813\n",
            "Train Epoch: 7 [11840/14166 (83.52144469525959%)]             Loss:0.2042209953069687            R_loss 6.357136249542236, KLD_loss 0.17793577909469604\n",
            "Train Epoch: 7 [12160/14166 (85.77878103837472%)]             Loss:0.21148192882537842            R_loss 6.454348087310791, KLD_loss 0.31307369470596313\n",
            "Train Epoch: 7 [12480/14166 (88.03611738148985%)]             Loss:0.20452401041984558            R_loss 6.323923110961914, KLD_loss 0.2208452671766281\n",
            "Train Epoch: 7 [12800/14166 (90.29345372460497%)]             Loss:0.20673848688602448            R_loss 6.387009143829346, KLD_loss 0.2286224663257599\n",
            "Train Epoch: 7 [13120/14166 (92.55079006772009%)]             Loss:0.20552653074264526            R_loss 6.365599632263184, KLD_loss 0.2112499326467514\n",
            "Train Epoch: 7 [13440/14166 (94.80812641083521%)]             Loss:0.21867680549621582            R_loss 6.675780773162842, KLD_loss 0.32187721133232117\n",
            "Train Epoch: 7 [13760/14166 (97.06546275395034%)]             Loss:0.21443596482276917            R_loss 6.6059889793396, KLD_loss 0.25596168637275696\n",
            "Train Epoch: 7 [14080/14166 (99.32279909706546%)]             Loss:0.21068382263183594            R_loss 6.434490203857422, KLD_loss 0.3073927164077759\n",
            "====> Epoch: 7 Average loss: 0.2094\n",
            "====> Test set loss: 0.2081\n",
            "Train Epoch: 8 [0/14166 (0.0%)]             Loss:0.19621042907238007            R_loss 6.05055570602417, KLD_loss 0.22817790508270264\n",
            "Train Epoch: 8 [320/14166 (2.2573363431151243%)]             Loss:0.21399205923080444            R_loss 6.447015762329102, KLD_loss 0.4007299244403839\n",
            "Train Epoch: 8 [640/14166 (4.514672686230249%)]             Loss:0.21153958141803741            R_loss 6.465484142303467, KLD_loss 0.30378299951553345\n",
            "Train Epoch: 8 [960/14166 (6.772009029345372%)]             Loss:0.22818945348262787            R_loss 6.895396709442139, KLD_loss 0.4066660702228546\n",
            "Train Epoch: 8 [1280/14166 (9.029345372460497%)]             Loss:0.20998252928256989            R_loss 6.377935409545898, KLD_loss 0.3415057361125946\n",
            "Train Epoch: 8 [1600/14166 (11.286681715575622%)]             Loss:0.20505371689796448            R_loss 6.340025901794434, KLD_loss 0.22169342637062073\n",
            "Train Epoch: 8 [1920/14166 (13.544018058690744%)]             Loss:0.20262818038463593            R_loss 6.2056193351745605, KLD_loss 0.2784830629825592\n",
            "Train Epoch: 8 [2240/14166 (15.801354401805868%)]             Loss:0.2059345543384552            R_loss 6.369828224182129, KLD_loss 0.22007744014263153\n",
            "Train Epoch: 8 [2560/14166 (18.058690744920995%)]             Loss:0.20361870527267456            R_loss 6.314302921295166, KLD_loss 0.20149573683738708\n",
            "Train Epoch: 8 [2880/14166 (20.31602708803612%)]             Loss:0.19677956402301788            R_loss 6.133249759674072, KLD_loss 0.16369633376598358\n",
            "Train Epoch: 8 [3200/14166 (22.573363431151243%)]             Loss:0.20929937064647675            R_loss 6.366243362426758, KLD_loss 0.33133670687675476\n",
            "Train Epoch: 8 [3520/14166 (24.830699774266364%)]             Loss:0.20206677913665771            R_loss 6.257716655731201, KLD_loss 0.20842070877552032\n",
            "Train Epoch: 8 [3840/14166 (27.08803611738149%)]             Loss:0.2253972589969635            R_loss 6.857303619384766, KLD_loss 0.35540878772735596\n",
            "Train Epoch: 8 [4160/14166 (29.345372460496613%)]             Loss:0.20363563299179077            R_loss 6.318617343902588, KLD_loss 0.19772282242774963\n",
            "Train Epoch: 8 [4480/14166 (31.602708803611737%)]             Loss:0.21869848668575287            R_loss 6.655059814453125, KLD_loss 0.34329211711883545\n",
            "Train Epoch: 8 [4800/14166 (33.86004514672686%)]             Loss:0.20185129344463348            R_loss 6.148864269256592, KLD_loss 0.3103767931461334\n",
            "Train Epoch: 8 [5120/14166 (36.11738148984199%)]             Loss:0.2140931785106659            R_loss 6.603263854980469, KLD_loss 0.24771748483181\n",
            "Train Epoch: 8 [5440/14166 (38.37471783295711%)]             Loss:0.20703373849391937            R_loss 6.308781623840332, KLD_loss 0.31629741191864014\n",
            "Train Epoch: 8 [5760/14166 (40.63205417607224%)]             Loss:0.21175891160964966            R_loss 6.530036926269531, KLD_loss 0.2462482750415802\n",
            "Train Epoch: 8 [6080/14166 (42.88939051918736%)]             Loss:0.21554197371006012            R_loss 6.560633659362793, KLD_loss 0.33670899271965027\n",
            "Train Epoch: 8 [6400/14166 (45.146726862302486%)]             Loss:0.22385422885417938            R_loss 6.774848937988281, KLD_loss 0.3884866237640381\n",
            "Train Epoch: 8 [6720/14166 (47.40406320541761%)]             Loss:0.19820770621299744            R_loss 6.068239212036133, KLD_loss 0.2744070887565613\n",
            "Train Epoch: 8 [7040/14166 (49.66139954853273%)]             Loss:0.21255037188529968            R_loss 6.438758373260498, KLD_loss 0.3628537356853485\n",
            "Train Epoch: 8 [7360/14166 (51.918735891647856%)]             Loss:0.19873999059200287            R_loss 6.149421215057373, KLD_loss 0.21025781333446503\n",
            "Train Epoch: 8 [7680/14166 (54.17607223476298%)]             Loss:0.20510362088680267            R_loss 6.318386554718018, KLD_loss 0.24492868781089783\n",
            "Train Epoch: 8 [8000/14166 (56.433408577878104%)]             Loss:0.19813169538974762            R_loss 6.147940158843994, KLD_loss 0.19227397441864014\n",
            "Train Epoch: 8 [8320/14166 (58.690744920993225%)]             Loss:0.19981512427330017            R_loss 6.182644367218018, KLD_loss 0.21143953502178192\n",
            "Train Epoch: 8 [8640/14166 (60.94808126410835%)]             Loss:0.20412933826446533            R_loss 6.306708812713623, KLD_loss 0.22542983293533325\n",
            "Train Epoch: 8 [8960/14166 (63.205417607223474%)]             Loss:0.21404778957366943            R_loss 6.571262836456299, KLD_loss 0.2782664895057678\n",
            "Train Epoch: 8 [9280/14166 (65.4627539503386%)]             Loss:0.2134113907814026            R_loss 6.4677324295043945, KLD_loss 0.36143219470977783\n",
            "Train Epoch: 8 [9600/14166 (67.72009029345372%)]             Loss:0.21637532114982605            R_loss 6.625267028808594, KLD_loss 0.2987431585788727\n",
            "Train Epoch: 8 [9920/14166 (69.97742663656885%)]             Loss:0.2120038866996765            R_loss 6.51991605758667, KLD_loss 0.264207661151886\n",
            "Train Epoch: 8 [10240/14166 (72.23476297968398%)]             Loss:0.21326619386672974            R_loss 6.564277172088623, KLD_loss 0.2602411210536957\n",
            "Train Epoch: 8 [10560/14166 (74.49209932279909%)]             Loss:0.21164017915725708            R_loss 6.5195512771606445, KLD_loss 0.2529336214065552\n",
            "Train Epoch: 8 [10880/14166 (76.74943566591422%)]             Loss:0.21121595799922943            R_loss 6.50613260269165, KLD_loss 0.2527783513069153\n",
            "Train Epoch: 8 [11200/14166 (79.00677200902935%)]             Loss:0.19598230719566345            R_loss 6.0979323387146, KLD_loss 0.1735013872385025\n",
            "Train Epoch: 8 [11520/14166 (81.26410835214448%)]             Loss:0.20093661546707153            R_loss 6.215210914611816, KLD_loss 0.21476022899150848\n",
            "Train Epoch: 8 [11840/14166 (83.52144469525959%)]             Loss:0.2144324779510498            R_loss 6.513605117797852, KLD_loss 0.3482345938682556\n",
            "Train Epoch: 8 [12160/14166 (85.77878103837472%)]             Loss:0.21421489119529724            R_loss 6.5742621421813965, KLD_loss 0.28061428666114807\n",
            "Train Epoch: 8 [12480/14166 (88.03611738148985%)]             Loss:0.21115261316299438            R_loss 6.443427085876465, KLD_loss 0.31345632672309875\n",
            "Train Epoch: 8 [12800/14166 (90.29345372460497%)]             Loss:0.21964144706726074            R_loss 6.78146505355835, KLD_loss 0.24706082046031952\n",
            "Train Epoch: 8 [13120/14166 (92.55079006772009%)]             Loss:0.20677393674850464            R_loss 6.350088119506836, KLD_loss 0.2666777968406677\n",
            "Train Epoch: 8 [13440/14166 (94.80812641083521%)]             Loss:0.19699977338314056            R_loss 6.082924842834473, KLD_loss 0.22106793522834778\n",
            "Train Epoch: 8 [13760/14166 (97.06546275395034%)]             Loss:0.2100088894367218            R_loss 6.453342914581299, KLD_loss 0.2669413983821869\n",
            "Train Epoch: 8 [14080/14166 (99.32279909706546%)]             Loss:0.20524287223815918            R_loss 6.345339775085449, KLD_loss 0.22243191301822662\n",
            "====> Epoch: 8 Average loss: 0.2082\n",
            "====> Test set loss: 0.2080\n",
            "Train Epoch: 9 [0/14166 (0.0%)]             Loss:0.20982933044433594            R_loss 6.45558500289917, KLD_loss 0.25895392894744873\n",
            "Train Epoch: 9 [320/14166 (2.2573363431151243%)]             Loss:0.2078004777431488            R_loss 6.317662239074707, KLD_loss 0.33195289969444275\n",
            "Train Epoch: 9 [640/14166 (4.514672686230249%)]             Loss:0.2076798379421234            R_loss 6.461277484893799, KLD_loss 0.18447735905647278\n",
            "Train Epoch: 9 [960/14166 (6.772009029345372%)]             Loss:0.21305806934833527            R_loss 6.567559242248535, KLD_loss 0.25029903650283813\n",
            "Train Epoch: 9 [1280/14166 (9.029345372460497%)]             Loss:0.20858646929264069            R_loss 6.418479919433594, KLD_loss 0.25628694891929626\n",
            "Train Epoch: 9 [1600/14166 (11.286681715575622%)]             Loss:0.2056836038827896            R_loss 6.404873847961426, KLD_loss 0.17700156569480896\n",
            "Train Epoch: 9 [1920/14166 (13.544018058690744%)]             Loss:0.2096119374036789            R_loss 6.422692775726318, KLD_loss 0.2848889231681824\n",
            "Train Epoch: 9 [2240/14166 (15.801354401805868%)]             Loss:0.20874202251434326            R_loss 6.386737823486328, KLD_loss 0.2930072546005249\n",
            "Train Epoch: 9 [2560/14166 (18.058690744920995%)]             Loss:0.2119905650615692            R_loss 6.542328357696533, KLD_loss 0.24136994779109955\n",
            "Train Epoch: 9 [2880/14166 (20.31602708803612%)]             Loss:0.2147894948720932            R_loss 6.636616230010986, KLD_loss 0.2366471290588379\n",
            "Train Epoch: 9 [3200/14166 (22.573363431151243%)]             Loss:0.20655763149261475            R_loss 6.332287788391113, KLD_loss 0.27755701541900635\n",
            "Train Epoch: 9 [3520/14166 (24.830699774266364%)]             Loss:0.1965658813714981            R_loss 6.049578666687012, KLD_loss 0.2405293732881546\n",
            "Train Epoch: 9 [3840/14166 (27.08803611738149%)]             Loss:0.21157313883304596            R_loss 6.438234329223633, KLD_loss 0.3321062922477722\n",
            "Train Epoch: 9 [4160/14166 (29.345372460496613%)]             Loss:0.20542503893375397            R_loss 6.345280647277832, KLD_loss 0.22832061350345612\n",
            "Train Epoch: 9 [4480/14166 (31.602708803611737%)]             Loss:0.20985284447669983            R_loss 6.403190612792969, KLD_loss 0.31210029125213623\n",
            "Train Epoch: 9 [4800/14166 (33.86004514672686%)]             Loss:0.2047014981508255            R_loss 6.356753826141357, KLD_loss 0.19369417428970337\n",
            "Train Epoch: 9 [5120/14166 (36.11738148984199%)]             Loss:0.20753055810928345            R_loss 6.348316192626953, KLD_loss 0.2926616370677948\n",
            "Train Epoch: 9 [5440/14166 (38.37471783295711%)]             Loss:0.20716845989227295            R_loss 6.415678024291992, KLD_loss 0.21371233463287354\n",
            "Train Epoch: 9 [5760/14166 (40.63205417607224%)]             Loss:0.2098923921585083            R_loss 6.459733963012695, KLD_loss 0.2568230628967285\n",
            "Train Epoch: 9 [6080/14166 (42.88939051918736%)]             Loss:0.20920245349407196            R_loss 6.34873628616333, KLD_loss 0.3457421362400055\n",
            "Train Epoch: 9 [6400/14166 (45.146726862302486%)]             Loss:0.211009219288826            R_loss 6.477184295654297, KLD_loss 0.27511104941368103\n",
            "Train Epoch: 9 [6720/14166 (47.40406320541761%)]             Loss:0.22231335937976837            R_loss 6.732543468475342, KLD_loss 0.381483793258667\n",
            "Train Epoch: 9 [7040/14166 (49.66139954853273%)]             Loss:0.20972658693790436            R_loss 6.500131130218506, KLD_loss 0.21111951768398285\n",
            "Train Epoch: 9 [7360/14166 (51.918735891647856%)]             Loss:0.2139674723148346            R_loss 6.539868354797363, KLD_loss 0.3070904016494751\n",
            "Train Epoch: 9 [7680/14166 (54.17607223476298%)]             Loss:0.20472262799739838            R_loss 6.330592155456543, KLD_loss 0.22053179144859314\n",
            "Train Epoch: 9 [8000/14166 (56.433408577878104%)]             Loss:0.20480988919734955            R_loss 6.27385950088501, KLD_loss 0.28005698323249817\n",
            "Train Epoch: 9 [8320/14166 (58.690744920993225%)]             Loss:0.2005709409713745            R_loss 6.186656475067139, KLD_loss 0.2316140979528427\n",
            "Train Epoch: 9 [8640/14166 (60.94808126410835%)]             Loss:0.204596146941185            R_loss 6.2802042961120605, KLD_loss 0.26687222719192505\n",
            "Train Epoch: 9 [8960/14166 (63.205417607223474%)]             Loss:0.20748308300971985            R_loss 6.32253360748291, KLD_loss 0.31692466139793396\n",
            "Train Epoch: 9 [9280/14166 (65.4627539503386%)]             Loss:0.21713484823703766            R_loss 6.587116718292236, KLD_loss 0.3611985146999359\n",
            "Train Epoch: 9 [9600/14166 (67.72009029345372%)]             Loss:0.21073195338249207            R_loss 6.442836761474609, KLD_loss 0.30058562755584717\n",
            "Train Epoch: 9 [9920/14166 (69.97742663656885%)]             Loss:0.2043619006872177            R_loss 6.292678356170654, KLD_loss 0.2469026744365692\n",
            "Train Epoch: 9 [10240/14166 (72.23476297968398%)]             Loss:0.2062447965145111            R_loss 6.330577850341797, KLD_loss 0.26925528049468994\n",
            "Train Epoch: 9 [10560/14166 (74.49209932279909%)]             Loss:0.20243781805038452            R_loss 6.198698043823242, KLD_loss 0.2793124318122864\n",
            "Train Epoch: 9 [10880/14166 (76.74943566591422%)]             Loss:0.2059093862771988            R_loss 6.33461856842041, KLD_loss 0.2544816732406616\n",
            "Train Epoch: 9 [11200/14166 (79.00677200902935%)]             Loss:0.20102378726005554            R_loss 6.236688613891602, KLD_loss 0.19607236981391907\n",
            "Train Epoch: 9 [11520/14166 (81.26410835214448%)]             Loss:0.1928267627954483            R_loss 6.000457286834717, KLD_loss 0.16999918222427368\n",
            "Train Epoch: 9 [11840/14166 (83.52144469525959%)]             Loss:0.2061987966299057            R_loss 6.29517126083374, KLD_loss 0.3031906187534332\n",
            "Train Epoch: 9 [12160/14166 (85.77878103837472%)]             Loss:0.206649512052536            R_loss 6.377322673797607, KLD_loss 0.2354615032672882\n",
            "Train Epoch: 9 [12480/14166 (88.03611738148985%)]             Loss:0.20960162580013275            R_loss 6.383152961730957, KLD_loss 0.3240991234779358\n",
            "Train Epoch: 9 [12800/14166 (90.29345372460497%)]             Loss:0.20481225848197937            R_loss 6.31792688369751, KLD_loss 0.2360657900571823\n",
            "Train Epoch: 9 [13120/14166 (92.55079006772009%)]             Loss:0.20770032703876495            R_loss 6.412797451019287, KLD_loss 0.233613520860672\n",
            "Train Epoch: 9 [13440/14166 (94.80812641083521%)]             Loss:0.216078519821167            R_loss 6.547775745391846, KLD_loss 0.3667365312576294\n",
            "Train Epoch: 9 [13760/14166 (97.06546275395034%)]             Loss:0.21548548340797424            R_loss 6.569786071777344, KLD_loss 0.32574981451034546\n",
            "Train Epoch: 9 [14080/14166 (99.32279909706546%)]             Loss:0.19950322806835175            R_loss 6.163465976715088, KLD_loss 0.22063690423965454\n",
            "====> Epoch: 9 Average loss: 0.2078\n",
            "====> Test set loss: 0.2076\n",
            "Train Epoch: 10 [0/14166 (0.0%)]             Loss:0.21366553008556366            R_loss 6.505645275115967, KLD_loss 0.33165186643600464\n",
            "Train Epoch: 10 [320/14166 (2.2573363431151243%)]             Loss:0.2068174183368683            R_loss 6.32146692276001, KLD_loss 0.29669076204299927\n",
            "Train Epoch: 10 [640/14166 (4.514672686230249%)]             Loss:0.20218390226364136            R_loss 6.232903480529785, KLD_loss 0.23698100447654724\n",
            "Train Epoch: 10 [960/14166 (6.772009029345372%)]             Loss:0.20042076706886292            R_loss 6.155358791351318, KLD_loss 0.2581060230731964\n",
            "Train Epoch: 10 [1280/14166 (9.029345372460497%)]             Loss:0.204412043094635            R_loss 6.251865386962891, KLD_loss 0.2893202602863312\n",
            "Train Epoch: 10 [1600/14166 (11.286681715575622%)]             Loss:0.20117723941802979            R_loss 6.248193740844727, KLD_loss 0.1894775629043579\n",
            "Train Epoch: 10 [1920/14166 (13.544018058690744%)]             Loss:0.19945254921913147            R_loss 6.102251052856445, KLD_loss 0.28023046255111694\n",
            "Train Epoch: 10 [2240/14166 (15.801354401805868%)]             Loss:0.20754025876522064            R_loss 6.373137474060059, KLD_loss 0.2681511640548706\n",
            "Train Epoch: 10 [2560/14166 (18.058690744920995%)]             Loss:0.1984710395336151            R_loss 6.1471991539001465, KLD_loss 0.20387443900108337\n",
            "Train Epoch: 10 [2880/14166 (20.31602708803612%)]             Loss:0.20470185577869415            R_loss 6.329563617706299, KLD_loss 0.2208954244852066\n",
            "Train Epoch: 10 [3200/14166 (22.573363431151243%)]             Loss:0.20218990743160248            R_loss 6.184327125549316, KLD_loss 0.28574997186660767\n",
            "Train Epoch: 10 [3520/14166 (24.830699774266364%)]             Loss:0.21786276996135712            R_loss 6.6877641677856445, KLD_loss 0.28384432196617126\n",
            "Train Epoch: 10 [3840/14166 (27.08803611738149%)]             Loss:0.20128846168518066            R_loss 6.19614315032959, KLD_loss 0.24508747458457947\n",
            "Train Epoch: 10 [4160/14166 (29.345372460496613%)]             Loss:0.20779547095298767            R_loss 6.380620002746582, KLD_loss 0.26883500814437866\n",
            "Train Epoch: 10 [4480/14166 (31.602708803611737%)]             Loss:0.21777725219726562            R_loss 6.672577857971191, KLD_loss 0.29629456996917725\n",
            "Train Epoch: 10 [4800/14166 (33.86004514672686%)]             Loss:0.22138002514839172            R_loss 6.676273822784424, KLD_loss 0.40788736939430237\n",
            "Train Epoch: 10 [5120/14166 (36.11738148984199%)]             Loss:0.20303818583488464            R_loss 6.212376594543457, KLD_loss 0.28484559059143066\n",
            "Train Epoch: 10 [5440/14166 (38.37471783295711%)]             Loss:0.2121550589799881            R_loss 6.502775192260742, KLD_loss 0.2861865758895874\n",
            "Train Epoch: 10 [5760/14166 (40.63205417607224%)]             Loss:0.21637754142284393            R_loss 6.569783687591553, KLD_loss 0.3542974889278412\n",
            "Train Epoch: 10 [6080/14166 (42.88939051918736%)]             Loss:0.19993633031845093            R_loss 6.150481224060059, KLD_loss 0.24748167395591736\n",
            "Train Epoch: 10 [6400/14166 (45.146726862302486%)]             Loss:0.2118169665336609            R_loss 6.433457374572754, KLD_loss 0.34468573331832886\n",
            "Train Epoch: 10 [6720/14166 (47.40406320541761%)]             Loss:0.20818829536437988            R_loss 6.366815090179443, KLD_loss 0.29521068930625916\n",
            "Train Epoch: 10 [7040/14166 (49.66139954853273%)]             Loss:0.22182828187942505            R_loss 6.664497375488281, KLD_loss 0.4340078830718994\n",
            "Train Epoch: 10 [7360/14166 (51.918735891647856%)]             Loss:0.20285557210445404            R_loss 6.248208522796631, KLD_loss 0.24316953122615814\n",
            "Train Epoch: 10 [7680/14166 (54.17607223476298%)]             Loss:0.20627880096435547            R_loss 6.356262683868408, KLD_loss 0.24465852975845337\n",
            "Train Epoch: 10 [8000/14166 (56.433408577878104%)]             Loss:0.20072230696678162            R_loss 6.147038459777832, KLD_loss 0.27607548236846924\n",
            "Train Epoch: 10 [8320/14166 (58.690744920993225%)]             Loss:0.2111312597990036            R_loss 6.437095642089844, KLD_loss 0.31910431385040283\n",
            "Train Epoch: 10 [8640/14166 (60.94808126410835%)]             Loss:0.20788739621639252            R_loss 6.354228973388672, KLD_loss 0.29816731810569763\n",
            "Train Epoch: 10 [8960/14166 (63.205417607223474%)]             Loss:0.20767581462860107            R_loss 6.368212699890137, KLD_loss 0.2774134874343872\n",
            "Train Epoch: 10 [9280/14166 (65.4627539503386%)]             Loss:0.21154026687145233            R_loss 6.500279426574707, KLD_loss 0.2690095603466034\n",
            "Train Epoch: 10 [9600/14166 (67.72009029345372%)]             Loss:0.20299914479255676            R_loss 6.271819591522217, KLD_loss 0.22415263950824738\n",
            "Train Epoch: 10 [9920/14166 (69.97742663656885%)]             Loss:0.2035950869321823            R_loss 6.233774185180664, KLD_loss 0.28126880526542664\n",
            "Train Epoch: 10 [10240/14166 (72.23476297968398%)]             Loss:0.21867036819458008            R_loss 6.746723651885986, KLD_loss 0.2507280707359314\n",
            "Train Epoch: 10 [10560/14166 (74.49209932279909%)]             Loss:0.20715075731277466            R_loss 6.372104167938232, KLD_loss 0.2567201852798462\n",
            "Train Epoch: 10 [10880/14166 (76.74943566591422%)]             Loss:0.20567810535430908            R_loss 6.3408098220825195, KLD_loss 0.24089014530181885\n",
            "Train Epoch: 10 [11200/14166 (79.00677200902935%)]             Loss:0.21223533153533936            R_loss 6.561394691467285, KLD_loss 0.2301359474658966\n",
            "Train Epoch: 10 [11520/14166 (81.26410835214448%)]             Loss:0.19613884389400482            R_loss 6.123443126678467, KLD_loss 0.15300016105175018\n",
            "Train Epoch: 10 [11840/14166 (83.52144469525959%)]             Loss:0.2043437361717224            R_loss 6.293751239776611, KLD_loss 0.245248481631279\n",
            "Train Epoch: 10 [12160/14166 (85.77878103837472%)]             Loss:0.20548798143863678            R_loss 6.298607349395752, KLD_loss 0.2770079970359802\n",
            "Train Epoch: 10 [12480/14166 (88.03611738148985%)]             Loss:0.22191683948040009            R_loss 6.698494911193848, KLD_loss 0.40284448862075806\n",
            "Train Epoch: 10 [12800/14166 (90.29345372460497%)]             Loss:0.2134152501821518            R_loss 6.549468517303467, KLD_loss 0.2798195481300354\n",
            "Train Epoch: 10 [13120/14166 (92.55079006772009%)]             Loss:0.20878124237060547            R_loss 6.467774391174316, KLD_loss 0.21322500705718994\n",
            "Train Epoch: 10 [13440/14166 (94.80812641083521%)]             Loss:0.20146679878234863            R_loss 6.231060981750488, KLD_loss 0.21587711572647095\n",
            "Train Epoch: 10 [13760/14166 (97.06546275395034%)]             Loss:0.2040780484676361            R_loss 6.279889106750488, KLD_loss 0.2506081759929657\n",
            "Train Epoch: 10 [14080/14166 (99.32279909706546%)]             Loss:0.20552708208560944            R_loss 6.230607509613037, KLD_loss 0.3462592661380768\n",
            "====> Epoch: 10 Average loss: 0.2073\n",
            "====> Test set loss: 0.2074\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  print(\"testing encoding\")\n",
        "  data = next(iter(test_loader))\n",
        "  print(data[0][0])\n",
        "  recon_batch, mu, logvar, gauss_z, dir_z = model(data[0])\n",
        "  print(recon_batch[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57g2SgMSQjPY",
        "outputId": "8fd270df-1311-4ae4-e3a9-46bf5bf81d43"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "testing encoding\n",
            "tensor([0.6984, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000])\n",
            "tensor([2.6903e-01, 8.0843e-05, 9.2014e-05, 4.0962e-05, 1.0109e-04, 4.2155e-05,\n",
            "        5.5653e-05, 5.4061e-05, 7.1064e-01, 6.1565e-05, 8.4037e-05, 4.3361e-05,\n",
            "        6.0945e-05, 1.3606e-04, 6.8994e-05, 3.8872e-05, 6.5831e-05, 3.5773e-05,\n",
            "        7.5131e-05, 6.7254e-05, 4.8592e-05, 4.0222e-05, 7.3991e-05, 3.9799e-04,\n",
            "        5.1316e-05, 9.3815e-05, 4.9631e-05, 6.6410e-05, 4.8605e-01, 9.9993e-01,\n",
            "        3.5088e-05, 4.5985e-05, 1.7122e-04, 1.1755e-04, 9.9993e-01, 9.9996e-01])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L3NKOc3mHlIP"
      },
      "execution_count": 32,
      "outputs": []
    }
  ]
}