{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JosephThompson607/dir_vae/blob/main/sepsis_vae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "phRvmO49rUUT"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we prepare the data for training"
      ],
      "metadata": {
        "id": "0SiJSH5EMXTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: Read this from the cloud\n",
        "patients = pd.read_csv(\"/content/unique_patient_dem.csv\")\n",
        "\n",
        "patients.drop(columns=['subject_id'], inplace=True)\n",
        "numeric_cols = patients.select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_cols = patients.select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "# Reorder DataFrame\n",
        "patients = patients[numeric_cols + categorical_cols]\n",
        "#1 hot encoding\n",
        "df_encoded = pd.get_dummies(patients, columns=['race', 'gender'])\n",
        "\n",
        "#If cuda is available, device is cuda, otherwise cpu\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "scaler = StandardScaler()\n",
        "df_encoded['anchor_age'] = scaler.fit_transform(df_encoded[['anchor_age']])\n",
        "features = df_encoded.astype('float32').values\n",
        "# print(features.columns)\n",
        "# print(features.dtypes)\n",
        "# Get indices for slicing\n",
        "num_indices = list(range(len(numeric_cols)))\n",
        "n_numeric = len(numeric_cols)\n",
        "cat_indices = list(range(len(numeric_cols), len(features)))\n",
        "tensor = torch.tensor(features, dtype=torch.float32)\n",
        "\n",
        "X_train, X_test = train_test_split(tensor, test_size=0.2, random_state=42)\n",
        "\n",
        "train_dataset = TensorDataset(X_train)  # or (X_train, y_train)\n",
        "test_dataset = TensorDataset(X_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "input_size = X_train[0].shape[0] #input size is the number of features going into the network\n",
        "print(input_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4j5hmBSfrZhl",
        "outputId": "6258ce92-f637-4337-fda1-07cb81b31c01"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we define the model and related functions"
      ],
      "metadata": {
        "id": "HeWHLkFqMPmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ngf = 64\n",
        "ndf = 64\n",
        "nc = 1\n",
        "\n",
        "def prior(K, alpha):\n",
        "    \"\"\"\n",
        "    Prior for the model.\n",
        "    :K: number of categories\n",
        "    :alpha: Hyper param of Dir\n",
        "    :return: mean and variance tensors\n",
        "    \"\"\"\n",
        "    # ラプラス近似で正規分布に近似\n",
        "    # Approximate to normal distribution using Laplace approximation\n",
        "    a = torch.Tensor(1, K).float().fill_(alpha)\n",
        "    mean = a.log().t() - a.log().mean(1)\n",
        "    var = ((1 - 2.0 / K) * a.reciprocal()).t() + (1.0 / K ** 2) * a.reciprocal().sum(1)\n",
        "    return mean.t(), var.t() # Parameters of prior distribution after approximation\n",
        "\n",
        "class Dir_VAE(nn.Module):\n",
        "    def __init__(self, input_size,n_numeric, latent_size=10, hidden_dim = 200):\n",
        "        self.num_numeric_cols = n_numeric\n",
        "        self.latent_size = latent_size\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.input_size = input_size\n",
        "        super(Dir_VAE, self).__init__()\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "          nn.Linear(self.input_size, self.hidden_dim),\n",
        "          nn.ReLU(),\n",
        "          # nn.Linear(self.hidden_dim, self.hidden_dim),\n",
        "          # nn.ReLU(),\n",
        "          # nn.Linear(self.hidden_dim, self.hidden_dim),\n",
        "          # nn.ReLU()\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "          nn.Linear(self.latent_size, self.hidden_dim),\n",
        "          nn.ReLU(),\n",
        "          # nn.Linear(self.hidden_dim, self.hidden_dim),\n",
        "          # nn.ReLU(),\n",
        "          # nn.Linear(self.hidden_dim, self.hidden_dim),\n",
        "          # nn.ReLU(),\n",
        "          nn.Linear(self.hidden_dim, self.input_size),\n",
        "\n",
        "          # nn.Unflatten(dim=1, unflattened_size=(1, 28, 28)) # This was for image data\n",
        "        )\n",
        "        #self.fc1 = nn.Linear(self.hidden_dim, 512)\n",
        "        self.fc21 = nn.Linear(self.hidden_dim, self.latent_size)\n",
        "        self.fc22 = nn.Linear(self.hidden_dim, self.latent_size)\n",
        "\n",
        "        #self.fc3 = nn.Linear(10, 512)\n",
        "        #self.fc4 = nn.Linear(512, self.hidden_dim)\n",
        "\n",
        "        self.lrelu = nn.LeakyReLU()\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Dir prior\n",
        "        self.prior_mean, self.prior_var = map(nn.Parameter, prior(self.latent_size, 0.3)) # 0.3 is a hyper param of Dirichlet distribution\n",
        "        self.prior_logvar = nn.Parameter(self.prior_var.log())\n",
        "        self.prior_mean.requires_grad = False\n",
        "        self.prior_var.requires_grad = False\n",
        "        self.prior_logvar.requires_grad = False\n",
        "\n",
        "\n",
        "    def encode(self, x):\n",
        "        encoding = self.encoder(x);\n",
        "        #h1 = self.fc1(encoding)\n",
        "        return self.fc21(encoding), self.fc22(encoding)\n",
        "\n",
        "    def decode(self, gauss_z):\n",
        "        dir_z = F.softmax(gauss_z,dim=1) #Reduntant, already done in forward\n",
        "        # This variable (z) can be treated as a variable that follows a Dirichlet distribution (a variable that can be interpreted as a probability that the sum is 1)\n",
        "        # Use the Softmax function to satisfy the simplex constraint\n",
        "        x_out = self.decoder(dir_z)\n",
        "        # Apply sigmoid to categorical output only\n",
        "        x_out[:, self.num_numeric_cols:] = torch.sigmoid(x_out[:, self.num_numeric_cols:])\n",
        "        return x_out\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps*std\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        gauss_z = self.reparameterize(mu, logvar)\n",
        "        # gause_z is a variable that follows a multivariate normal distribution\n",
        "        # Inputting gause_z into softmax func yields a random variable that follows a Dirichlet distribution (Softmax func are used in decoder)\n",
        "        dir_z = F.softmax(gauss_z,dim=1) # This variable follows a Dirichlet distribution\n",
        "        return self.decode(gauss_z), mu, logvar, gauss_z, dir_z\n",
        "\n",
        "    def reconstruction_loss(self, x_true, x_recon):\n",
        "        # Slice the tensors\n",
        "        x_true_num = x_true[:, :self.num_numeric_cols]\n",
        "        x_true_cat = x_true[:, self.num_numeric_cols:]\n",
        "\n",
        "        x_recon_num = x_recon[:, :self.num_numeric_cols]\n",
        "        x_recon_cat = x_recon[:, self.num_numeric_cols:]\n",
        "\n",
        "        # Compute losses\n",
        "        num_loss = F.mse_loss(x_recon_num, x_true_num)\n",
        "        cat_loss = F.cross_entropy(x_recon_cat, x_true_cat)\n",
        "\n",
        "        return num_loss + cat_loss\n",
        "\n",
        "    # Reconstruction + KL divergence losses summed over all elements and batch\n",
        "    def loss_function(self, recon_x, x, mu, logvar):\n",
        "        # Apply sigmoid to the input data x to ensure values are between 0 and 1\n",
        "        recon_loss = self.reconstruction_loss(x, recon_x, )\n",
        "        # ディリクレ事前分布と変分事後分布とのKLを計算\n",
        "        # Calculating KL with Dirichlet prior and variational posterior distributions\n",
        "        # Original paper:\"Autoencodeing variational inference for topic model\"-https://arxiv.org/pdf/1703.01488\n",
        "        prior_mean = self.prior_mean.expand_as(mu)\n",
        "        prior_var = self.prior_var.expand_as(logvar)\n",
        "        prior_logvar = self.prior_logvar.expand_as(logvar)\n",
        "        var_division = logvar.exp() / prior_var # Σ_0 / Σ_1\n",
        "        diff = mu - prior_mean # μ_１ - μ_0\n",
        "        diff_term = diff *diff / prior_var # (μ_1 - μ_0)(μ_1 - μ_0)/Σ_1\n",
        "        logvar_division = prior_logvar - logvar # log|Σ_1| - log|Σ_0| = log(|Σ_1|/|Σ_2|)\n",
        "        # KL\n",
        "        KLD = 0.5 * ((var_division + diff_term + logvar_division).sum(1) - self.latent_size)\n",
        "        self.last_KLD = torch.mean(KLD) #Used for reporting\n",
        "        self.last_BCE = recon_loss\n",
        "        return recon_loss + KLD"
      ],
      "metadata": {
        "id": "jFoNCRlVMNud"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below are the training and test loop\n"
      ],
      "metadata": {
        "id": "EQ-82vOxMqkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = Dir_VAE(input_size, n_numeric, latent_size=2, hidden_dim=20).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data,) in enumerate(train_loader): # Unpack only one element\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar, gauss_z, dir_z = model(data)\n",
        "\n",
        "        loss = model.loss_function(recon_batch, data, mu, logvar, )\n",
        "        loss = loss.mean()\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 10 == 0:\n",
        "            #print(f\"gause_z:{gauss_z[0]}\")\n",
        "            #print(f\"dir_z:{dir_z[0]},SUM:{torch.sum(dir_z[0])}\")\n",
        "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader)}%)] \\\n",
        "            Loss:{loss.item() / len(data)}\\\n",
        "            R_loss {model.last_BCE}, KLD_loss {model.last_KLD}')\n",
        "\n",
        "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
        "          epoch, train_loss / len(train_loader.dataset)))\n",
        "\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for i, (data,) in enumerate(test_loader): # Unpack only one element\n",
        "            data = data.to(device)\n",
        "            recon_batch, mu, logvar, gauss_z, dir_z = model(data)\n",
        "            loss = model.loss_function(recon_batch, data, mu, logvar)\n",
        "            test_loss += loss.mean()\n",
        "            test_loss.item()\n",
        "            # if i == 0:\n",
        "            #     n = min(data.size(0), 18)\n",
        "            #     #comparison = torch.cat([data[:n],\n",
        "            #     #                      recon_batch.view(args.batch_size, 1, 28, 28)[:n]])\n",
        "            #     #save_image(comparison.cpu(),\n",
        "            #     #         'image/recon_' + str(epoch) + '.png', nrow=n)\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('====> Test set loss: {:.4f}'.format(test_loss))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 学習(Train)\n",
        "    for epoch in range(1, 10 + 1):\n",
        "        train(epoch)\n",
        "        test(epoch)\n",
        "        #with torch.no_grad():\n",
        "            #sample = torch.randn(64, args.category).to(device)\n",
        "            #sample = model.decode(sample).cpu()\n",
        "            #save_image(sample.view(64, 1, 28, 28),'image/sample_' + str(epoch) + '.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgOw9CXb2p_l",
        "outputId": "768c1d3f-29e5-4d0d-d3b1-a997e0cee3c6"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/14166 (0.0%)]             Loss:0.2674112617969513            R_loss 8.469375610351562, KLD_loss 0.08778417110443115\n",
            "Train Epoch: 1 [320/14166 (2.2573363431151243%)]             Loss:0.2533678710460663            R_loss 8.04049301147461, KLD_loss 0.0672779530286789\n",
            "Train Epoch: 1 [640/14166 (4.514672686230249%)]             Loss:0.2529548108577728            R_loss 8.049893379211426, KLD_loss 0.04466040059924126\n",
            "Train Epoch: 1 [960/14166 (6.772009029345372%)]             Loss:0.25442609190940857            R_loss 8.108431816101074, KLD_loss 0.03320269286632538\n",
            "Train Epoch: 1 [1280/14166 (9.029345372460497%)]             Loss:0.24856720864772797            R_loss 7.9317169189453125, KLD_loss 0.022433705627918243\n",
            "Train Epoch: 1 [1600/14166 (11.286681715575622%)]             Loss:0.2527603507041931            R_loss 8.076199531555176, KLD_loss 0.012132454663515091\n",
            "Train Epoch: 1 [1920/14166 (13.544018058690744%)]             Loss:0.25334155559539795            R_loss 8.096527099609375, KLD_loss 0.010402806103229523\n",
            "Train Epoch: 1 [2240/14166 (15.801354401805868%)]             Loss:0.2440582513809204            R_loss 7.802545547485352, KLD_loss 0.007318656891584396\n",
            "Train Epoch: 1 [2560/14166 (18.058690744920995%)]             Loss:0.2457638680934906            R_loss 7.860095977783203, KLD_loss 0.0043479278683662415\n",
            "Train Epoch: 1 [2880/14166 (20.31602708803612%)]             Loss:0.23069319128990173            R_loss 7.379595756530762, KLD_loss 0.002586808055639267\n",
            "Train Epoch: 1 [3200/14166 (22.573363431151243%)]             Loss:0.23406732082366943            R_loss 7.48573112487793, KLD_loss 0.0044236257672309875\n",
            "Train Epoch: 1 [3520/14166 (24.830699774266364%)]             Loss:0.23759643733501434            R_loss 7.600600719451904, KLD_loss 0.0024851374328136444\n",
            "Train Epoch: 1 [3840/14166 (27.08803611738149%)]             Loss:0.24380068480968475            R_loss 7.798857688903809, KLD_loss 0.002764463424682617\n",
            "Train Epoch: 1 [4160/14166 (29.345372460496613%)]             Loss:0.2419893592596054            R_loss 7.741018295288086, KLD_loss 0.002640865743160248\n",
            "Train Epoch: 1 [4480/14166 (31.602708803611737%)]             Loss:0.22566306591033936            R_loss 7.219266891479492, KLD_loss 0.0019517764449119568\n",
            "Train Epoch: 1 [4800/14166 (33.86004514672686%)]             Loss:0.22498349845409393            R_loss 7.1971282958984375, KLD_loss 0.0023433901369571686\n",
            "Train Epoch: 1 [5120/14166 (36.11738148984199%)]             Loss:0.2158207893371582            R_loss 6.904839992523193, KLD_loss 0.0014252550899982452\n",
            "Train Epoch: 1 [5440/14166 (38.37471783295711%)]             Loss:0.22219547629356384            R_loss 7.108374118804932, KLD_loss 0.0018810294568538666\n",
            "Train Epoch: 1 [5760/14166 (40.63205417607224%)]             Loss:0.22579649090766907            R_loss 7.222798824310303, KLD_loss 0.002689030021429062\n",
            "Train Epoch: 1 [6080/14166 (42.88939051918736%)]             Loss:0.23227548599243164            R_loss 7.430344581604004, KLD_loss 0.0024705566465854645\n",
            "Train Epoch: 1 [6400/14166 (45.146726862302486%)]             Loss:0.21474789083003998            R_loss 6.869978427886963, KLD_loss 0.0019540712237358093\n",
            "Train Epoch: 1 [6720/14166 (47.40406320541761%)]             Loss:0.2228538691997528            R_loss 7.128536701202393, KLD_loss 0.0027868635952472687\n",
            "Train Epoch: 1 [7040/14166 (49.66139954853273%)]             Loss:0.21768520772457123            R_loss 6.964441776275635, KLD_loss 0.0014848411083221436\n",
            "Train Epoch: 1 [7360/14166 (51.918735891647856%)]             Loss:0.2185332030057907            R_loss 6.991374969482422, KLD_loss 0.0016878806054592133\n",
            "Train Epoch: 1 [7680/14166 (54.17607223476298%)]             Loss:0.21094384789466858            R_loss 6.748642444610596, KLD_loss 0.0015609189867973328\n",
            "Train Epoch: 1 [8000/14166 (56.433408577878104%)]             Loss:0.222306489944458            R_loss 7.112161636352539, KLD_loss 0.0016461722552776337\n",
            "Train Epoch: 1 [8320/14166 (58.690744920993225%)]             Loss:0.20413541793823242            R_loss 6.5312347412109375, KLD_loss 0.001098591834306717\n",
            "Train Epoch: 1 [8640/14166 (60.94808126410835%)]             Loss:0.20966848731040955            R_loss 6.707937717437744, KLD_loss 0.001453939825296402\n",
            "Train Epoch: 1 [8960/14166 (63.205417607223474%)]             Loss:0.22170822322368622            R_loss 7.091981887817383, KLD_loss 0.002681579440832138\n",
            "Train Epoch: 1 [9280/14166 (65.4627539503386%)]             Loss:0.20102781057357788            R_loss 6.431496620178223, KLD_loss 0.001393347978591919\n",
            "Train Epoch: 1 [9600/14166 (67.72009029345372%)]             Loss:0.20536096394062042            R_loss 6.569696426391602, KLD_loss 0.0018542669713497162\n",
            "Train Epoch: 1 [9920/14166 (69.97742663656885%)]             Loss:0.2100849598646164            R_loss 6.720602512359619, KLD_loss 0.0021165721118450165\n",
            "Train Epoch: 1 [10240/14166 (72.23476297968398%)]             Loss:0.20563183724880219            R_loss 6.578742980957031, KLD_loss 0.0014759339392185211\n",
            "Train Epoch: 1 [10560/14166 (74.49209932279909%)]             Loss:0.21752792596817017            R_loss 6.958980560302734, KLD_loss 0.0019134841859340668\n",
            "Train Epoch: 1 [10880/14166 (76.74943566591422%)]             Loss:0.21599650382995605            R_loss 6.909476280212402, KLD_loss 0.0024122335016727448\n",
            "Train Epoch: 1 [11200/14166 (79.00677200902935%)]             Loss:0.21378043293952942            R_loss 6.838657379150391, KLD_loss 0.0023158863186836243\n",
            "Train Epoch: 1 [11520/14166 (81.26410835214448%)]             Loss:0.20439653098583221            R_loss 6.539061546325684, KLD_loss 0.0016272924840450287\n",
            "Train Epoch: 1 [11840/14166 (83.52144469525959%)]             Loss:0.20864495635032654            R_loss 6.674056053161621, KLD_loss 0.0025827214121818542\n",
            "Train Epoch: 1 [12160/14166 (85.77878103837472%)]             Loss:0.21130108833312988            R_loss 6.758382320404053, KLD_loss 0.003252144902944565\n",
            "Train Epoch: 1 [12480/14166 (88.03611738148985%)]             Loss:0.21600402891635895            R_loss 6.908199310302734, KLD_loss 0.003929413855075836\n",
            "Train Epoch: 1 [12800/14166 (90.29345372460497%)]             Loss:0.20841413736343384            R_loss 6.665813446044922, KLD_loss 0.0034388527274131775\n",
            "Train Epoch: 1 [13120/14166 (92.55079006772009%)]             Loss:0.21163241565227509            R_loss 6.767861366271973, KLD_loss 0.004375617951154709\n",
            "Train Epoch: 1 [13440/14166 (94.80812641083521%)]             Loss:0.21733355522155762            R_loss 6.948686122894287, KLD_loss 0.0059881024062633514\n",
            "Train Epoch: 1 [13760/14166 (97.06546275395034%)]             Loss:0.208290234208107            R_loss 6.659854888916016, KLD_loss 0.005432609468698502\n",
            "Train Epoch: 1 [14080/14166 (99.32279909706546%)]             Loss:0.21539297699928284            R_loss 6.887088298797607, KLD_loss 0.005486536771059036\n",
            "====> Epoch: 1 Average loss: 0.2268\n",
            "====> Test set loss: 0.2127\n",
            "Train Epoch: 2 [0/14166 (0.0%)]             Loss:0.2100830078125            R_loss 6.716799259185791, KLD_loss 0.0058571770787239075\n",
            "Train Epoch: 2 [320/14166 (2.2573363431151243%)]             Loss:0.21191340684890747            R_loss 6.775453567504883, KLD_loss 0.005775488913059235\n",
            "Train Epoch: 2 [640/14166 (4.514672686230249%)]             Loss:0.2007177770137787            R_loss 6.4184088706970215, KLD_loss 0.004559684544801712\n",
            "Train Epoch: 2 [960/14166 (6.772009029345372%)]             Loss:0.21192654967308044            R_loss 6.773865699768066, KLD_loss 0.0077843330800533295\n",
            "Train Epoch: 2 [1280/14166 (9.029345372460497%)]             Loss:0.22995975613594055            R_loss 7.347832679748535, KLD_loss 0.01087917760014534\n",
            "Train Epoch: 2 [1600/14166 (11.286681715575622%)]             Loss:0.20595993101596832            R_loss 6.585272312164307, KLD_loss 0.005444969981908798\n",
            "Train Epoch: 2 [1920/14166 (13.544018058690744%)]             Loss:0.21217243373394012            R_loss 6.782081604003906, KLD_loss 0.007436320185661316\n",
            "Train Epoch: 2 [2240/14166 (15.801354401805868%)]             Loss:0.21689701080322266            R_loss 6.931662559509277, KLD_loss 0.00904141366481781\n",
            "Train Epoch: 2 [2560/14166 (18.058690744920995%)]             Loss:0.20882421731948853            R_loss 6.674586296081543, KLD_loss 0.007789060473442078\n",
            "Train Epoch: 2 [2880/14166 (20.31602708803612%)]             Loss:0.22737914323806763            R_loss 7.258738040924072, KLD_loss 0.017394427210092545\n",
            "Train Epoch: 2 [3200/14166 (22.573363431151243%)]             Loss:0.20409588515758514            R_loss 6.519804000854492, KLD_loss 0.011264726519584656\n",
            "Train Epoch: 2 [3520/14166 (24.830699774266364%)]             Loss:0.2061941921710968            R_loss 6.588178634643555, KLD_loss 0.010035466402769089\n",
            "Train Epoch: 2 [3840/14166 (27.08803611738149%)]             Loss:0.211873859167099            R_loss 6.764822006225586, KLD_loss 0.015141420066356659\n",
            "Train Epoch: 2 [4160/14166 (29.345372460496613%)]             Loss:0.21908973157405853            R_loss 6.993704795837402, KLD_loss 0.017166167497634888\n",
            "Train Epoch: 2 [4480/14166 (31.602708803611737%)]             Loss:0.21304850280284882            R_loss 6.8034539222717285, KLD_loss 0.014098096638917923\n",
            "Train Epoch: 2 [4800/14166 (33.86004514672686%)]             Loss:0.21186479926109314            R_loss 6.759828567504883, KLD_loss 0.01984531804919243\n",
            "Train Epoch: 2 [5120/14166 (36.11738148984199%)]             Loss:0.2066553235054016            R_loss 6.594820022583008, KLD_loss 0.01815025508403778\n",
            "Train Epoch: 2 [5440/14166 (38.37471783295711%)]             Loss:0.21058540046215057            R_loss 6.721836566925049, KLD_loss 0.016895964741706848\n",
            "Train Epoch: 2 [5760/14166 (40.63205417607224%)]             Loss:0.21382150053977966            R_loss 6.816584587097168, KLD_loss 0.02570297196507454\n",
            "Train Epoch: 2 [6080/14166 (42.88939051918736%)]             Loss:0.21889728307724            R_loss 6.975193500518799, KLD_loss 0.0295199453830719\n",
            "Train Epoch: 2 [6400/14166 (45.146726862302486%)]             Loss:0.22050559520721436            R_loss 7.024563312530518, KLD_loss 0.03161580115556717\n",
            "Train Epoch: 2 [6720/14166 (47.40406320541761%)]             Loss:0.20994694530963898            R_loss 6.691828727722168, KLD_loss 0.02647322043776512\n",
            "Train Epoch: 2 [7040/14166 (49.66139954853273%)]             Loss:0.2217676043510437            R_loss 7.056613445281982, KLD_loss 0.039950065314769745\n",
            "Train Epoch: 2 [7360/14166 (51.918735891647856%)]             Loss:0.21495361626148224            R_loss 6.846442222595215, KLD_loss 0.03207387402653694\n",
            "Train Epoch: 2 [7680/14166 (54.17607223476298%)]             Loss:0.21792049705982208            R_loss 6.93098258972168, KLD_loss 0.04247348755598068\n",
            "Train Epoch: 2 [8000/14166 (56.433408577878104%)]             Loss:0.21592438220977783            R_loss 6.872901439666748, KLD_loss 0.03667892888188362\n",
            "Train Epoch: 2 [8320/14166 (58.690744920993225%)]             Loss:0.20980650186538696            R_loss 6.678214073181152, KLD_loss 0.03559451550245285\n",
            "Train Epoch: 2 [8640/14166 (60.94808126410835%)]             Loss:0.21160700917243958            R_loss 6.731053352355957, KLD_loss 0.040370840579271317\n",
            "Train Epoch: 2 [8960/14166 (63.205417607223474%)]             Loss:0.20910502970218658            R_loss 6.649075508117676, KLD_loss 0.04228561744093895\n",
            "Train Epoch: 2 [9280/14166 (65.4627539503386%)]             Loss:0.2200259268283844            R_loss 6.9813079833984375, KLD_loss 0.059520889073610306\n",
            "Train Epoch: 2 [9600/14166 (67.72009029345372%)]             Loss:0.22470146417617798            R_loss 7.124900817871094, KLD_loss 0.06554624438285828\n",
            "Train Epoch: 2 [9920/14166 (69.97742663656885%)]             Loss:0.22409072518348694            R_loss 7.100260257720947, KLD_loss 0.07064268738031387\n",
            "Train Epoch: 2 [10240/14166 (72.23476297968398%)]             Loss:0.2071402668952942            R_loss 6.572154998779297, KLD_loss 0.05633358284831047\n",
            "Train Epoch: 2 [10560/14166 (74.49209932279909%)]             Loss:0.21511922776699066            R_loss 6.8114094734191895, KLD_loss 0.07240553200244904\n",
            "Train Epoch: 2 [10880/14166 (76.74943566591422%)]             Loss:0.21151448786258698            R_loss 6.710510730743408, KLD_loss 0.057953234761953354\n",
            "Train Epoch: 2 [11200/14166 (79.00677200902935%)]             Loss:0.2038109004497528            R_loss 6.457365989685059, KLD_loss 0.06458263844251633\n",
            "Train Epoch: 2 [11520/14166 (81.26410835214448%)]             Loss:0.20912563800811768            R_loss 6.631168365478516, KLD_loss 0.060851648449897766\n",
            "Train Epoch: 2 [11840/14166 (83.52144469525959%)]             Loss:0.21754677593708038            R_loss 6.877327919006348, KLD_loss 0.08416837453842163\n",
            "Train Epoch: 2 [12160/14166 (85.77878103837472%)]             Loss:0.2115895301103592            R_loss 6.705370903015137, KLD_loss 0.06549406051635742\n",
            "Train Epoch: 2 [12480/14166 (88.03611738148985%)]             Loss:0.20955635607242584            R_loss 6.62921142578125, KLD_loss 0.07659196853637695\n",
            "Train Epoch: 2 [12800/14166 (90.29345372460497%)]             Loss:0.2008284330368042            R_loss 6.35812520980835, KLD_loss 0.06838471442461014\n",
            "Train Epoch: 2 [13120/14166 (92.55079006772009%)]             Loss:0.22202639281749725            R_loss 6.962985992431641, KLD_loss 0.14185857772827148\n",
            "Train Epoch: 2 [13440/14166 (94.80812641083521%)]             Loss:0.2062612920999527            R_loss 6.492089748382568, KLD_loss 0.10827173292636871\n",
            "Train Epoch: 2 [13760/14166 (97.06546275395034%)]             Loss:0.22045373916625977            R_loss 6.944121837615967, KLD_loss 0.11039812862873077\n",
            "Train Epoch: 2 [14080/14166 (99.32279909706546%)]             Loss:0.20422936975955963            R_loss 6.449928283691406, KLD_loss 0.08541157096624374\n",
            "====> Epoch: 2 Average loss: 0.2114\n",
            "====> Test set loss: 0.2096\n",
            "Train Epoch: 3 [0/14166 (0.0%)]             Loss:0.1987609714269638            R_loss 6.288392543792725, KLD_loss 0.07195885479450226\n",
            "Train Epoch: 3 [320/14166 (2.2573363431151243%)]             Loss:0.20883765816688538            R_loss 6.5536885261535645, KLD_loss 0.12911614775657654\n",
            "Train Epoch: 3 [640/14166 (4.514672686230249%)]             Loss:0.21556180715560913            R_loss 6.759142875671387, KLD_loss 0.13883453607559204\n",
            "Train Epoch: 3 [960/14166 (6.772009029345372%)]             Loss:0.2053246647119522            R_loss 6.475542068481445, KLD_loss 0.09484736621379852\n",
            "Train Epoch: 3 [1280/14166 (9.029345372460497%)]             Loss:0.21144676208496094            R_loss 6.624440670013428, KLD_loss 0.14185583591461182\n",
            "Train Epoch: 3 [1600/14166 (11.286681715575622%)]             Loss:0.20627370476722717            R_loss 6.486775875091553, KLD_loss 0.11398254334926605\n",
            "Train Epoch: 3 [1920/14166 (13.544018058690744%)]             Loss:0.21709394454956055            R_loss 6.78947639465332, KLD_loss 0.15753021836280823\n",
            "Train Epoch: 3 [2240/14166 (15.801354401805868%)]             Loss:0.19632670283317566            R_loss 6.195990562438965, KLD_loss 0.08646409958600998\n",
            "Train Epoch: 3 [2560/14166 (18.058690744920995%)]             Loss:0.21492235362529755            R_loss 6.723824501037598, KLD_loss 0.1536906212568283\n",
            "Train Epoch: 3 [2880/14166 (20.31602708803612%)]             Loss:0.1965445727109909            R_loss 6.18626594543457, KLD_loss 0.10316065698862076\n",
            "Train Epoch: 3 [3200/14166 (22.573363431151243%)]             Loss:0.21843776106834412            R_loss 6.79537296295166, KLD_loss 0.19463588297367096\n",
            "Train Epoch: 3 [3520/14166 (24.830699774266364%)]             Loss:0.20283499360084534            R_loss 6.377870559692383, KLD_loss 0.11284923553466797\n",
            "Train Epoch: 3 [3840/14166 (27.08803611738149%)]             Loss:0.20474249124526978            R_loss 6.418704032897949, KLD_loss 0.13305610418319702\n",
            "Train Epoch: 3 [4160/14166 (29.345372460496613%)]             Loss:0.20807313919067383            R_loss 6.505040168762207, KLD_loss 0.1532999724149704\n",
            "Train Epoch: 3 [4480/14166 (31.602708803611737%)]             Loss:0.21130068600177765            R_loss 6.5511884689331055, KLD_loss 0.21043342351913452\n",
            "Train Epoch: 3 [4800/14166 (33.86004514672686%)]             Loss:0.20263703167438507            R_loss 6.309926986694336, KLD_loss 0.17445750534534454\n",
            "Train Epoch: 3 [5120/14166 (36.11738148984199%)]             Loss:0.21802382171154022            R_loss 6.753638744354248, KLD_loss 0.22312390804290771\n",
            "Train Epoch: 3 [5440/14166 (38.37471783295711%)]             Loss:0.21430672705173492            R_loss 6.651516914367676, KLD_loss 0.20629873871803284\n",
            "Train Epoch: 3 [5760/14166 (40.63205417607224%)]             Loss:0.20708508789539337            R_loss 6.3867058753967285, KLD_loss 0.24001725018024445\n",
            "Train Epoch: 3 [6080/14166 (42.88939051918736%)]             Loss:0.20798355340957642            R_loss 6.506730079650879, KLD_loss 0.1487438827753067\n",
            "Train Epoch: 3 [6400/14166 (45.146726862302486%)]             Loss:0.20590096712112427            R_loss 6.398928165435791, KLD_loss 0.18990235030651093\n",
            "Train Epoch: 3 [6720/14166 (47.40406320541761%)]             Loss:0.19912302494049072            R_loss 6.257302284240723, KLD_loss 0.11463461816310883\n",
            "Train Epoch: 3 [7040/14166 (49.66139954853273%)]             Loss:0.20098188519477844            R_loss 6.264326095581055, KLD_loss 0.1670946180820465\n",
            "Train Epoch: 3 [7360/14166 (51.918735891647856%)]             Loss:0.2028304487466812            R_loss 6.2966742515563965, KLD_loss 0.1939004361629486\n",
            "Train Epoch: 3 [7680/14166 (54.17607223476298%)]             Loss:0.22440354526042938            R_loss 6.931373596191406, KLD_loss 0.24954015016555786\n",
            "Train Epoch: 3 [8000/14166 (56.433408577878104%)]             Loss:0.21563740074634552            R_loss 6.605267524719238, KLD_loss 0.2951294183731079\n",
            "Train Epoch: 3 [8320/14166 (58.690744920993225%)]             Loss:0.2107037901878357            R_loss 6.4556884765625, KLD_loss 0.28683310747146606\n",
            "Train Epoch: 3 [8640/14166 (60.94808126410835%)]             Loss:0.21100865304470062            R_loss 6.535184860229492, KLD_loss 0.21709167957305908\n",
            "Train Epoch: 3 [8960/14166 (63.205417607223474%)]             Loss:0.2097150683403015            R_loss 6.499709606170654, KLD_loss 0.21117281913757324\n",
            "Train Epoch: 3 [9280/14166 (65.4627539503386%)]             Loss:0.22048626840114594            R_loss 6.776961803436279, KLD_loss 0.2785983383655548\n",
            "Train Epoch: 3 [9600/14166 (67.72009029345372%)]             Loss:0.20024333894252777            R_loss 6.248900413513184, KLD_loss 0.15888670086860657\n",
            "Train Epoch: 3 [9920/14166 (69.97742663656885%)]             Loss:0.22346210479736328            R_loss 6.881180763244629, KLD_loss 0.2696070671081543\n",
            "Train Epoch: 3 [10240/14166 (72.23476297968398%)]             Loss:0.20857110619544983            R_loss 6.437521457672119, KLD_loss 0.23675408959388733\n",
            "Train Epoch: 3 [10560/14166 (74.49209932279909%)]             Loss:0.20567071437835693            R_loss 6.333403587341309, KLD_loss 0.24805954098701477\n",
            "Train Epoch: 3 [10880/14166 (76.74943566591422%)]             Loss:0.20053936541080475            R_loss 6.203750133514404, KLD_loss 0.2135097235441208\n",
            "Train Epoch: 3 [11200/14166 (79.00677200902935%)]             Loss:0.2071092128753662            R_loss 6.429399490356445, KLD_loss 0.19809561967849731\n",
            "Train Epoch: 3 [11520/14166 (81.26410835214448%)]             Loss:0.20633728802204132            R_loss 6.3798065185546875, KLD_loss 0.22298642992973328\n",
            "Train Epoch: 3 [11840/14166 (83.52144469525959%)]             Loss:0.20732198655605316            R_loss 6.37194299697876, KLD_loss 0.26236099004745483\n",
            "Train Epoch: 3 [12160/14166 (85.77878103837472%)]             Loss:0.21664316952228546            R_loss 6.651051044464111, KLD_loss 0.2815310060977936\n",
            "Train Epoch: 3 [12480/14166 (88.03611738148985%)]             Loss:0.21346427500247955            R_loss 6.564568519592285, KLD_loss 0.2662883996963501\n",
            "Train Epoch: 3 [12800/14166 (90.29345372460497%)]             Loss:0.21392491459846497            R_loss 6.566707134246826, KLD_loss 0.2788900136947632\n",
            "Train Epoch: 3 [13120/14166 (92.55079006772009%)]             Loss:0.20363759994506836            R_loss 6.233466625213623, KLD_loss 0.28293633460998535\n",
            "Train Epoch: 3 [13440/14166 (94.80812641083521%)]             Loss:0.2042720913887024            R_loss 6.35235071182251, KLD_loss 0.18435657024383545\n",
            "Train Epoch: 3 [13760/14166 (97.06546275395034%)]             Loss:0.2095092236995697            R_loss 6.433835983276367, KLD_loss 0.27045923471450806\n",
            "Train Epoch: 3 [14080/14166 (99.32279909706546%)]             Loss:0.20600579679012299            R_loss 6.3524065017700195, KLD_loss 0.23977887630462646\n",
            "====> Epoch: 3 Average loss: 0.2089\n",
            "====> Test set loss: 0.2081\n",
            "Train Epoch: 4 [0/14166 (0.0%)]             Loss:0.21251972019672394            R_loss 6.509039402008057, KLD_loss 0.2915918827056885\n",
            "Train Epoch: 4 [320/14166 (2.2573363431151243%)]             Loss:0.2016371190547943            R_loss 6.250079154968262, KLD_loss 0.20230865478515625\n",
            "Train Epoch: 4 [640/14166 (4.514672686230249%)]             Loss:0.20325331389904022            R_loss 6.2428178787231445, KLD_loss 0.2612884044647217\n",
            "Train Epoch: 4 [960/14166 (6.772009029345372%)]             Loss:0.2101743519306183            R_loss 6.46500825881958, KLD_loss 0.26057079434394836\n",
            "Train Epoch: 4 [1280/14166 (9.029345372460497%)]             Loss:0.21188779175281525            R_loss 6.470335006713867, KLD_loss 0.31007471680641174\n",
            "Train Epoch: 4 [1600/14166 (11.286681715575622%)]             Loss:0.20207130908966064            R_loss 6.194949626922607, KLD_loss 0.271331787109375\n",
            "Train Epoch: 4 [1920/14166 (13.544018058690744%)]             Loss:0.20497797429561615            R_loss 6.307291507720947, KLD_loss 0.2520034611225128\n",
            "Train Epoch: 4 [2240/14166 (15.801354401805868%)]             Loss:0.20098049938678741            R_loss 6.182137966156006, KLD_loss 0.24923792481422424\n",
            "Train Epoch: 4 [2560/14166 (18.058690744920995%)]             Loss:0.21151123940944672            R_loss 6.429570198059082, KLD_loss 0.3387894034385681\n",
            "Train Epoch: 4 [2880/14166 (20.31602708803612%)]             Loss:0.21332797408103943            R_loss 6.534017562866211, KLD_loss 0.29247793555259705\n",
            "Train Epoch: 4 [3200/14166 (22.573363431151243%)]             Loss:0.20870599150657654            R_loss 6.422561168670654, KLD_loss 0.2560305893421173\n",
            "Train Epoch: 4 [3520/14166 (24.830699774266364%)]             Loss:0.20902013778686523            R_loss 6.375529766082764, KLD_loss 0.3131146728992462\n",
            "Train Epoch: 4 [3840/14166 (27.08803611738149%)]             Loss:0.20978714525699615            R_loss 6.402203559875488, KLD_loss 0.3109849989414215\n",
            "Train Epoch: 4 [4160/14166 (29.345372460496613%)]             Loss:0.20459744334220886            R_loss 6.328226566314697, KLD_loss 0.21889150142669678\n",
            "Train Epoch: 4 [4480/14166 (31.602708803611737%)]             Loss:0.21245624125003815            R_loss 6.534248352050781, KLD_loss 0.26435139775276184\n",
            "Train Epoch: 4 [4800/14166 (33.86004514672686%)]             Loss:0.21468505263328552            R_loss 6.590734004974365, KLD_loss 0.27918750047683716\n",
            "Train Epoch: 4 [5120/14166 (36.11738148984199%)]             Loss:0.18870198726654053            R_loss 5.897746562957764, KLD_loss 0.1407172977924347\n",
            "Train Epoch: 4 [5440/14166 (38.37471783295711%)]             Loss:0.20653383433818817            R_loss 6.345457077026367, KLD_loss 0.2636258602142334\n",
            "Train Epoch: 4 [5760/14166 (40.63205417607224%)]             Loss:0.21847295761108398            R_loss 6.735114574432373, KLD_loss 0.2560196816921234\n",
            "Train Epoch: 4 [6080/14166 (42.88939051918736%)]             Loss:0.20103329420089722            R_loss 6.2233123779296875, KLD_loss 0.20975270867347717\n",
            "Train Epoch: 4 [6400/14166 (45.146726862302486%)]             Loss:0.21116295456886292            R_loss 6.5151448249816895, KLD_loss 0.24206960201263428\n",
            "Train Epoch: 4 [6720/14166 (47.40406320541761%)]             Loss:0.21195648610591888            R_loss 6.510385036468506, KLD_loss 0.272222101688385\n",
            "Train Epoch: 4 [7040/14166 (49.66139954853273%)]             Loss:0.2182319462299347            R_loss 6.662003517150879, KLD_loss 0.3214178681373596\n",
            "Train Epoch: 4 [7360/14166 (51.918735891647856%)]             Loss:0.19845059514045715            R_loss 6.176708698272705, KLD_loss 0.17371037602424622\n",
            "Train Epoch: 4 [7680/14166 (54.17607223476298%)]             Loss:0.21399642527103424            R_loss 6.555727958679199, KLD_loss 0.2921578586101532\n",
            "Train Epoch: 4 [8000/14166 (56.433408577878104%)]             Loss:0.20994818210601807            R_loss 6.4347333908081055, KLD_loss 0.2836088240146637\n",
            "Train Epoch: 4 [8320/14166 (58.690744920993225%)]             Loss:0.20213162899017334            R_loss 6.289993762969971, KLD_loss 0.17821797728538513\n",
            "Train Epoch: 4 [8640/14166 (60.94808126410835%)]             Loss:0.2116963118314743            R_loss 6.514131546020508, KLD_loss 0.26015040278434753\n",
            "Train Epoch: 4 [8960/14166 (63.205417607223474%)]             Loss:0.2001955658197403            R_loss 6.152775764465332, KLD_loss 0.2534828186035156\n",
            "Train Epoch: 4 [9280/14166 (65.4627539503386%)]             Loss:0.21274061501026154            R_loss 6.5762038230896, KLD_loss 0.231495663523674\n",
            "Train Epoch: 4 [9600/14166 (67.72009029345372%)]             Loss:0.20129838585853577            R_loss 6.202446460723877, KLD_loss 0.23910221457481384\n",
            "Train Epoch: 4 [9920/14166 (69.97742663656885%)]             Loss:0.20366042852401733            R_loss 6.319904327392578, KLD_loss 0.19722938537597656\n",
            "Train Epoch: 4 [10240/14166 (72.23476297968398%)]             Loss:0.20932139456272125            R_loss 6.493300437927246, KLD_loss 0.20498374104499817\n",
            "Train Epoch: 4 [10560/14166 (74.49209932279909%)]             Loss:0.20555880665779114            R_loss 6.289984703063965, KLD_loss 0.28789758682250977\n",
            "Train Epoch: 4 [10880/14166 (76.74943566591422%)]             Loss:0.2181723713874817            R_loss 6.676953315734863, KLD_loss 0.3045625388622284\n",
            "Train Epoch: 4 [11200/14166 (79.00677200902935%)]             Loss:0.21123255789279938            R_loss 6.4245991706848145, KLD_loss 0.3348422348499298\n",
            "Train Epoch: 4 [11520/14166 (81.26410835214448%)]             Loss:0.2022654116153717            R_loss 6.22211217880249, KLD_loss 0.25038057565689087\n",
            "Train Epoch: 4 [11840/14166 (83.52144469525959%)]             Loss:0.2110910564661026            R_loss 6.490318775177002, KLD_loss 0.26459556818008423\n",
            "Train Epoch: 4 [12160/14166 (85.77878103837472%)]             Loss:0.20600810647010803            R_loss 6.295703411102295, KLD_loss 0.29655584692955017\n",
            "Train Epoch: 4 [12480/14166 (88.03611738148985%)]             Loss:0.21116197109222412            R_loss 6.442605018615723, KLD_loss 0.3145778775215149\n",
            "Train Epoch: 4 [12800/14166 (90.29345372460497%)]             Loss:0.2048644870519638            R_loss 6.320382595062256, KLD_loss 0.23528096079826355\n",
            "Train Epoch: 4 [13120/14166 (92.55079006772009%)]             Loss:0.19945916533470154            R_loss 6.1801557540893555, KLD_loss 0.2025374174118042\n",
            "Train Epoch: 4 [13440/14166 (94.80812641083521%)]             Loss:0.207915797829628            R_loss 6.34575891494751, KLD_loss 0.3075462877750397\n",
            "Train Epoch: 4 [13760/14166 (97.06546275395034%)]             Loss:0.2030026912689209            R_loss 6.26547908782959, KLD_loss 0.23060733079910278\n",
            "Train Epoch: 4 [14080/14166 (99.32279909706546%)]             Loss:0.21118874847888947            R_loss 6.5247802734375, KLD_loss 0.23325994610786438\n",
            "====> Epoch: 4 Average loss: 0.2082\n",
            "====> Test set loss: 0.2079\n",
            "Train Epoch: 5 [0/14166 (0.0%)]             Loss:0.2089727371931076            R_loss 6.410187721252441, KLD_loss 0.2769396901130676\n",
            "Train Epoch: 5 [320/14166 (2.2573363431151243%)]             Loss:0.21699224412441254            R_loss 6.587583065032959, KLD_loss 0.356168657541275\n",
            "Train Epoch: 5 [640/14166 (4.514672686230249%)]             Loss:0.2173699289560318            R_loss 6.594402313232422, KLD_loss 0.3614353835582733\n",
            "Train Epoch: 5 [960/14166 (6.772009029345372%)]             Loss:0.20046734809875488            R_loss 6.152095794677734, KLD_loss 0.2628592252731323\n",
            "Train Epoch: 5 [1280/14166 (9.029345372460497%)]             Loss:0.20189779996871948            R_loss 6.254416465759277, KLD_loss 0.2063133716583252\n",
            "Train Epoch: 5 [1600/14166 (11.286681715575622%)]             Loss:0.20706163346767426            R_loss 6.347613334655762, KLD_loss 0.2783588171005249\n",
            "Train Epoch: 5 [1920/14166 (13.544018058690744%)]             Loss:0.20616059005260468            R_loss 6.360358715057373, KLD_loss 0.23678022623062134\n",
            "Train Epoch: 5 [2240/14166 (15.801354401805868%)]             Loss:0.22318795323371887            R_loss 6.832283020019531, KLD_loss 0.30973154306411743\n",
            "Train Epoch: 5 [2560/14166 (18.058690744920995%)]             Loss:0.19721722602844238            R_loss 6.126554489135742, KLD_loss 0.1843963861465454\n",
            "Train Epoch: 5 [2880/14166 (20.31602708803612%)]             Loss:0.206316739320755            R_loss 6.371607780456543, KLD_loss 0.23052826523780823\n",
            "Train Epoch: 5 [3200/14166 (22.573363431151243%)]             Loss:0.20519332587718964            R_loss 6.33499002456665, KLD_loss 0.231196790933609\n",
            "Train Epoch: 5 [3520/14166 (24.830699774266364%)]             Loss:0.21002903580665588            R_loss 6.454246520996094, KLD_loss 0.266682893037796\n",
            "Train Epoch: 5 [3840/14166 (27.08803611738149%)]             Loss:0.2142467498779297            R_loss 6.493594169616699, KLD_loss 0.362301766872406\n",
            "Train Epoch: 5 [4160/14166 (29.345372460496613%)]             Loss:0.21998514235019684            R_loss 6.706151485443115, KLD_loss 0.33337268233299255\n",
            "Train Epoch: 5 [4480/14166 (31.602708803611737%)]             Loss:0.20500478148460388            R_loss 6.398166179656982, KLD_loss 0.16198688745498657\n",
            "Train Epoch: 5 [4800/14166 (33.86004514672686%)]             Loss:0.20219847559928894            R_loss 6.214677810668945, KLD_loss 0.2556734085083008\n",
            "Train Epoch: 5 [5120/14166 (36.11738148984199%)]             Loss:0.2094356268644333            R_loss 6.482254981994629, KLD_loss 0.21968504786491394\n",
            "Train Epoch: 5 [5440/14166 (38.37471783295711%)]             Loss:0.20367449522018433            R_loss 6.244121551513672, KLD_loss 0.27346271276474\n",
            "Train Epoch: 5 [5760/14166 (40.63205417607224%)]             Loss:0.20327894389629364            R_loss 6.216128349304199, KLD_loss 0.2887977957725525\n",
            "Train Epoch: 5 [6080/14166 (42.88939051918736%)]             Loss:0.2045329362154007            R_loss 6.282200813293457, KLD_loss 0.2628532350063324\n",
            "Train Epoch: 5 [6400/14166 (45.146726862302486%)]             Loss:0.19862379133701324            R_loss 6.180135726928711, KLD_loss 0.1758255511522293\n",
            "Train Epoch: 5 [6720/14166 (47.40406320541761%)]             Loss:0.211610347032547            R_loss 6.523473262786865, KLD_loss 0.24805808067321777\n",
            "Train Epoch: 5 [7040/14166 (49.66139954853273%)]             Loss:0.21145176887512207            R_loss 6.477649688720703, KLD_loss 0.2888070344924927\n",
            "Train Epoch: 5 [7360/14166 (51.918735891647856%)]             Loss:0.203739196062088            R_loss 6.2724761962890625, KLD_loss 0.24717751145362854\n",
            "Train Epoch: 5 [7680/14166 (54.17607223476298%)]             Loss:0.20862367749214172            R_loss 6.376832962036133, KLD_loss 0.29912447929382324\n",
            "Train Epoch: 5 [8000/14166 (56.433408577878104%)]             Loss:0.20771583914756775            R_loss 6.360496520996094, KLD_loss 0.28640973567962646\n",
            "Train Epoch: 5 [8320/14166 (58.690744920993225%)]             Loss:0.2044009417295456            R_loss 6.290421485900879, KLD_loss 0.25040924549102783\n",
            "Train Epoch: 5 [8640/14166 (60.94808126410835%)]             Loss:0.20140385627746582            R_loss 6.213172912597656, KLD_loss 0.2317507266998291\n",
            "Train Epoch: 5 [8960/14166 (63.205417607223474%)]             Loss:0.2137010395526886            R_loss 6.534709453582764, KLD_loss 0.30372345447540283\n",
            "Train Epoch: 5 [9280/14166 (65.4627539503386%)]             Loss:0.200123593211174            R_loss 6.193816184997559, KLD_loss 0.21013802289962769\n",
            "Train Epoch: 5 [9600/14166 (67.72009029345372%)]             Loss:0.20979516208171844            R_loss 6.442555904388428, KLD_loss 0.2708897590637207\n",
            "Train Epoch: 5 [9920/14166 (69.97742663656885%)]             Loss:0.2064731866121292            R_loss 6.370784282684326, KLD_loss 0.23635733127593994\n",
            "Train Epoch: 5 [10240/14166 (72.23476297968398%)]             Loss:0.22346659004688263            R_loss 6.823724269866943, KLD_loss 0.32720646262168884\n",
            "Train Epoch: 5 [10560/14166 (74.49209932279909%)]             Loss:0.21867769956588745            R_loss 6.695413112640381, KLD_loss 0.302273154258728\n",
            "Train Epoch: 5 [10880/14166 (76.74943566591422%)]             Loss:0.21412479877471924            R_loss 6.516422271728516, KLD_loss 0.3355708420276642\n",
            "Train Epoch: 5 [11200/14166 (79.00677200902935%)]             Loss:0.21192309260368347            R_loss 6.535519123077393, KLD_loss 0.24601973593235016\n",
            "Train Epoch: 5 [11520/14166 (81.26410835214448%)]             Loss:0.20634803175926208            R_loss 6.247372627258301, KLD_loss 0.35576462745666504\n",
            "Train Epoch: 5 [11840/14166 (83.52144469525959%)]             Loss:0.21807439625263214            R_loss 6.691295623779297, KLD_loss 0.2870851457118988\n",
            "Train Epoch: 5 [12160/14166 (85.77878103837472%)]             Loss:0.21992884576320648            R_loss 6.736507892608643, KLD_loss 0.3012149930000305\n",
            "Train Epoch: 5 [12480/14166 (88.03611738148985%)]             Loss:0.1961967945098877            R_loss 6.064320087432861, KLD_loss 0.21397718787193298\n",
            "Train Epoch: 5 [12800/14166 (90.29345372460497%)]             Loss:0.21337862312793732            R_loss 6.511543273925781, KLD_loss 0.31657227873802185\n",
            "Train Epoch: 5 [13120/14166 (92.55079006772009%)]             Loss:0.20865580439567566            R_loss 6.4012227058410645, KLD_loss 0.27576303482055664\n",
            "Train Epoch: 5 [13440/14166 (94.80812641083521%)]             Loss:0.21573899686336517            R_loss 6.556317329406738, KLD_loss 0.3473309278488159\n",
            "Train Epoch: 5 [13760/14166 (97.06546275395034%)]             Loss:0.2041095793247223            R_loss 6.31981897354126, KLD_loss 0.21168707311153412\n",
            "Train Epoch: 5 [14080/14166 (99.32279909706546%)]             Loss:0.20518019795417786            R_loss 6.333897113800049, KLD_loss 0.2318693995475769\n",
            "====> Epoch: 5 Average loss: 0.2083\n",
            "====> Test set loss: 0.2075\n",
            "Train Epoch: 6 [0/14166 (0.0%)]             Loss:0.21084816753864288            R_loss 6.4600043296813965, KLD_loss 0.28713715076446533\n",
            "Train Epoch: 6 [320/14166 (2.2573363431151243%)]             Loss:0.20511692762374878            R_loss 6.375707149505615, KLD_loss 0.18803466856479645\n",
            "Train Epoch: 6 [640/14166 (4.514672686230249%)]             Loss:0.20547668635845184            R_loss 6.307431697845459, KLD_loss 0.267821729183197\n",
            "Train Epoch: 6 [960/14166 (6.772009029345372%)]             Loss:0.20098616182804108            R_loss 6.242278099060059, KLD_loss 0.1892789602279663\n",
            "Train Epoch: 6 [1280/14166 (9.029345372460497%)]             Loss:0.20968535542488098            R_loss 6.454689025878906, KLD_loss 0.2552425265312195\n",
            "Train Epoch: 6 [1600/14166 (11.286681715575622%)]             Loss:0.20121832191944122            R_loss 6.168636798858643, KLD_loss 0.2703495919704437\n",
            "Train Epoch: 6 [1920/14166 (13.544018058690744%)]             Loss:0.19924643635749817            R_loss 6.134321212768555, KLD_loss 0.24156464636325836\n",
            "Train Epoch: 6 [2240/14166 (15.801354401805868%)]             Loss:0.1997041404247284            R_loss 6.196070671081543, KLD_loss 0.19446201622486115\n",
            "Train Epoch: 6 [2560/14166 (18.058690744920995%)]             Loss:0.2050202637910843            R_loss 6.3155317306518555, KLD_loss 0.2451169192790985\n",
            "Train Epoch: 6 [2880/14166 (20.31602708803612%)]             Loss:0.20609097182750702            R_loss 6.3357648849487305, KLD_loss 0.2591462731361389\n",
            "Train Epoch: 6 [3200/14166 (22.573363431151243%)]             Loss:0.20875567197799683            R_loss 6.441308975219727, KLD_loss 0.23887239396572113\n",
            "Train Epoch: 6 [3520/14166 (24.830699774266364%)]             Loss:0.2046961486339569            R_loss 6.290783882141113, KLD_loss 0.2594926655292511\n",
            "Train Epoch: 6 [3840/14166 (27.08803611738149%)]             Loss:0.2166881114244461            R_loss 6.690402984619141, KLD_loss 0.2436167448759079\n",
            "Train Epoch: 6 [4160/14166 (29.345372460496613%)]             Loss:0.21339847147464752            R_loss 6.5001420974731445, KLD_loss 0.3286086618900299\n",
            "Train Epoch: 6 [4480/14166 (31.602708803611737%)]             Loss:0.2061018943786621            R_loss 6.392012596130371, KLD_loss 0.20324833691120148\n",
            "Train Epoch: 6 [4800/14166 (33.86004514672686%)]             Loss:0.207718163728714            R_loss 6.358293533325195, KLD_loss 0.28868788480758667\n",
            "Train Epoch: 6 [5120/14166 (36.11738148984199%)]             Loss:0.20899561047554016            R_loss 6.4687981605529785, KLD_loss 0.21906116604804993\n",
            "Train Epoch: 6 [5440/14166 (38.37471783295711%)]             Loss:0.20511801540851593            R_loss 6.36628532409668, KLD_loss 0.19749116897583008\n",
            "Train Epoch: 6 [5760/14166 (40.63205417607224%)]             Loss:0.20009052753448486            R_loss 6.172969341278076, KLD_loss 0.22992703318595886\n",
            "Train Epoch: 6 [6080/14166 (42.88939051918736%)]             Loss:0.2022910714149475            R_loss 6.185240745544434, KLD_loss 0.28807333111763\n",
            "Train Epoch: 6 [6400/14166 (45.146726862302486%)]             Loss:0.21492478251457214            R_loss 6.591168403625488, KLD_loss 0.2864244878292084\n",
            "Train Epoch: 6 [6720/14166 (47.40406320541761%)]             Loss:0.20080946385860443            R_loss 6.202610969543457, KLD_loss 0.22329187393188477\n",
            "Train Epoch: 6 [7040/14166 (49.66139954853273%)]             Loss:0.20637141168117523            R_loss 6.3535356521606445, KLD_loss 0.2503495216369629\n",
            "Train Epoch: 6 [7360/14166 (51.918735891647856%)]             Loss:0.21010205149650574            R_loss 6.425026893615723, KLD_loss 0.2982386648654938\n",
            "Train Epoch: 6 [7680/14166 (54.17607223476298%)]             Loss:0.21117661893367767            R_loss 6.509774684906006, KLD_loss 0.2478770613670349\n",
            "Train Epoch: 6 [8000/14166 (56.433408577878104%)]             Loss:0.20233912765979767            R_loss 6.240729808807373, KLD_loss 0.23412178456783295\n",
            "Train Epoch: 6 [8320/14166 (58.690744920993225%)]             Loss:0.20639030635356903            R_loss 6.355025291442871, KLD_loss 0.24946458637714386\n",
            "Train Epoch: 6 [8640/14166 (60.94808126410835%)]             Loss:0.2072160392999649            R_loss 6.395475387573242, KLD_loss 0.23543831706047058\n",
            "Train Epoch: 6 [8960/14166 (63.205417607223474%)]             Loss:0.20079265534877777            R_loss 6.199225425720215, KLD_loss 0.2261401116847992\n",
            "Train Epoch: 6 [9280/14166 (65.4627539503386%)]             Loss:0.2041926383972168            R_loss 6.2250566482543945, KLD_loss 0.30910825729370117\n",
            "Train Epoch: 6 [9600/14166 (67.72009029345372%)]             Loss:0.20437271893024445            R_loss 6.276528835296631, KLD_loss 0.26339849829673767\n",
            "Train Epoch: 6 [9920/14166 (69.97742663656885%)]             Loss:0.2081228643655777            R_loss 6.376940727233887, KLD_loss 0.28299111127853394\n",
            "Train Epoch: 6 [10240/14166 (72.23476297968398%)]             Loss:0.22710615396499634            R_loss 6.829913139343262, KLD_loss 0.4374835789203644\n",
            "Train Epoch: 6 [10560/14166 (74.49209932279909%)]             Loss:0.21463316679000854            R_loss 6.53359317779541, KLD_loss 0.33466777205467224\n",
            "Train Epoch: 6 [10880/14166 (76.74943566591422%)]             Loss:0.2045261710882187            R_loss 6.335127353668213, KLD_loss 0.2097102254629135\n",
            "Train Epoch: 6 [11200/14166 (79.00677200902935%)]             Loss:0.2049102485179901            R_loss 6.33254861831665, KLD_loss 0.2245795875787735\n",
            "Train Epoch: 6 [11520/14166 (81.26410835214448%)]             Loss:0.20411933958530426            R_loss 6.25181245803833, KLD_loss 0.280006468296051\n",
            "Train Epoch: 6 [11840/14166 (83.52144469525959%)]             Loss:0.21186406910419464            R_loss 6.540485382080078, KLD_loss 0.2391652911901474\n",
            "Train Epoch: 6 [12160/14166 (85.77878103837472%)]             Loss:0.21084193885326385            R_loss 6.493094444274902, KLD_loss 0.2538476288318634\n",
            "Train Epoch: 6 [12480/14166 (88.03611738148985%)]             Loss:0.21261711418628693            R_loss 6.456684589385986, KLD_loss 0.3470628559589386\n",
            "Train Epoch: 6 [12800/14166 (90.29345372460497%)]             Loss:0.2125915139913559            R_loss 6.511981964111328, KLD_loss 0.29094618558883667\n",
            "Train Epoch: 6 [13120/14166 (92.55079006772009%)]             Loss:0.20032544434070587            R_loss 6.182717800140381, KLD_loss 0.2276965081691742\n",
            "Train Epoch: 6 [13440/14166 (94.80812641083521%)]             Loss:0.2054775357246399            R_loss 6.2757415771484375, KLD_loss 0.29953882098197937\n",
            "Train Epoch: 6 [13760/14166 (97.06546275395034%)]             Loss:0.20203451812267303            R_loss 6.198165416717529, KLD_loss 0.2669391334056854\n",
            "Train Epoch: 6 [14080/14166 (99.32279909706546%)]             Loss:0.201417937874794            R_loss 6.277145862579346, KLD_loss 0.1682281196117401\n",
            "====> Epoch: 6 Average loss: 0.2081\n",
            "====> Test set loss: 0.2077\n",
            "Train Epoch: 7 [0/14166 (0.0%)]             Loss:0.19379431009292603            R_loss 5.995342254638672, KLD_loss 0.20607557892799377\n",
            "Train Epoch: 7 [320/14166 (2.2573363431151243%)]             Loss:0.20889243483543396            R_loss 6.433353900909424, KLD_loss 0.25120338797569275\n",
            "Train Epoch: 7 [640/14166 (4.514672686230249%)]             Loss:0.2055312842130661            R_loss 6.335608005523682, KLD_loss 0.24139302968978882\n",
            "Train Epoch: 7 [960/14166 (6.772009029345372%)]             Loss:0.198110893368721            R_loss 6.083085060119629, KLD_loss 0.25646311044692993\n",
            "Train Epoch: 7 [1280/14166 (9.029345372460497%)]             Loss:0.2054872214794159            R_loss 6.336109638214111, KLD_loss 0.23948141932487488\n",
            "Train Epoch: 7 [1600/14166 (11.286681715575622%)]             Loss:0.19889694452285767            R_loss 6.1565022468566895, KLD_loss 0.20819984376430511\n",
            "Train Epoch: 7 [1920/14166 (13.544018058690744%)]             Loss:0.20715390145778656            R_loss 6.418010234832764, KLD_loss 0.21091505885124207\n",
            "Train Epoch: 7 [2240/14166 (15.801354401805868%)]             Loss:0.20410065352916718            R_loss 6.314934253692627, KLD_loss 0.21628662943840027\n",
            "Train Epoch: 7 [2560/14166 (18.058690744920995%)]             Loss:0.20414641499519348            R_loss 6.269710540771484, KLD_loss 0.26297473907470703\n",
            "Train Epoch: 7 [2880/14166 (20.31602708803612%)]             Loss:0.20415568351745605            R_loss 6.296558856964111, KLD_loss 0.23642335832118988\n",
            "Train Epoch: 7 [3200/14166 (22.573363431151243%)]             Loss:0.20801711082458496            R_loss 6.4202728271484375, KLD_loss 0.2362743765115738\n",
            "Train Epoch: 7 [3520/14166 (24.830699774266364%)]             Loss:0.2093658149242401            R_loss 6.427847385406494, KLD_loss 0.27185916900634766\n",
            "Train Epoch: 7 [3840/14166 (27.08803611738149%)]             Loss:0.21417248249053955            R_loss 6.5459136962890625, KLD_loss 0.3076050281524658\n",
            "Train Epoch: 7 [4160/14166 (29.345372460496613%)]             Loss:0.213999941945076            R_loss 6.539719104766846, KLD_loss 0.3082793354988098\n",
            "Train Epoch: 7 [4480/14166 (31.602708803611737%)]             Loss:0.2040267437696457            R_loss 6.296228885650635, KLD_loss 0.23262690007686615\n",
            "Train Epoch: 7 [4800/14166 (33.86004514672686%)]             Loss:0.22254545986652374            R_loss 6.729696750640869, KLD_loss 0.3917574882507324\n",
            "Train Epoch: 7 [5120/14166 (36.11738148984199%)]             Loss:0.21548748016357422            R_loss 6.556756496429443, KLD_loss 0.33884310722351074\n",
            "Train Epoch: 7 [5440/14166 (38.37471783295711%)]             Loss:0.21310974657535553            R_loss 6.476123332977295, KLD_loss 0.3433891534805298\n",
            "Train Epoch: 7 [5760/14166 (40.63205417607224%)]             Loss:0.20422464609146118            R_loss 6.327105522155762, KLD_loss 0.20808318257331848\n",
            "Train Epoch: 7 [6080/14166 (42.88939051918736%)]             Loss:0.2072330117225647            R_loss 6.33829402923584, KLD_loss 0.2931619882583618\n",
            "Train Epoch: 7 [6400/14166 (45.146726862302486%)]             Loss:0.20626887679100037            R_loss 6.334506511688232, KLD_loss 0.2660979628562927\n",
            "Train Epoch: 7 [6720/14166 (47.40406320541761%)]             Loss:0.20371860265731812            R_loss 6.299522876739502, KLD_loss 0.21947282552719116\n",
            "Train Epoch: 7 [7040/14166 (49.66139954853273%)]             Loss:0.20740869641304016            R_loss 6.384816646575928, KLD_loss 0.2522616982460022\n",
            "Train Epoch: 7 [7360/14166 (51.918735891647856%)]             Loss:0.20369312167167664            R_loss 6.240564346313477, KLD_loss 0.27761539816856384\n",
            "Train Epoch: 7 [7680/14166 (54.17607223476298%)]             Loss:0.20880532264709473            R_loss 6.36606502532959, KLD_loss 0.31570571660995483\n",
            "Train Epoch: 7 [8000/14166 (56.433408577878104%)]             Loss:0.2096051126718521            R_loss 6.466193675994873, KLD_loss 0.2411695122718811\n",
            "Train Epoch: 7 [8320/14166 (58.690744920993225%)]             Loss:0.20724080502986908            R_loss 6.316114902496338, KLD_loss 0.3155907988548279\n",
            "Train Epoch: 7 [8640/14166 (60.94808126410835%)]             Loss:0.22057151794433594            R_loss 6.696364879608154, KLD_loss 0.361923485994339\n",
            "Train Epoch: 7 [8960/14166 (63.205417607223474%)]             Loss:0.20631960034370422            R_loss 6.337198257446289, KLD_loss 0.2650292217731476\n",
            "Train Epoch: 7 [9280/14166 (65.4627539503386%)]             Loss:0.20330649614334106            R_loss 6.233194351196289, KLD_loss 0.27261388301849365\n",
            "Train Epoch: 7 [9600/14166 (67.72009029345372%)]             Loss:0.20564229786396027            R_loss 6.347548484802246, KLD_loss 0.23300546407699585\n",
            "Train Epoch: 7 [9920/14166 (69.97742663656885%)]             Loss:0.21408963203430176            R_loss 6.526098251342773, KLD_loss 0.32476988434791565\n",
            "Train Epoch: 7 [10240/14166 (72.23476297968398%)]             Loss:0.21284085512161255            R_loss 6.51065731048584, KLD_loss 0.30025050044059753\n",
            "Train Epoch: 7 [10560/14166 (74.49209932279909%)]             Loss:0.2016061544418335            R_loss 6.270024299621582, KLD_loss 0.18137219548225403\n",
            "Train Epoch: 7 [10880/14166 (76.74943566591422%)]             Loss:0.20440071821212769            R_loss 6.319605827331543, KLD_loss 0.22121715545654297\n",
            "Train Epoch: 7 [11200/14166 (79.00677200902935%)]             Loss:0.19773100316524506            R_loss 6.149460792541504, KLD_loss 0.17793121933937073\n",
            "Train Epoch: 7 [11520/14166 (81.26410835214448%)]             Loss:0.20094183087348938            R_loss 6.217041015625, KLD_loss 0.2130976766347885\n",
            "Train Epoch: 7 [11840/14166 (83.52144469525959%)]             Loss:0.20586949586868286            R_loss 6.417236328125, KLD_loss 0.17058804631233215\n",
            "Train Epoch: 7 [12160/14166 (85.77878103837472%)]             Loss:0.2007259875535965            R_loss 6.186962604522705, KLD_loss 0.2362687587738037\n",
            "Train Epoch: 7 [12480/14166 (88.03611738148985%)]             Loss:0.21376290917396545            R_loss 6.517696857452393, KLD_loss 0.3227158486843109\n",
            "Train Epoch: 7 [12800/14166 (90.29345372460497%)]             Loss:0.21250291168689728            R_loss 6.557467460632324, KLD_loss 0.2426258623600006\n",
            "Train Epoch: 7 [13120/14166 (92.55079006772009%)]             Loss:0.20574353635311127            R_loss 6.263697147369385, KLD_loss 0.32009631395339966\n",
            "Train Epoch: 7 [13440/14166 (94.80812641083521%)]             Loss:0.21019962430000305            R_loss 6.403172492980957, KLD_loss 0.3232155442237854\n",
            "Train Epoch: 7 [13760/14166 (97.06546275395034%)]             Loss:0.20259541273117065            R_loss 6.260831832885742, KLD_loss 0.22222158312797546\n",
            "Train Epoch: 7 [14080/14166 (99.32279909706546%)]             Loss:0.2159210443496704            R_loss 6.543587684631348, KLD_loss 0.3658854365348816\n",
            "====> Epoch: 7 Average loss: 0.2082\n",
            "====> Test set loss: 0.2078\n",
            "Train Epoch: 8 [0/14166 (0.0%)]             Loss:0.20586849749088287            R_loss 6.307593822479248, KLD_loss 0.28019773960113525\n",
            "Train Epoch: 8 [320/14166 (2.2573363431151243%)]             Loss:0.20855283737182617            R_loss 6.350854396820068, KLD_loss 0.32283705472946167\n",
            "Train Epoch: 8 [640/14166 (4.514672686230249%)]             Loss:0.20295076072216034            R_loss 6.280473709106445, KLD_loss 0.2139512300491333\n",
            "Train Epoch: 8 [960/14166 (6.772009029345372%)]             Loss:0.20409917831420898            R_loss 6.28953742980957, KLD_loss 0.2416362762451172\n",
            "Train Epoch: 8 [1280/14166 (9.029345372460497%)]             Loss:0.2152549922466278            R_loss 6.596006870269775, KLD_loss 0.2921525537967682\n",
            "Train Epoch: 8 [1600/14166 (11.286681715575622%)]             Loss:0.20480996370315552            R_loss 6.378837585449219, KLD_loss 0.17508135735988617\n",
            "Train Epoch: 8 [1920/14166 (13.544018058690744%)]             Loss:0.21313561499118805            R_loss 6.521502494812012, KLD_loss 0.29883724451065063\n",
            "Train Epoch: 8 [2240/14166 (15.801354401805868%)]             Loss:0.21373815834522247            R_loss 6.530921459197998, KLD_loss 0.30869969725608826\n",
            "Train Epoch: 8 [2560/14166 (18.058690744920995%)]             Loss:0.20222152769565582            R_loss 6.221691131591797, KLD_loss 0.24939824640750885\n",
            "Train Epoch: 8 [2880/14166 (20.31602708803612%)]             Loss:0.20128270983695984            R_loss 6.268793106079102, KLD_loss 0.17225350439548492\n",
            "Train Epoch: 8 [3200/14166 (22.573363431151243%)]             Loss:0.21052271127700806            R_loss 6.446373462677002, KLD_loss 0.29035431146621704\n",
            "Train Epoch: 8 [3520/14166 (24.830699774266364%)]             Loss:0.20893791317939758            R_loss 6.361732482910156, KLD_loss 0.3242805004119873\n",
            "Train Epoch: 8 [3840/14166 (27.08803611738149%)]             Loss:0.20488673448562622            R_loss 6.341653823852539, KLD_loss 0.21472175419330597\n",
            "Train Epoch: 8 [4160/14166 (29.345372460496613%)]             Loss:0.21164357662200928            R_loss 6.462292194366455, KLD_loss 0.3103025555610657\n",
            "Train Epoch: 8 [4480/14166 (31.602708803611737%)]             Loss:0.20309412479400635            R_loss 6.303045272827148, KLD_loss 0.19596678018569946\n",
            "Train Epoch: 8 [4800/14166 (33.86004514672686%)]             Loss:0.20683978497982025            R_loss 6.352534294128418, KLD_loss 0.2663397789001465\n",
            "Train Epoch: 8 [5120/14166 (36.11738148984199%)]             Loss:0.214509516954422            R_loss 6.620087146759033, KLD_loss 0.24421775341033936\n",
            "Train Epoch: 8 [5440/14166 (38.37471783295711%)]             Loss:0.19700701534748077            R_loss 6.079763889312744, KLD_loss 0.22446060180664062\n",
            "Train Epoch: 8 [5760/14166 (40.63205417607224%)]             Loss:0.19883182644844055            R_loss 6.193408966064453, KLD_loss 0.16920937597751617\n",
            "Train Epoch: 8 [6080/14166 (42.88939051918736%)]             Loss:0.193525493144989            R_loss 6.002407550811768, KLD_loss 0.1904083788394928\n",
            "Train Epoch: 8 [6400/14166 (45.146726862302486%)]             Loss:0.20584158599376678            R_loss 6.3534255027771, KLD_loss 0.23350496590137482\n",
            "Train Epoch: 8 [6720/14166 (47.40406320541761%)]             Loss:0.21412037312984467            R_loss 6.518506050109863, KLD_loss 0.33334577083587646\n",
            "Train Epoch: 8 [7040/14166 (49.66139954853273%)]             Loss:0.22489163279533386            R_loss 6.854240417480469, KLD_loss 0.3422916531562805\n",
            "Train Epoch: 8 [7360/14166 (51.918735891647856%)]             Loss:0.1994151622056961            R_loss 6.179072380065918, KLD_loss 0.202212855219841\n",
            "Train Epoch: 8 [7680/14166 (54.17607223476298%)]             Loss:0.22033564746379852            R_loss 6.69081974029541, KLD_loss 0.35992079973220825\n",
            "Train Epoch: 8 [8000/14166 (56.433408577878104%)]             Loss:0.21406102180480957            R_loss 6.4785542488098145, KLD_loss 0.3713981509208679\n",
            "Train Epoch: 8 [8320/14166 (58.690744920993225%)]             Loss:0.201523095369339            R_loss 6.154512882232666, KLD_loss 0.2942258417606354\n",
            "Train Epoch: 8 [8640/14166 (60.94808126410835%)]             Loss:0.19783836603164673            R_loss 6.084833145141602, KLD_loss 0.24599438905715942\n",
            "Train Epoch: 8 [8960/14166 (63.205417607223474%)]             Loss:0.2086845487356186            R_loss 6.438155651092529, KLD_loss 0.23974932730197906\n",
            "Train Epoch: 8 [9280/14166 (65.4627539503386%)]             Loss:0.20488891005516052            R_loss 6.3409833908081055, KLD_loss 0.21546116471290588\n",
            "Train Epoch: 8 [9600/14166 (67.72009029345372%)]             Loss:0.1987362951040268            R_loss 6.195185661315918, KLD_loss 0.16437597572803497\n",
            "Train Epoch: 8 [9920/14166 (69.97742663656885%)]             Loss:0.20895394682884216            R_loss 6.4112091064453125, KLD_loss 0.2753176689147949\n",
            "Train Epoch: 8 [10240/14166 (72.23476297968398%)]             Loss:0.20272038877010345            R_loss 6.269216537475586, KLD_loss 0.21783576905727386\n",
            "Train Epoch: 8 [10560/14166 (74.49209932279909%)]             Loss:0.2089959681034088            R_loss 6.440413475036621, KLD_loss 0.2474573850631714\n",
            "Train Epoch: 8 [10880/14166 (76.74943566591422%)]             Loss:0.1999121904373169            R_loss 6.17167854309082, KLD_loss 0.22551119327545166\n",
            "Train Epoch: 8 [11200/14166 (79.00677200902935%)]             Loss:0.2111974060535431            R_loss 6.488890171051025, KLD_loss 0.26942694187164307\n",
            "Train Epoch: 8 [11520/14166 (81.26410835214448%)]             Loss:0.2046147584915161            R_loss 6.310794830322266, KLD_loss 0.23687730729579926\n",
            "Train Epoch: 8 [11840/14166 (83.52144469525959%)]             Loss:0.2029828429222107            R_loss 6.298683166503906, KLD_loss 0.19676804542541504\n",
            "Train Epoch: 8 [12160/14166 (85.77878103837472%)]             Loss:0.21783597767353058            R_loss 6.671943664550781, KLD_loss 0.29880765080451965\n",
            "Train Epoch: 8 [12480/14166 (88.03611738148985%)]             Loss:0.21794700622558594            R_loss 6.6673479080200195, KLD_loss 0.3069557547569275\n",
            "Train Epoch: 8 [12800/14166 (90.29345372460497%)]             Loss:0.20651395618915558            R_loss 6.3918938636779785, KLD_loss 0.2165532410144806\n",
            "Train Epoch: 8 [13120/14166 (92.55079006772009%)]             Loss:0.2001323103904724            R_loss 6.2536702156066895, KLD_loss 0.15056391060352325\n",
            "Train Epoch: 8 [13440/14166 (94.80812641083521%)]             Loss:0.20585021376609802            R_loss 6.334482669830322, KLD_loss 0.25272423028945923\n",
            "Train Epoch: 8 [13760/14166 (97.06546275395034%)]             Loss:0.20455613732337952            R_loss 6.285808086395264, KLD_loss 0.2599881589412689\n",
            "Train Epoch: 8 [14080/14166 (99.32279909706546%)]             Loss:0.2098030149936676            R_loss 6.444104194641113, KLD_loss 0.26959291100502014\n",
            "====> Epoch: 8 Average loss: 0.2081\n",
            "====> Test set loss: 0.2082\n",
            "Train Epoch: 9 [0/14166 (0.0%)]             Loss:0.2082219123840332            R_loss 6.427375316619873, KLD_loss 0.23572590947151184\n",
            "Train Epoch: 9 [320/14166 (2.2573363431151243%)]             Loss:0.20471887290477753            R_loss 6.336857795715332, KLD_loss 0.214146226644516\n",
            "Train Epoch: 9 [640/14166 (4.514672686230249%)]             Loss:0.19616030156612396            R_loss 6.077090263366699, KLD_loss 0.2000400573015213\n",
            "Train Epoch: 9 [960/14166 (6.772009029345372%)]             Loss:0.20169144868850708            R_loss 6.26315975189209, KLD_loss 0.1909666657447815\n",
            "Train Epoch: 9 [1280/14166 (9.029345372460497%)]             Loss:0.19709022343158722            R_loss 6.081419944763184, KLD_loss 0.2254670411348343\n",
            "Train Epoch: 9 [1600/14166 (11.286681715575622%)]             Loss:0.2114696055650711            R_loss 6.452612400054932, KLD_loss 0.31441518664360046\n",
            "Train Epoch: 9 [1920/14166 (13.544018058690744%)]             Loss:0.2032507210969925            R_loss 6.31607723236084, KLD_loss 0.18794581294059753\n",
            "Train Epoch: 9 [2240/14166 (15.801354401805868%)]             Loss:0.202135369181633            R_loss 6.265005111694336, KLD_loss 0.20332670211791992\n",
            "Train Epoch: 9 [2560/14166 (18.058690744920995%)]             Loss:0.19987958669662476            R_loss 6.178426265716553, KLD_loss 0.21772050857543945\n",
            "Train Epoch: 9 [2880/14166 (20.31602708803612%)]             Loss:0.20057982206344604            R_loss 6.197694778442383, KLD_loss 0.2208595722913742\n",
            "Train Epoch: 9 [3200/14166 (22.573363431151243%)]             Loss:0.20594102144241333            R_loss 6.3235554695129395, KLD_loss 0.2665565609931946\n",
            "Train Epoch: 9 [3520/14166 (24.830699774266364%)]             Loss:0.21403270959854126            R_loss 6.533112525939941, KLD_loss 0.3159340023994446\n",
            "Train Epoch: 9 [3840/14166 (27.08803611738149%)]             Loss:0.2107652723789215            R_loss 6.490458011627197, KLD_loss 0.2540309429168701\n",
            "Train Epoch: 9 [4160/14166 (29.345372460496613%)]             Loss:0.2118397057056427            R_loss 6.551575183868408, KLD_loss 0.22729575634002686\n",
            "Train Epoch: 9 [4480/14166 (31.602708803611737%)]             Loss:0.20567576587200165            R_loss 6.268429756164551, KLD_loss 0.3131946623325348\n",
            "Train Epoch: 9 [4800/14166 (33.86004514672686%)]             Loss:0.2067134976387024            R_loss 6.393202781677246, KLD_loss 0.22162896394729614\n",
            "Train Epoch: 9 [5120/14166 (36.11738148984199%)]             Loss:0.20608367025852203            R_loss 6.341766834259033, KLD_loss 0.2529109716415405\n",
            "Train Epoch: 9 [5440/14166 (38.37471783295711%)]             Loss:0.21373683214187622            R_loss 6.525115489959717, KLD_loss 0.3144627809524536\n",
            "Train Epoch: 9 [5760/14166 (40.63205417607224%)]             Loss:0.20211265981197357            R_loss 6.205977439880371, KLD_loss 0.26162779331207275\n",
            "Train Epoch: 9 [6080/14166 (42.88939051918736%)]             Loss:0.21031306684017181            R_loss 6.384963035583496, KLD_loss 0.34505510330200195\n",
            "Train Epoch: 9 [6400/14166 (45.146726862302486%)]             Loss:0.21424677968025208            R_loss 6.555482864379883, KLD_loss 0.30041423439979553\n",
            "Train Epoch: 9 [6720/14166 (47.40406320541761%)]             Loss:0.21122530102729797            R_loss 6.520371913909912, KLD_loss 0.2388375997543335\n",
            "Train Epoch: 9 [7040/14166 (49.66139954853273%)]             Loss:0.21139539778232574            R_loss 6.508990287780762, KLD_loss 0.2556619644165039\n",
            "Train Epoch: 9 [7360/14166 (51.918735891647856%)]             Loss:0.202037051320076            R_loss 6.176242351531982, KLD_loss 0.28894293308258057\n",
            "Train Epoch: 9 [7680/14166 (54.17607223476298%)]             Loss:0.20924513041973114            R_loss 6.473561763763428, KLD_loss 0.22228233516216278\n",
            "Train Epoch: 9 [8000/14166 (56.433408577878104%)]             Loss:0.20335158705711365            R_loss 6.252273082733154, KLD_loss 0.254978209733963\n",
            "Train Epoch: 9 [8320/14166 (58.690744920993225%)]             Loss:0.205297589302063            R_loss 6.320257186889648, KLD_loss 0.2492659091949463\n",
            "Train Epoch: 9 [8640/14166 (60.94808126410835%)]             Loss:0.2111063301563263            R_loss 6.494571685791016, KLD_loss 0.2608315348625183\n",
            "Train Epoch: 9 [8960/14166 (63.205417607223474%)]             Loss:0.20412838459014893            R_loss 6.214028835296631, KLD_loss 0.31807953119277954\n",
            "Train Epoch: 9 [9280/14166 (65.4627539503386%)]             Loss:0.20875045657157898            R_loss 6.38218355178833, KLD_loss 0.29783084988594055\n",
            "Train Epoch: 9 [9600/14166 (67.72009029345372%)]             Loss:0.19723767042160034            R_loss 6.123195648193359, KLD_loss 0.18840986490249634\n",
            "Train Epoch: 9 [9920/14166 (69.97742663656885%)]             Loss:0.21279679238796234            R_loss 6.470946311950684, KLD_loss 0.3385508358478546\n",
            "Train Epoch: 9 [10240/14166 (72.23476297968398%)]             Loss:0.20476371049880981            R_loss 6.286162376403809, KLD_loss 0.2662762403488159\n",
            "Train Epoch: 9 [10560/14166 (74.49209932279909%)]             Loss:0.20360559225082397            R_loss 6.276735305786133, KLD_loss 0.23864376544952393\n",
            "Train Epoch: 9 [10880/14166 (76.74943566591422%)]             Loss:0.21008576452732086            R_loss 6.432611465454102, KLD_loss 0.290132999420166\n",
            "Train Epoch: 9 [11200/14166 (79.00677200902935%)]             Loss:0.20530761778354645            R_loss 6.2966108322143555, KLD_loss 0.2732333242893219\n",
            "Train Epoch: 9 [11520/14166 (81.26410835214448%)]             Loss:0.20119860768318176            R_loss 6.206198692321777, KLD_loss 0.2321566939353943\n",
            "Train Epoch: 9 [11840/14166 (83.52144469525959%)]             Loss:0.20982638001441956            R_loss 6.445605278015137, KLD_loss 0.26883870363235474\n",
            "Train Epoch: 9 [12160/14166 (85.77878103837472%)]             Loss:0.21706774830818176            R_loss 6.696336269378662, KLD_loss 0.24983102083206177\n",
            "Train Epoch: 9 [12480/14166 (88.03611738148985%)]             Loss:0.2051229476928711            R_loss 6.285933971405029, KLD_loss 0.27800047397613525\n",
            "Train Epoch: 9 [12800/14166 (90.29345372460497%)]             Loss:0.20838098227977753            R_loss 6.379710674285889, KLD_loss 0.2884806990623474\n",
            "Train Epoch: 9 [13120/14166 (92.55079006772009%)]             Loss:0.219265878200531            R_loss 6.704250335693359, KLD_loss 0.3122580349445343\n",
            "Train Epoch: 9 [13440/14166 (94.80812641083521%)]             Loss:0.21440210938453674            R_loss 6.524648189544678, KLD_loss 0.33621904253959656\n",
            "Train Epoch: 9 [13760/14166 (97.06546275395034%)]             Loss:0.21654433012008667            R_loss 6.673483371734619, KLD_loss 0.25593554973602295\n",
            "Train Epoch: 9 [14080/14166 (99.32279909706546%)]             Loss:0.20614628493785858            R_loss 6.334870338439941, KLD_loss 0.2618105411529541\n",
            "====> Epoch: 9 Average loss: 0.2083\n",
            "====> Test set loss: 0.2076\n",
            "Train Epoch: 10 [0/14166 (0.0%)]             Loss:0.20252010226249695            R_loss 6.273710250854492, KLD_loss 0.20693305134773254\n",
            "Train Epoch: 10 [320/14166 (2.2573363431151243%)]             Loss:0.21973511576652527            R_loss 6.741743087768555, KLD_loss 0.28978031873703003\n",
            "Train Epoch: 10 [640/14166 (4.514672686230249%)]             Loss:0.20241811871528625            R_loss 6.2916412353515625, KLD_loss 0.18573857843875885\n",
            "Train Epoch: 10 [960/14166 (6.772009029345372%)]             Loss:0.2052694708108902            R_loss 6.3613128662109375, KLD_loss 0.2073100060224533\n",
            "Train Epoch: 10 [1280/14166 (9.029345372460497%)]             Loss:0.21500283479690552            R_loss 6.605314254760742, KLD_loss 0.2747761011123657\n",
            "Train Epoch: 10 [1600/14166 (11.286681715575622%)]             Loss:0.2044430524110794            R_loss 6.309370517730713, KLD_loss 0.23280739784240723\n",
            "Train Epoch: 10 [1920/14166 (13.544018058690744%)]             Loss:0.20451003313064575            R_loss 6.334877014160156, KLD_loss 0.20944441854953766\n",
            "Train Epoch: 10 [2240/14166 (15.801354401805868%)]             Loss:0.20904698967933655            R_loss 6.446803092956543, KLD_loss 0.24270017445087433\n",
            "Train Epoch: 10 [2560/14166 (18.058690744920995%)]             Loss:0.19861021637916565            R_loss 6.184240341186523, KLD_loss 0.17128679156303406\n",
            "Train Epoch: 10 [2880/14166 (20.31602708803612%)]             Loss:0.20959706604480743            R_loss 6.361094951629639, KLD_loss 0.3460111618041992\n",
            "Train Epoch: 10 [3200/14166 (22.573363431151243%)]             Loss:0.21001413464546204            R_loss 6.434195518493652, KLD_loss 0.28625696897506714\n",
            "Train Epoch: 10 [3520/14166 (24.830699774266364%)]             Loss:0.20205368101596832            R_loss 6.252744197845459, KLD_loss 0.21297335624694824\n",
            "Train Epoch: 10 [3840/14166 (27.08803611738149%)]             Loss:0.20971572399139404            R_loss 6.435689926147461, KLD_loss 0.27521324157714844\n",
            "Train Epoch: 10 [4160/14166 (29.345372460496613%)]             Loss:0.20616772770881653            R_loss 6.3320112228393555, KLD_loss 0.2653562128543854\n",
            "Train Epoch: 10 [4480/14166 (31.602708803611737%)]             Loss:0.20316912233829498            R_loss 6.25912618637085, KLD_loss 0.24228569865226746\n",
            "Train Epoch: 10 [4800/14166 (33.86004514672686%)]             Loss:0.19930768013000488            R_loss 6.215449810028076, KLD_loss 0.1623963862657547\n",
            "Train Epoch: 10 [5120/14166 (36.11738148984199%)]             Loss:0.20902986824512482            R_loss 6.411993980407715, KLD_loss 0.27696219086647034\n",
            "Train Epoch: 10 [5440/14166 (38.37471783295711%)]             Loss:0.21558871865272522            R_loss 6.571835517883301, KLD_loss 0.32700368762016296\n",
            "Train Epoch: 10 [5760/14166 (40.63205417607224%)]             Loss:0.20518457889556885            R_loss 6.381885528564453, KLD_loss 0.18402063846588135\n",
            "Train Epoch: 10 [6080/14166 (42.88939051918736%)]             Loss:0.19755521416664124            R_loss 6.119076251983643, KLD_loss 0.20269033312797546\n",
            "Train Epoch: 10 [6400/14166 (45.146726862302486%)]             Loss:0.22633790969848633            R_loss 6.898495674133301, KLD_loss 0.34431731700897217\n",
            "Train Epoch: 10 [6720/14166 (47.40406320541761%)]             Loss:0.21044321358203888            R_loss 6.487978935241699, KLD_loss 0.24620410799980164\n",
            "Train Epoch: 10 [7040/14166 (49.66139954853273%)]             Loss:0.1989160031080246            R_loss 6.18613862991333, KLD_loss 0.1791733205318451\n",
            "Train Epoch: 10 [7360/14166 (51.918735891647856%)]             Loss:0.20186050236225128            R_loss 6.22865629196167, KLD_loss 0.23087933659553528\n",
            "Train Epoch: 10 [7680/14166 (54.17607223476298%)]             Loss:0.20071133971214294            R_loss 6.175455093383789, KLD_loss 0.24730795621871948\n",
            "Train Epoch: 10 [8000/14166 (56.433408577878104%)]             Loss:0.20726588368415833            R_loss 6.449473857879639, KLD_loss 0.18303412199020386\n",
            "Train Epoch: 10 [8320/14166 (58.690744920993225%)]             Loss:0.2167767882347107            R_loss 6.61448860168457, KLD_loss 0.322368860244751\n",
            "Train Epoch: 10 [8640/14166 (60.94808126410835%)]             Loss:0.20805281400680542            R_loss 6.403573036193848, KLD_loss 0.25411736965179443\n",
            "Train Epoch: 10 [8960/14166 (63.205417607223474%)]             Loss:0.2075348049402237            R_loss 6.406249046325684, KLD_loss 0.23486460745334625\n",
            "Train Epoch: 10 [9280/14166 (65.4627539503386%)]             Loss:0.2071092575788498            R_loss 6.391313552856445, KLD_loss 0.23618261516094208\n",
            "Train Epoch: 10 [9600/14166 (67.72009029345372%)]             Loss:0.2008202075958252            R_loss 6.2356743812561035, KLD_loss 0.1905720829963684\n",
            "Train Epoch: 10 [9920/14166 (69.97742663656885%)]             Loss:0.2145053595304489            R_loss 6.560843467712402, KLD_loss 0.30332833528518677\n",
            "Train Epoch: 10 [10240/14166 (72.23476297968398%)]             Loss:0.2088610827922821            R_loss 6.415151119232178, KLD_loss 0.268403023481369\n",
            "Train Epoch: 10 [10560/14166 (74.49209932279909%)]             Loss:0.2072971761226654            R_loss 6.363424777984619, KLD_loss 0.27008503675460815\n",
            "Train Epoch: 10 [10880/14166 (76.74943566591422%)]             Loss:0.20329566299915314            R_loss 6.280125617980957, KLD_loss 0.2253355085849762\n",
            "Train Epoch: 10 [11200/14166 (79.00677200902935%)]             Loss:0.20795978605747223            R_loss 6.397436141967773, KLD_loss 0.2572774887084961\n",
            "Train Epoch: 10 [11520/14166 (81.26410835214448%)]             Loss:0.2048911303281784            R_loss 6.326179504394531, KLD_loss 0.23033678531646729\n",
            "Train Epoch: 10 [11840/14166 (83.52144469525959%)]             Loss:0.20773138105869293            R_loss 6.389671325683594, KLD_loss 0.2577328383922577\n",
            "Train Epoch: 10 [12160/14166 (85.77878103837472%)]             Loss:0.21663592755794525            R_loss 6.658026695251465, KLD_loss 0.27432310581207275\n",
            "Train Epoch: 10 [12480/14166 (88.03611738148985%)]             Loss:0.20037366449832916            R_loss 6.2053446769714355, KLD_loss 0.2066127210855484\n",
            "Train Epoch: 10 [12800/14166 (90.29345372460497%)]             Loss:0.215257927775383            R_loss 6.5810675621032715, KLD_loss 0.30718597769737244\n",
            "Train Epoch: 10 [13120/14166 (92.55079006772009%)]             Loss:0.20521867275238037            R_loss 6.282731533050537, KLD_loss 0.2842658460140228\n",
            "Train Epoch: 10 [13440/14166 (94.80812641083521%)]             Loss:0.21703234314918518            R_loss 6.596312522888184, KLD_loss 0.3487219512462616\n",
            "Train Epoch: 10 [13760/14166 (97.06546275395034%)]             Loss:0.21000780165195465            R_loss 6.454356670379639, KLD_loss 0.26589304208755493\n",
            "Train Epoch: 10 [14080/14166 (99.32279909706546%)]             Loss:0.20291903614997864            R_loss 6.320202827453613, KLD_loss 0.17320659756660461\n",
            "====> Epoch: 10 Average loss: 0.2079\n",
            "====> Test set loss: 0.2074\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L3NKOc3mHlIP"
      },
      "execution_count": 36,
      "outputs": []
    }
  ]
}