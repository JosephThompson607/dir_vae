{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JosephThompson607/dir_vae/blob/main/sepsis_vae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ],
      "metadata": {
        "id": "phRvmO49rUUT"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "#Reading df\n",
        "# Kaggle toy set\n",
        "housing_df = pd.read_csv(\"/content/Housing.csv\")\n",
        "housing_df[\"sepsis\"] = housing_df[\"price\"] > 1000000 #for evaluation (Sepsis standin)\n",
        "patients = housing_df.drop(columns=[\"sepsis\",\"price\"] )\n",
        "subject_ids = patients.index\n",
        "\n",
        "#NOTE this is the actual df, uncomment to use\n",
        "# patients = pd.read_csv(\"/content/unique_patient_dem.csv\")\n",
        "# print(patients.columns)\n",
        "# subject_ids = patients['subject_id'].values\n",
        "\n",
        "# # Drop subject_id from the data for processing\n",
        "# patients.drop(columns=['subject_id'], inplace=True)"
      ],
      "metadata": {
        "id": "q2-vbXWMIFBE"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we prepare the data for training"
      ],
      "metadata": {
        "id": "0SiJSH5EMXTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Get numeric and categorical columns\n",
        "numeric_cols = patients.select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_cols = patients.select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "\n",
        "# Reorder DataFrame\n",
        "patients = patients[numeric_cols + categorical_cols]\n",
        "\n",
        "# One-hot encode\n",
        "df_encoded = pd.get_dummies(patients, columns=categorical_cols)\n",
        "\n",
        "# Scale numeric columns\n",
        "scaler = StandardScaler()\n",
        "df_encoded[numeric_cols] = scaler.fit_transform(df_encoded[numeric_cols])\n",
        "\n",
        "# Convert to features array\n",
        "features = df_encoded.astype('float32').values\n",
        "tensor = torch.tensor(features, dtype=torch.float32)\n",
        "\n",
        "# Convert subject_ids to tensor (if needed later)\n",
        "subject_ids_tensor = torch.tensor(subject_ids)\n",
        "\n",
        "# Split features and ids together (so they match!)\n",
        "X_train, X_test, ids_train, ids_test = train_test_split(\n",
        "    tensor, subject_ids_tensor, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Wrap both tensors in the dataset ids_train ids_test are the subject ids\n",
        "train_dataset = TensorDataset(X_train, ids_train)\n",
        "test_dataset = TensorDataset(X_test, ids_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Input size (excluding subject_id, since it's not input to model)\n",
        "input_size = X_train.shape[1]\n",
        "print(input_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4j5hmBSfrZhl",
        "outputId": "ff82b376-7bbc-4bc0-c8f3-4ce136940912"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we define the model and related functions"
      ],
      "metadata": {
        "id": "HeWHLkFqMPmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ngf = 64\n",
        "ndf = 64\n",
        "nc = 1\n",
        "\n",
        "def prior(K, alpha):\n",
        "    \"\"\"\n",
        "    Prior for the model.\n",
        "    :K: number of categories\n",
        "    :alpha: Hyper param of Dir\n",
        "    :return: mean and variance tensors\n",
        "    \"\"\"\n",
        "    # Approximate to normal distribution using Laplace approximation\n",
        "    a = torch.Tensor(1, K).float().fill_(alpha)\n",
        "    mean = a.log().t() - a.log().mean(1)\n",
        "    var = ((1 - 2.0 / K) * a.reciprocal()).t() + (1.0 / K ** 2) * a.reciprocal().sum(1)\n",
        "    return mean.t(), var.t() # Parameters of prior distribution after approximation\n",
        "\n",
        "class Dir_VAE(nn.Module):\n",
        "    def __init__(self, input_size,n_numeric, latent_size=10,\n",
        "                 hidden_dim = 200, num_weight=1.0, cat_weight=1.0, kld_weight=1.0):\n",
        "        self.num_weight = num_weight\n",
        "        self.cat_weight = cat_weight\n",
        "        self.kld_weight = kld_weight\n",
        "        self.num_numeric_cols = n_numeric\n",
        "        self.latent_size = latent_size\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.input_size = input_size\n",
        "        super(Dir_VAE, self).__init__()\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "          nn.Linear(self.input_size, self.hidden_dim),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(self.hidden_dim, self.hidden_dim),\n",
        "          nn.ReLU(),\n",
        "          # nn.Linear(self.hidden_dim, self.hidden_dim),\n",
        "          # nn.ReLU()\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "          nn.Linear(self.latent_size, self.hidden_dim),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(self.hidden_dim, self.hidden_dim),\n",
        "          nn.ReLU(),\n",
        "          # nn.Linear(self.hidden_dim, self.hidden_dim),\n",
        "          # nn.ReLU(),\n",
        "          nn.Linear(self.hidden_dim, self.hidden_dim),\n",
        "          nn.ReLU(),\n",
        "          # nn.Unflatten(dim=1, unflattened_size=(1, 28, 28)) # This was for image data\n",
        "        )\n",
        "        #self.fc1 = nn.Linear(self.hidden_dim, 512)\n",
        "        self.fc21 = nn.Linear(self.hidden_dim, self.latent_size)\n",
        "        self.fc22 = nn.Linear(self.hidden_dim, self.latent_size)\n",
        "\n",
        "        self.fc3 = nn.Linear(self.hidden_dim, self.input_size)#last layer\n",
        "        #self.fc4 = nn.Linear(512, self.hidden_dim)\n",
        "\n",
        "        self.lrelu = nn.LeakyReLU()\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Dir prior\n",
        "        self.prior_mean, self.prior_var = map(nn.Parameter, prior(self.latent_size, 0.3)) # 0.3 is a hyper param of Dirichlet distribution\n",
        "        self.prior_logvar = nn.Parameter(self.prior_var.log())\n",
        "        self.prior_mean.requires_grad = False\n",
        "        self.prior_var.requires_grad = False\n",
        "        self.prior_logvar.requires_grad = False\n",
        "\n",
        "\n",
        "    def encode(self, x):\n",
        "        encoding = self.encoder(x);\n",
        "        #h1 = self.fc1(encoding)\n",
        "        return self.fc21(encoding), self.fc22(encoding)\n",
        "\n",
        "    def decode(self, gauss_z):\n",
        "        dir_z = F.softmax(gauss_z,dim=1) #Reduntant, already done in forward\n",
        "        # This variable (z) can be treated as a variable that follows a Dirichlet distribution (a variable that can be interpreted as a probability that the sum is 1)\n",
        "        # Use the Softmax function to satisfy the simplex constraint\n",
        "        x_out = self.fc3(self.decoder(dir_z))\n",
        "\n",
        "        # Apply sigmoid to categorical output only\n",
        "        x_out[:, self.num_numeric_cols:] = torch.sigmoid(x_out[:, self.num_numeric_cols:])\n",
        "        return x_out\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps*std\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        gauss_z = self.reparameterize(mu, logvar)\n",
        "        # gause_z is a variable that follows a multivariate normal distribution\n",
        "        # Inputting gause_z into softmax func yields a random variable that follows a Dirichlet distribution (Softmax func are used in decoder)\n",
        "        dir_z = F.softmax(gauss_z,dim=1) # This variable follows a Dirichlet distribution\n",
        "        return self.decode(gauss_z), mu, logvar, gauss_z, dir_z\n",
        "\n",
        "    def reconstruction_loss(self, x_true, x_recon):\n",
        "        # Slice the tensors\n",
        "        x_true_num = x_true[:, :self.num_numeric_cols]\n",
        "        x_true_cat = x_true[:, self.num_numeric_cols:]\n",
        "\n",
        "        x_recon_num = x_recon[:, :self.num_numeric_cols]\n",
        "        x_recon_cat = x_recon[:, self.num_numeric_cols:]\n",
        "\n",
        "        # Compute losses\n",
        "        num_loss = F.mse_loss(x_recon_num, x_true_num)\n",
        "        cat_loss = F.cross_entropy(x_recon_cat, x_true_cat)\n",
        "        #putting weights to device\n",
        "        num_weight = torch.tensor(self.num_weight, device = x_recon.device)\n",
        "        cat_weight = torch.tensor(self.cat_weight, device =x_recon.device )\n",
        "        return num_weight *num_loss + cat_weight *cat_loss\n",
        "\n",
        "    # Reconstruction + KL divergence losses summed over all elements and batch\n",
        "    def loss_function(self, recon_x, x, mu, logvar):\n",
        "        # Apply sigmoid to the input data x to ensure values are between 0 and 1\n",
        "        recon_loss = self.reconstruction_loss(x, recon_x, )\n",
        "        # ディリクレ事前分布と変分事後分布とのKLを計算\n",
        "        # Calculating KL with Dirichlet prior and variational posterior distributions\n",
        "        # Original paper:\"Autoencodeing variational inference for topic model\"-https://arxiv.org/pdf/1703.01488\n",
        "        prior_mean = self.prior_mean.expand_as(mu)\n",
        "        prior_var = self.prior_var.expand_as(logvar)\n",
        "        prior_logvar = self.prior_logvar.expand_as(logvar)\n",
        "        var_division = logvar.exp() / prior_var # Σ_0 / Σ_1\n",
        "        diff = mu - prior_mean # μ_１ - μ_0\n",
        "        diff_term = diff *diff / prior_var # (μ_1 - μ_0)(μ_1 - μ_0)/Σ_1\n",
        "        logvar_division = prior_logvar - logvar # log|Σ_1| - log|Σ_0| = log(|Σ_1|/|Σ_2|)\n",
        "        # KLD\n",
        "        kld_weight = torch.tensor(self.kld_weight, device = recon_x.device)\n",
        "        KLD = 0.5 * ((var_division + diff_term + logvar_division).sum(1) - self.latent_size) * kld_weight\n",
        "        self.last_KLD = torch.mean(KLD) #Used for reporting\n",
        "        self.last_BCE = recon_loss\n",
        "        return recon_loss + KLD"
      ],
      "metadata": {
        "id": "jFoNCRlVMNud"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below are the training and test loops\n"
      ],
      "metadata": {
        "id": "EQ-82vOxMqkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = Dir_VAE(input_size, n_numeric, latent_size=2, hidden_dim=20).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data,batch_ids) in enumerate(train_loader): # Unpack only one element\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar, gauss_z, dir_z = model(data)\n",
        "\n",
        "        loss = model.loss_function(recon_batch, data, mu, logvar, )\n",
        "        loss = loss.mean()\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 10 == 0:\n",
        "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader)}%)] \\\n",
        "            Loss:{loss.item() / len(data)}\\\n",
        "            R_loss {model.last_BCE}, KLD_loss {model.last_KLD}')\n",
        "\n",
        "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
        "          epoch, train_loss / len(train_loader.dataset)))\n",
        "\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for i, (data,batch_ids) in enumerate(test_loader): # Unpack only one element\n",
        "            data = data.to(device)\n",
        "            recon_batch, mu, logvar, gauss_z, dir_z = model(data)\n",
        "            loss = model.loss_function(recon_batch, data, mu, logvar)\n",
        "            test_loss += loss.mean()\n",
        "            test_loss.item()\n",
        "\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('====> Test set loss: {:.4f}'.format(test_loss))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    for epoch in range(1, 10 + 1):\n",
        "        train(epoch)\n",
        "        test(epoch)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgOw9CXb2p_l",
        "outputId": "94729b4c-815a-4ee0-e837-b0e4a9238911"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/436 (0.0%)]             Loss:0.6184365153312683            R_loss 19.663787841796875, KLD_loss 0.12618115544319153\n",
            "Train Epoch: 1 [320/436 (71.42857142857143%)]             Loss:0.6203287839889526            R_loss 19.77236557006836, KLD_loss 0.07815662026405334\n",
            "====> Epoch: 1 Average loss: 0.6419\n",
            "====> Test set loss: 0.7349\n",
            "Train Epoch: 2 [0/436 (0.0%)]             Loss:0.6187695860862732            R_loss 19.738971710205078, KLD_loss 0.06165476515889168\n",
            "Train Epoch: 2 [320/436 (71.42857142857143%)]             Loss:0.6189548969268799            R_loss 19.782852172851562, KLD_loss 0.02370303124189377\n",
            "====> Epoch: 2 Average loss: 0.6379\n",
            "====> Test set loss: 0.7309\n",
            "Train Epoch: 3 [0/436 (0.0%)]             Loss:0.6129969954490662            R_loss 19.601526260375977, KLD_loss 0.014377586543560028\n",
            "Train Epoch: 3 [320/436 (71.42857142857143%)]             Loss:0.6159773468971252            R_loss 19.70222282409668, KLD_loss 0.009051166474819183\n",
            "====> Epoch: 3 Average loss: 0.6347\n",
            "====> Test set loss: 0.7275\n",
            "Train Epoch: 4 [0/436 (0.0%)]             Loss:0.611260712146759            R_loss 19.554067611694336, KLD_loss 0.006274070590734482\n",
            "Train Epoch: 4 [320/436 (71.42857142857143%)]             Loss:0.6104987263679504            R_loss 19.531829833984375, KLD_loss 0.004128742963075638\n",
            "====> Epoch: 4 Average loss: 0.6311\n",
            "====> Test set loss: 0.7228\n",
            "Train Epoch: 5 [0/436 (0.0%)]             Loss:0.6103187799453735            R_loss 19.527097702026367, KLD_loss 0.003103777766227722\n",
            "Train Epoch: 5 [320/436 (71.42857142857143%)]             Loss:0.6103062033653259            R_loss 19.52552604675293, KLD_loss 0.004274751991033554\n",
            "====> Epoch: 5 Average loss: 0.6263\n",
            "====> Test set loss: 0.7164\n",
            "Train Epoch: 6 [0/436 (0.0%)]             Loss:0.6070893406867981            R_loss 19.423171997070312, KLD_loss 0.0036886371672153473\n",
            "Train Epoch: 6 [320/436 (71.42857142857143%)]             Loss:0.6038762331008911            R_loss 19.321346282958984, KLD_loss 0.002693016082048416\n",
            "====> Epoch: 6 Average loss: 0.6203\n",
            "====> Test set loss: 0.7088\n",
            "Train Epoch: 7 [0/436 (0.0%)]             Loss:0.5993438959121704            R_loss 19.17679214477539, KLD_loss 0.0022124014794826508\n",
            "Train Epoch: 7 [320/436 (71.42857142857143%)]             Loss:0.6008743643760681            R_loss 19.226131439208984, KLD_loss 0.0018488354980945587\n",
            "====> Epoch: 7 Average loss: 0.6141\n",
            "====> Test set loss: 0.7021\n",
            "Train Epoch: 8 [0/436 (0.0%)]             Loss:0.59485924243927            R_loss 19.034008026123047, KLD_loss 0.0014886856079101562\n",
            "Train Epoch: 8 [320/436 (71.42857142857143%)]             Loss:0.595673680305481            R_loss 19.06005859375, KLD_loss 0.0014989189803600311\n",
            "====> Epoch: 8 Average loss: 0.6098\n",
            "====> Test set loss: 0.6980\n",
            "Train Epoch: 9 [0/436 (0.0%)]             Loss:0.5950613021850586            R_loss 19.04088592529297, KLD_loss 0.00107526034116745\n",
            "Train Epoch: 9 [320/436 (71.42857142857143%)]             Loss:0.5833933353424072            R_loss 18.667816162109375, KLD_loss 0.0007713735103607178\n",
            "====> Epoch: 9 Average loss: 0.6077\n",
            "====> Test set loss: 0.6963\n",
            "Train Epoch: 10 [0/436 (0.0%)]             Loss:0.5928075909614563            R_loss 18.968700408935547, KLD_loss 0.0011416077613830566\n",
            "Train Epoch: 10 [320/436 (71.42857142857143%)]             Loss:0.595237672328949            R_loss 19.04654312133789, KLD_loss 0.0010611265897750854\n",
            "====> Epoch: 10 Average loss: 0.6059\n",
            "====> Test set loss: 0.6951\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def initalize_centroids(X, k):\n",
        "    indices = np.random.choice(X.shape[0], k, replace=False)\n",
        "    return X[indices]\n",
        "def assign_clusters(X, centroids):\n",
        "    distances = np.linalg.norm(X[:, np.newaxis, :] - centroids, axis=2)\n",
        "    return np.argmin(distances, axis=1)\n",
        "def update_centroids(X, clusters, k):\n",
        "    centroids = np.zeros((k, X.shape[1]))\n",
        "    for i in range(k):\n",
        "        centroids[i] = np.mean(X[clusters == i], axis=0)\n",
        "    return centroids\n",
        "def kmeans(X, k, max_iter=100):\n",
        "    centroids = initalize_centroids(X, k)\n",
        "    for iter in range(max_iter):\n",
        "        print('Iteration '+str(iter))\n",
        "        clusters = assign_clusters(X, centroids)\n",
        "        new_centroids = update_centroids(X, clusters, k)\n",
        "        if np.allclose(new_centroids, centroids):\n",
        "            break\n",
        "        centroids = new_centroids\n",
        "    return clusters, centroids"
      ],
      "metadata": {
        "id": "L3NKOc3mHlIP"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  x = []\n",
        "  y = []\n",
        "  for i, (data,ids) in enumerate(test_loader): # Unpack only one element\n",
        "      data = data.to(device)\n",
        "      print(data[0])\n",
        "      recon_batch, mu, logvar, gauss_z, dir_z = model(data)\n",
        "      x+= dir_z[:,0]\n",
        "      y+= dir_z[:,1]\n",
        "\n",
        "\n",
        "# Example dummy data\n",
        "# df = pd.DataFrame({\n",
        "#     'x': X[:, 0].numpy(),            # first variable\n",
        "#     'y': model_output.numpy(),       # or any target/prediction\n",
        "#     'race': race_ids.numpy()         # use as hue\n",
        "# })\n",
        "\n",
        "sns.scatterplot(data=df, x='x', y='y', hue='race', palette='tab10')\n",
        "\n",
        "plt.scatter(x,y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "57g2SgMSQjPY",
        "outputId": "0d67dffb-a15a-4700-dbdf-29a7a85832a3"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.3457, 1.4034, 1.4218, 0.2244, 0.3560, 1.0000, 0.0000, 1.0000, 0.0000,\n",
            "        0.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "        0.0000, 1.0000])\n",
            "tensor([ 0.3918,  0.0473,  1.4218,  1.3782, -0.8057,  0.0000,  1.0000,  0.0000,\n",
            "         1.0000,  1.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,  1.0000,\n",
            "         0.0000,  1.0000,  0.0000,  0.0000])\n",
            "tensor([ 0.0966,  0.0473, -0.5702,  0.2244,  1.5177,  0.0000,  1.0000,  1.0000,\n",
            "         0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  0.0000,\n",
            "         1.0000,  0.0000,  0.0000,  1.0000])\n",
            "tensor([ 0.3125,  1.4034, -0.5702,  2.5320, -0.8057,  0.0000,  1.0000,  0.0000,\n",
            "         1.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
            "         0.0000,  0.0000,  1.0000,  0.0000])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-67-2208474950.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# })\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatterplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'race'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tab10'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    }
  ]
}