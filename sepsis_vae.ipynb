{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JosephThompson607/dir_vae/blob/main/sepsis_vae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "phRvmO49rUUT"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: Read this from the cloud\n",
        "patients = pd.read_csv(\"/content/unique_patient_dem.csv\")\n",
        "df_encoded = pd.get_dummies(patients, columns=['race', 'gender'])\n",
        "\n",
        "#If cuda is available, device is cuda, otherwise cpu\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "scaler = StandardScaler()\n",
        "df_encoded['anchor_age'] = scaler.fit_transform(df_encoded[['anchor_age']])\n",
        "features = df_encoded.drop(columns=['subject_id']).astype('float32').values\n",
        "# print(features.columns)\n",
        "# print(features.dtypes)\n",
        "print([type(feature) for feature in features[0]])\n",
        "tensor = torch.tensor(features, dtype=torch.float32)\n",
        "\n",
        "X_train, X_test = train_test_split(tensor, test_size=0.2, random_state=42)\n",
        "\n",
        "train_dataset = TensorDataset(X_train)  # or (X_train, y_train)\n",
        "test_dataset = TensorDataset(X_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "input_size = X_train[0].shape[0] #input size is the number of features going into the network\n",
        "print(input_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4j5hmBSfrZhl",
        "outputId": "795c6eca-23f3-4a4d-bb47-995b3eec5e49"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<class 'numpy.float32'>, <class 'numpy.float32'>, <class 'numpy.float32'>, <class 'numpy.float32'>, <class 'numpy.float32'>, <class 'numpy.float32'>, <class 'numpy.float32'>, <class 'numpy.float32'>, <class 'numpy.float32'>, <class 'numpy.float32'>, <class 'numpy.float32'>, <class 'numpy.float32'>, <class 'numpy.float32'>, <class 'numpy.float32'>, <class 'numpy.float32'>, <class 'numpy.float32'>, <class 'numpy.float32'>, <class 'numpy.float32'>, <class 'numpy.float32'>, <class 'numpy.float32'>, <class 'numpy.float32'>, <class 'numpy.float32'>, <class 'numpy.float32'>, <class 'numpy.float32'>, <class 'numpy.float32'>, <class 'numpy.float32'>, <class 'numpy.float32'>, <class 'numpy.float32'>, <class 'numpy.float32'>, <class 'numpy.float32'>, <class 'numpy.float32'>, <class 'numpy.float32'>, <class 'numpy.float32'>, <class 'numpy.float32'>, <class 'numpy.float32'>, <class 'numpy.float32'>]\n",
            "36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# train_loader = torch.utils.data.DataLoader(\n",
        "#     datasets.MNIST('../data', train=True, download=True,\n",
        "#                    transform=transforms.ToTensor()),\n",
        "#     batch_size=256, shuffle=True)\n",
        "# test_loader = torch.utils.data.DataLoader(\n",
        "#     datasets.MNIST('../data', train=False, transform=transforms.ToTensor()),\n",
        "#     batch_size=256, shuffle=False)\n",
        "#input_size = 28 * 28 #mnist dimensions, to change\n",
        "\n",
        "ngf = 64\n",
        "ndf = 64\n",
        "nc = 1\n",
        "\n",
        "def prior(K, alpha):\n",
        "    \"\"\"\n",
        "    Prior for the model.\n",
        "    :K: number of categories\n",
        "    :alpha: Hyper param of Dir\n",
        "    :return: mean and variance tensors\n",
        "    \"\"\"\n",
        "    # ラプラス近似で正規分布に近似\n",
        "    # Approximate to normal distribution using Laplace approximation\n",
        "    a = torch.Tensor(1, K).float().fill_(alpha)\n",
        "    mean = a.log().t() - a.log().mean(1)\n",
        "    var = ((1 - 2.0 / K) * a.reciprocal()).t() + (1.0 / K ** 2) * a.reciprocal().sum(1)\n",
        "    return mean.t(), var.t() # Parameters of prior distribution after approximation\n",
        "\n",
        "class Dir_VAE(nn.Module):\n",
        "    def __init__(self, input_size,latent_size=10, hidden_dim = 200):\n",
        "        self.latent_size = latent_size\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.input_size = input_size\n",
        "        super(Dir_VAE, self).__init__()\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "          nn.Linear(self.input_size, self.hidden_dim),\n",
        "          nn.ReLU(),\n",
        "          # nn.Linear(self.hidden_dim, self.hidden_dim),\n",
        "          # nn.ReLU(),\n",
        "          # nn.Linear(self.hidden_dim, self.hidden_dim),\n",
        "          # nn.ReLU()\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "          nn.Linear(self.latent_size, self.hidden_dim),\n",
        "          nn.ReLU(),\n",
        "          # nn.Linear(self.hidden_dim, self.hidden_dim),\n",
        "          # nn.ReLU(),\n",
        "          # nn.Linear(self.hidden_dim, self.hidden_dim),\n",
        "          # nn.ReLU(),\n",
        "          nn.Linear(self.hidden_dim, self.input_size),\n",
        "          nn.Sigmoid(),\n",
        "          # nn.Unflatten(dim=1, unflattened_size=(1, 28, 28)) # This was for image data\n",
        "        )\n",
        "        # self.encoder = nn.Sequential(\n",
        "        #     # input is (nc) x 28 x 28\n",
        "        #     nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
        "        #     nn.LeakyReLU(0.2, inplace=True),\n",
        "        #     # state size. (ndf) x 14 x 14\n",
        "        #     nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "        #     nn.BatchNorm2d(ndf * 2),\n",
        "        #     nn.LeakyReLU(0.2, inplace=True),\n",
        "        #     # state size. (ndf*2) x 7 x 7\n",
        "        #     nn.Conv2d(ndf * 2, ndf * 4, 3, 2, 1, bias=False),\n",
        "        #     nn.BatchNorm2d(ndf * 4),\n",
        "        #     nn.LeakyReLU(0.2, inplace=True),\n",
        "        #     # state size. (ndf*4) x 4 x 4\n",
        "        #     nn.Conv2d(ndf * 4, 1024, 4, 1, 0, bias=False),\n",
        "        #     # nn.BatchNorm2d(1024),\n",
        "        #     nn.LeakyReLU(0.2, inplace=True),\n",
        "        #     # nn.Sigmoid()\n",
        "        # )\n",
        "        # self.decoder = nn.Sequential(\n",
        "        #     # input is Z, going into a convolution\n",
        "        #     nn.ConvTranspose2d(     1024, ngf * 8, 4, 1, 0, bias=False),\n",
        "        #     nn.BatchNorm2d(ngf * 8),\n",
        "        #     nn.ReLU(True),\n",
        "        #     # state size. (ngf*8) x 4 x 4\n",
        "        #     nn.ConvTranspose2d(ngf * 8, ngf * 4, 3, 2, 1, bias=False),\n",
        "        #     nn.BatchNorm2d(ngf * 4),\n",
        "        #     nn.ReLU(True),\n",
        "        #     # state size. (ngf*4) x 8 x 8\n",
        "        #     nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "        #     nn.BatchNorm2d(ngf * 2),\n",
        "        #     nn.ReLU(True),\n",
        "        #     # state size. (ngf*2) x 16 x 16\n",
        "        #     nn.ConvTranspose2d(ngf * 2,     nc, 4, 2, 1, bias=False),\n",
        "        #     # nn.BatchNorm2d(ngf),\n",
        "        #     # nn.ReLU(True),\n",
        "        #     # state size. (ngf) x 32 x 32\n",
        "        #     # nn.ConvTranspose2d(    ngf,      nc, 4, 2, 1, bias=False),\n",
        "        #     # nn.Tanh()\n",
        "        #     nn.Sigmoid()\n",
        "        #     # state size. (nc) x 64 x 64\n",
        "        # )\n",
        "        #self.fc1 = nn.Linear(self.hidden_dim, 512)\n",
        "        self.fc21 = nn.Linear(self.hidden_dim, self.latent_size)\n",
        "        self.fc22 = nn.Linear(self.hidden_dim, self.latent_size)\n",
        "\n",
        "        #self.fc3 = nn.Linear(10, 512)\n",
        "        #self.fc4 = nn.Linear(512, self.hidden_dim)\n",
        "\n",
        "        self.lrelu = nn.LeakyReLU()\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Dir prior\n",
        "        self.prior_mean, self.prior_var = map(nn.Parameter, prior(self.latent_size, 0.3)) # 0.3 is a hyper param of Dirichlet distribution\n",
        "        self.prior_logvar = nn.Parameter(self.prior_var.log())\n",
        "        self.prior_mean.requires_grad = False\n",
        "        self.prior_var.requires_grad = False\n",
        "        self.prior_logvar.requires_grad = False\n",
        "\n",
        "\n",
        "    def encode(self, x):\n",
        "        encoding = self.encoder(x);\n",
        "        #h1 = self.fc1(encoding)\n",
        "        return self.fc21(encoding), self.fc22(encoding)\n",
        "\n",
        "    def decode(self, gauss_z):\n",
        "        dir_z = F.softmax(gauss_z,dim=1) #Reduntant, already done in forward\n",
        "        # This variable (z) can be treated as a variable that follows a Dirichlet distribution (a variable that can be interpreted as a probability that the sum is 1)\n",
        "        # Use the Softmax function to satisfy the simplex constraint\n",
        "        # シンプレックス制約を満たすようにソフトマックス関数を使用\n",
        "        #h3 = self.relu(self.fc3(dir_z))\n",
        "        #decode_input = self.fc4(h3)\n",
        "        #deconv_input = deconv_input.view(-1,1024,1,1)\n",
        "        return self.decoder(dir_z)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps*std\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        gauss_z = self.reparameterize(mu, logvar)\n",
        "        # gause_z is a variable that follows a multivariate normal distribution\n",
        "        # Inputting gause_z into softmax func yields a random variable that follows a Dirichlet distribution (Softmax func are used in decoder)\n",
        "        dir_z = F.softmax(gauss_z,dim=1) # This variable follows a Dirichlet distribution\n",
        "        return self.decode(gauss_z), mu, logvar, gauss_z, dir_z\n",
        "\n",
        "    # Reconstruction + KL divergence losses summed over all elements and batch\n",
        "    def loss_function(self, recon_x, x, mu, logvar, K):\n",
        "        beta = 1.0\n",
        "        # Apply sigmoid to the input data x to ensure values are between 0 and 1\n",
        "        x_sigmoid = torch.sigmoid(x)\n",
        "        BCE = F.binary_cross_entropy(recon_x, x_sigmoid, reduction='sum')\n",
        "        # Calculate BCE based on the actual input size\n",
        "        BCE = F.binary_cross_entropy(recon_x, x_sigmoid, reduction='sum')\n",
        "        # ディリクレ事前分布と変分事後分布とのKLを計算\n",
        "        # Calculating KL with Dirichlet prior and variational posterior distributions\n",
        "        # Original paper:\"Autoencodeing variational inference for topic model\"-https://arxiv.org/pdf/1703.01488\n",
        "        prior_mean = self.prior_mean.expand_as(mu)\n",
        "        prior_var = self.prior_var.expand_as(logvar)\n",
        "        prior_logvar = self.prior_logvar.expand_as(logvar)\n",
        "        var_division = logvar.exp() / prior_var # Σ_0 / Σ_1\n",
        "        diff = mu - prior_mean # μ_１ - μ_0\n",
        "        diff_term = diff *diff / prior_var # (μ_1 - μ_0)(μ_1 - μ_0)/Σ_1\n",
        "        logvar_division = prior_logvar - logvar # log|Σ_1| - log|Σ_0| = log(|Σ_1|/|Σ_2|)\n",
        "        # KL\n",
        "        KLD = 0.5 * ((var_division + diff_term + logvar_division).sum(1) - K)\n",
        "        self.last_KLD = torch.mean(KLD) #Used for reporting\n",
        "        self.last_BCE = BCE\n",
        "        return BCE + KLD\n",
        "\n",
        "\n",
        "model = Dir_VAE(input_size).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data,) in enumerate(train_loader): # Unpack only one element\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar, gauss_z, dir_z = model(data)\n",
        "\n",
        "        loss = model.loss_function(recon_batch, data, mu, logvar, 10)\n",
        "        loss = loss.mean()\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 10 == 0:\n",
        "            #print(f\"gause_z:{gauss_z[0]}\")\n",
        "            #print(f\"dir_z:{dir_z[0]},SUM:{torch.sum(dir_z[0])}\")\n",
        "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader)}%)] Loss:{loss.item() / len(data)}')\n",
        "            print(f'BCE loss {model.last_BCE}, KLD_loss {model.last_KLD}')\n",
        "\n",
        "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
        "          epoch, train_loss / len(train_loader.dataset)))\n",
        "\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for i, (data,) in enumerate(test_loader): # Unpack only one element\n",
        "            data = data.to(device)\n",
        "            recon_batch, mu, logvar, gauss_z, dir_z = model(data)\n",
        "            loss = model.loss_function(recon_batch, data, mu, logvar, 10)\n",
        "            test_loss += loss.mean()\n",
        "            test_loss.item()\n",
        "            if i == 0:\n",
        "                n = min(data.size(0), 18)\n",
        "                #comparison = torch.cat([data[:n],\n",
        "                #                      recon_batch.view(args.batch_size, 1, 28, 28)[:n]])\n",
        "                #save_image(comparison.cpu(),\n",
        "                #         'image/recon_' + str(epoch) + '.png', nrow=n)\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('====> Test set loss: {:.4f}'.format(test_loss))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 学習(Train)\n",
        "    for epoch in range(1, 10 + 1):\n",
        "        train(epoch)\n",
        "        test(epoch)\n",
        "        #with torch.no_grad():\n",
        "            #sample = torch.randn(64, args.category).to(device)\n",
        "            #sample = model.decode(sample).cpu()\n",
        "            #save_image(sample.view(64, 1, 28, 28),'image/sample_' + str(epoch) + '.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgOw9CXb2p_l",
        "outputId": "a8ab1aa0-4792-44c8-ae48-4dc198174ea0"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/14166 (0.0%)] Loss:25.042680740356445\n",
            "BCE loss 799.3179931640625, KLD_loss 2.047797918319702\n",
            "Train Epoch: 1 [320/14166 (2.2573363431151243%)] Loss:24.92779541015625\n",
            "BCE loss 796.27880859375, KLD_loss 1.4106773138046265\n",
            "Train Epoch: 1 [640/14166 (4.514672686230249%)] Loss:24.908340454101562\n",
            "BCE loss 796.26904296875, KLD_loss 0.7978402972221375\n",
            "Train Epoch: 1 [960/14166 (6.772009029345372%)] Loss:24.86931800842285\n",
            "BCE loss 795.5565795898438, KLD_loss 0.2615854740142822\n",
            "Train Epoch: 1 [1280/14166 (9.029345372460497%)] Loss:24.85173988342285\n",
            "BCE loss 795.1852416992188, KLD_loss 0.07043272256851196\n",
            "Train Epoch: 1 [1600/14166 (11.286681715575622%)] Loss:24.873863220214844\n",
            "BCE loss 795.8679809570312, KLD_loss 0.09561517834663391\n",
            "Train Epoch: 1 [1920/14166 (13.544018058690744%)] Loss:24.870481491088867\n",
            "BCE loss 795.76708984375, KLD_loss 0.08829174935817719\n",
            "Train Epoch: 1 [2240/14166 (15.801354401805868%)] Loss:24.851245880126953\n",
            "BCE loss 795.1669921875, KLD_loss 0.07283104956150055\n",
            "Train Epoch: 1 [2560/14166 (18.058690744920995%)] Loss:24.851659774780273\n",
            "BCE loss 795.1744384765625, KLD_loss 0.07874453067779541\n",
            "Train Epoch: 1 [2880/14166 (20.31602708803612%)] Loss:24.849258422851562\n",
            "BCE loss 795.1075439453125, KLD_loss 0.06877517700195312\n",
            "Train Epoch: 1 [3200/14166 (22.573363431151243%)] Loss:24.880939483642578\n",
            "BCE loss 796.062255859375, KLD_loss 0.12779755890369415\n",
            "Train Epoch: 1 [3520/14166 (24.830699774266364%)] Loss:24.84560203552246\n",
            "BCE loss 794.9168701171875, KLD_loss 0.14238892495632172\n",
            "Train Epoch: 1 [3840/14166 (27.08803611738149%)] Loss:24.83837890625\n",
            "BCE loss 794.705810546875, KLD_loss 0.12228290736675262\n",
            "Train Epoch: 1 [4160/14166 (29.345372460496613%)] Loss:24.868289947509766\n",
            "BCE loss 795.6271362304688, KLD_loss 0.15808944404125214\n",
            "Train Epoch: 1 [4480/14166 (31.602708803611737%)] Loss:24.850814819335938\n",
            "BCE loss 795.0101928710938, KLD_loss 0.21587856113910675\n",
            "Train Epoch: 1 [4800/14166 (33.86004514672686%)] Loss:24.85711669921875\n",
            "BCE loss 795.1595458984375, KLD_loss 0.2682431936264038\n",
            "Train Epoch: 1 [5120/14166 (36.11738148984199%)] Loss:24.871511459350586\n",
            "BCE loss 795.6129760742188, KLD_loss 0.2753692865371704\n",
            "Train Epoch: 1 [5440/14166 (38.37471783295711%)] Loss:24.837553024291992\n",
            "BCE loss 794.3657836914062, KLD_loss 0.43593403697013855\n",
            "Train Epoch: 1 [5760/14166 (40.63205417607224%)] Loss:24.85171890258789\n",
            "BCE loss 794.7281494140625, KLD_loss 0.5268373489379883\n",
            "Train Epoch: 1 [6080/14166 (42.88939051918736%)] Loss:24.839492797851562\n",
            "BCE loss 794.4147338867188, KLD_loss 0.4490205645561218\n",
            "Train Epoch: 1 [6400/14166 (45.146726862302486%)] Loss:24.835872650146484\n",
            "BCE loss 794.0349731445312, KLD_loss 0.7129712104797363\n",
            "Train Epoch: 1 [6720/14166 (47.40406320541761%)] Loss:24.823259353637695\n",
            "BCE loss 793.6002807617188, KLD_loss 0.7440180778503418\n",
            "Train Epoch: 1 [7040/14166 (49.66139954853273%)] Loss:24.83100128173828\n",
            "BCE loss 793.9356689453125, KLD_loss 0.6564547419548035\n",
            "Train Epoch: 1 [7360/14166 (51.918735891647856%)] Loss:24.800508499145508\n",
            "BCE loss 792.7647705078125, KLD_loss 0.8514498472213745\n",
            "Train Epoch: 1 [7680/14166 (54.17607223476298%)] Loss:24.836288452148438\n",
            "BCE loss 794.0073852539062, KLD_loss 0.7538937330245972\n",
            "Train Epoch: 1 [8000/14166 (56.433408577878104%)] Loss:24.824926376342773\n",
            "BCE loss 793.7777709960938, KLD_loss 0.6198852062225342\n",
            "Train Epoch: 1 [8320/14166 (58.690744920993225%)] Loss:24.80824851989746\n",
            "BCE loss 792.7813110351562, KLD_loss 1.0826525688171387\n",
            "Train Epoch: 1 [8640/14166 (60.94808126410835%)] Loss:24.83745002746582\n",
            "BCE loss 793.8294067382812, KLD_loss 0.9690039753913879\n",
            "Train Epoch: 1 [8960/14166 (63.205417607223474%)] Loss:24.81685447692871\n",
            "BCE loss 793.1688842773438, KLD_loss 0.9704051613807678\n",
            "Train Epoch: 1 [9280/14166 (65.4627539503386%)] Loss:24.83677864074707\n",
            "BCE loss 793.8092041015625, KLD_loss 0.9677314162254333\n",
            "Train Epoch: 1 [9600/14166 (67.72009029345372%)] Loss:24.806455612182617\n",
            "BCE loss 792.6760864257812, KLD_loss 1.1304852962493896\n",
            "Train Epoch: 1 [9920/14166 (69.97742663656885%)] Loss:24.800472259521484\n",
            "BCE loss 792.2915649414062, KLD_loss 1.32354736328125\n",
            "Train Epoch: 1 [10240/14166 (72.23476297968398%)] Loss:24.83833885192871\n",
            "BCE loss 793.8173217773438, KLD_loss 1.0095083713531494\n",
            "Train Epoch: 1 [10560/14166 (74.49209932279909%)] Loss:24.816755294799805\n",
            "BCE loss 792.889404296875, KLD_loss 1.2467433214187622\n",
            "Train Epoch: 1 [10880/14166 (76.74943566591422%)] Loss:24.816871643066406\n",
            "BCE loss 793.0859375, KLD_loss 1.0539093017578125\n",
            "Train Epoch: 1 [11200/14166 (79.00677200902935%)] Loss:24.826444625854492\n",
            "BCE loss 793.30224609375, KLD_loss 1.143996238708496\n",
            "Train Epoch: 1 [11520/14166 (81.26410835214448%)] Loss:24.799354553222656\n",
            "BCE loss 792.30126953125, KLD_loss 1.2780990600585938\n",
            "Train Epoch: 1 [11840/14166 (83.52144469525959%)] Loss:24.802326202392578\n",
            "BCE loss 792.582275390625, KLD_loss 1.0921788215637207\n",
            "Train Epoch: 1 [12160/14166 (85.77878103837472%)] Loss:24.816198348999023\n",
            "BCE loss 792.61865234375, KLD_loss 1.4997270107269287\n",
            "Train Epoch: 1 [12480/14166 (88.03611738148985%)] Loss:24.83817481994629\n",
            "BCE loss 793.5885009765625, KLD_loss 1.2330269813537598\n",
            "Train Epoch: 1 [12800/14166 (90.29345372460497%)] Loss:24.822463989257812\n",
            "BCE loss 792.9498901367188, KLD_loss 1.3689700365066528\n",
            "Train Epoch: 1 [13120/14166 (92.55079006772009%)] Loss:24.81144905090332\n",
            "BCE loss 792.6981811523438, KLD_loss 1.2682267427444458\n",
            "Train Epoch: 1 [13440/14166 (94.80812641083521%)] Loss:24.810367584228516\n",
            "BCE loss 792.4931030273438, KLD_loss 1.4386686086654663\n",
            "Train Epoch: 1 [13760/14166 (97.06546275395034%)] Loss:24.79810333251953\n",
            "BCE loss 792.10009765625, KLD_loss 1.4391931295394897\n",
            "Train Epoch: 1 [14080/14166 (99.32279909706546%)] Loss:24.7943172454834\n",
            "BCE loss 791.8916625976562, KLD_loss 1.5264461040496826\n",
            "====> Epoch: 1 Average loss: 24.8394\n",
            "====> Test set loss: 24.8134\n",
            "Train Epoch: 2 [0/14166 (0.0%)] Loss:24.774812698364258\n",
            "BCE loss 791.1340942382812, KLD_loss 1.6598831415176392\n",
            "Train Epoch: 2 [320/14166 (2.2573363431151243%)] Loss:24.830854415893555\n",
            "BCE loss 793.3441772460938, KLD_loss 1.2431793212890625\n",
            "Train Epoch: 2 [640/14166 (4.514672686230249%)] Loss:24.80225372314453\n",
            "BCE loss 792.3909301757812, KLD_loss 1.2812843322753906\n",
            "Train Epoch: 2 [960/14166 (6.772009029345372%)] Loss:24.78317642211914\n",
            "BCE loss 791.8025512695312, KLD_loss 1.2590761184692383\n",
            "Train Epoch: 2 [1280/14166 (9.029345372460497%)] Loss:24.789203643798828\n",
            "BCE loss 791.903076171875, KLD_loss 1.3514800071716309\n",
            "Train Epoch: 2 [1600/14166 (11.286681715575622%)] Loss:24.83829116821289\n",
            "BCE loss 793.577392578125, KLD_loss 1.2479838132858276\n",
            "Train Epoch: 2 [1920/14166 (13.544018058690744%)] Loss:24.81290054321289\n",
            "BCE loss 792.60009765625, KLD_loss 1.4127880334854126\n",
            "Train Epoch: 2 [2240/14166 (15.801354401805868%)] Loss:24.833288192749023\n",
            "BCE loss 793.4513549804688, KLD_loss 1.2138770818710327\n",
            "Train Epoch: 2 [2560/14166 (18.058690744920995%)] Loss:24.828371047973633\n",
            "BCE loss 793.23681640625, KLD_loss 1.2710789442062378\n",
            "Train Epoch: 2 [2880/14166 (20.31602708803612%)] Loss:24.821935653686523\n",
            "BCE loss 792.8567504882812, KLD_loss 1.445223093032837\n",
            "Train Epoch: 2 [3200/14166 (22.573363431151243%)] Loss:24.817115783691406\n",
            "BCE loss 792.9224853515625, KLD_loss 1.225198745727539\n",
            "Train Epoch: 2 [3520/14166 (24.830699774266364%)] Loss:24.821168899536133\n",
            "BCE loss 793.0961303710938, KLD_loss 1.181247353553772\n",
            "Train Epoch: 2 [3840/14166 (27.08803611738149%)] Loss:24.80315589904785\n",
            "BCE loss 792.117431640625, KLD_loss 1.5834578275680542\n",
            "Train Epoch: 2 [4160/14166 (29.345372460496613%)] Loss:24.829837799072266\n",
            "BCE loss 793.34228515625, KLD_loss 1.212552785873413\n",
            "Train Epoch: 2 [4480/14166 (31.602708803611737%)] Loss:24.830081939697266\n",
            "BCE loss 793.1055908203125, KLD_loss 1.4570543766021729\n",
            "Train Epoch: 2 [4800/14166 (33.86004514672686%)] Loss:24.772499084472656\n",
            "BCE loss 791.0238647460938, KLD_loss 1.696083664894104\n",
            "Train Epoch: 2 [5120/14166 (36.11738148984199%)] Loss:24.807727813720703\n",
            "BCE loss 792.4560546875, KLD_loss 1.3912384510040283\n",
            "Train Epoch: 2 [5440/14166 (38.37471783295711%)] Loss:24.79961395263672\n",
            "BCE loss 792.3317260742188, KLD_loss 1.2559367418289185\n",
            "Train Epoch: 2 [5760/14166 (40.63205417607224%)] Loss:24.799060821533203\n",
            "BCE loss 792.0892333984375, KLD_loss 1.4806913137435913\n",
            "Train Epoch: 2 [6080/14166 (42.88939051918736%)] Loss:24.847742080688477\n",
            "BCE loss 793.8963012695312, KLD_loss 1.2314181327819824\n",
            "Train Epoch: 2 [6400/14166 (45.146726862302486%)] Loss:24.825712203979492\n",
            "BCE loss 793.0963745117188, KLD_loss 1.3263797760009766\n",
            "Train Epoch: 2 [6720/14166 (47.40406320541761%)] Loss:24.813486099243164\n",
            "BCE loss 792.4886474609375, KLD_loss 1.542921781539917\n",
            "Train Epoch: 2 [7040/14166 (49.66139954853273%)] Loss:24.781524658203125\n",
            "BCE loss 791.2845458984375, KLD_loss 1.724160075187683\n",
            "Train Epoch: 2 [7360/14166 (51.918735891647856%)] Loss:24.802391052246094\n",
            "BCE loss 792.13232421875, KLD_loss 1.54417884349823\n",
            "Train Epoch: 2 [7680/14166 (54.17607223476298%)] Loss:24.782100677490234\n",
            "BCE loss 791.4248657226562, KLD_loss 1.6023555994033813\n",
            "Train Epoch: 2 [8000/14166 (56.433408577878104%)] Loss:24.800065994262695\n",
            "BCE loss 791.9515991210938, KLD_loss 1.65055251121521\n",
            "Train Epoch: 2 [8320/14166 (58.690744920993225%)] Loss:24.8021240234375\n",
            "BCE loss 792.0856323242188, KLD_loss 1.58224356174469\n",
            "Train Epoch: 2 [8640/14166 (60.94808126410835%)] Loss:24.77996253967285\n",
            "BCE loss 791.5196533203125, KLD_loss 1.4391461610794067\n",
            "Train Epoch: 2 [8960/14166 (63.205417607223474%)] Loss:24.829246520996094\n",
            "BCE loss 793.25390625, KLD_loss 1.2819520235061646\n",
            "Train Epoch: 2 [9280/14166 (65.4627539503386%)] Loss:24.809406280517578\n",
            "BCE loss 792.4161987304688, KLD_loss 1.4847776889801025\n",
            "Train Epoch: 2 [9600/14166 (67.72009029345372%)] Loss:24.783023834228516\n",
            "BCE loss 791.5771484375, KLD_loss 1.4796308279037476\n",
            "Train Epoch: 2 [9920/14166 (69.97742663656885%)] Loss:24.821544647216797\n",
            "BCE loss 792.8526611328125, KLD_loss 1.4367280006408691\n",
            "Train Epoch: 2 [10240/14166 (72.23476297968398%)] Loss:24.81079864501953\n",
            "BCE loss 792.6029052734375, KLD_loss 1.342607855796814\n",
            "Train Epoch: 2 [10560/14166 (74.49209932279909%)] Loss:24.823928833007812\n",
            "BCE loss 792.9058837890625, KLD_loss 1.4598380327224731\n",
            "Train Epoch: 2 [10880/14166 (76.74943566591422%)] Loss:24.75860595703125\n",
            "BCE loss 790.8323974609375, KLD_loss 1.4429930448532104\n",
            "Train Epoch: 2 [11200/14166 (79.00677200902935%)] Loss:24.838403701782227\n",
            "BCE loss 793.5493774414062, KLD_loss 1.2795524597167969\n",
            "Train Epoch: 2 [11520/14166 (81.26410835214448%)] Loss:24.791269302368164\n",
            "BCE loss 791.6272583007812, KLD_loss 1.6934146881103516\n",
            "Train Epoch: 2 [11840/14166 (83.52144469525959%)] Loss:24.776229858398438\n",
            "BCE loss 791.1027221679688, KLD_loss 1.7365667819976807\n",
            "Train Epoch: 2 [12160/14166 (85.77878103837472%)] Loss:24.805503845214844\n",
            "BCE loss 792.08154296875, KLD_loss 1.6946220397949219\n",
            "Train Epoch: 2 [12480/14166 (88.03611738148985%)] Loss:24.82828712463379\n",
            "BCE loss 793.0651245117188, KLD_loss 1.4400421380996704\n",
            "Train Epoch: 2 [12800/14166 (90.29345372460497%)] Loss:24.81072425842285\n",
            "BCE loss 792.37646484375, KLD_loss 1.5667024850845337\n",
            "Train Epoch: 2 [13120/14166 (92.55079006772009%)] Loss:24.796586990356445\n",
            "BCE loss 791.800048828125, KLD_loss 1.6908278465270996\n",
            "Train Epoch: 2 [13440/14166 (94.80812641083521%)] Loss:24.80194854736328\n",
            "BCE loss 792.1649780273438, KLD_loss 1.4973982572555542\n",
            "Train Epoch: 2 [13760/14166 (97.06546275395034%)] Loss:24.81143569946289\n",
            "BCE loss 792.5136108398438, KLD_loss 1.4523276090621948\n",
            "Train Epoch: 2 [14080/14166 (99.32279909706546%)] Loss:24.82788848876953\n",
            "BCE loss 793.0442504882812, KLD_loss 1.448215365409851\n",
            "====> Epoch: 2 Average loss: 24.8103\n",
            "====> Test set loss: 24.8079\n",
            "Train Epoch: 3 [0/14166 (0.0%)] Loss:24.812925338745117\n",
            "BCE loss 792.435791015625, KLD_loss 1.5777925252914429\n",
            "Train Epoch: 3 [320/14166 (2.2573363431151243%)] Loss:24.810253143310547\n",
            "BCE loss 792.3877563476562, KLD_loss 1.540299415588379\n",
            "Train Epoch: 3 [640/14166 (4.514672686230249%)] Loss:24.788070678710938\n",
            "BCE loss 791.4254760742188, KLD_loss 1.792808175086975\n",
            "Train Epoch: 3 [960/14166 (6.772009029345372%)] Loss:24.80672264099121\n",
            "BCE loss 792.4053955078125, KLD_loss 1.4097343683242798\n",
            "Train Epoch: 3 [1280/14166 (9.029345372460497%)] Loss:24.804277420043945\n",
            "BCE loss 792.0384521484375, KLD_loss 1.6984179019927979\n",
            "Train Epoch: 3 [1600/14166 (11.286681715575622%)] Loss:24.79146957397461\n",
            "BCE loss 791.8681640625, KLD_loss 1.4589086771011353\n",
            "Train Epoch: 3 [1920/14166 (13.544018058690744%)] Loss:24.799612045288086\n",
            "BCE loss 791.6878051757812, KLD_loss 1.8997772932052612\n",
            "Train Epoch: 3 [2240/14166 (15.801354401805868%)] Loss:24.83692741394043\n",
            "BCE loss 793.2049560546875, KLD_loss 1.5766899585723877\n",
            "Train Epoch: 3 [2560/14166 (18.058690744920995%)] Loss:24.787906646728516\n",
            "BCE loss 791.7623291015625, KLD_loss 1.4506183862686157\n",
            "Train Epoch: 3 [2880/14166 (20.31602708803612%)] Loss:24.807823181152344\n",
            "BCE loss 792.2274169921875, KLD_loss 1.622896432876587\n",
            "Train Epoch: 3 [3200/14166 (22.573363431151243%)] Loss:24.803356170654297\n",
            "BCE loss 792.2776489257812, KLD_loss 1.4297051429748535\n",
            "Train Epoch: 3 [3520/14166 (24.830699774266364%)] Loss:24.788005828857422\n",
            "BCE loss 791.7890625, KLD_loss 1.4271068572998047\n",
            "Train Epoch: 3 [3840/14166 (27.08803611738149%)] Loss:24.811494827270508\n",
            "BCE loss 792.5796508789062, KLD_loss 1.3882064819335938\n",
            "Train Epoch: 3 [4160/14166 (29.345372460496613%)] Loss:24.812599182128906\n",
            "BCE loss 792.6224975585938, KLD_loss 1.3807120323181152\n",
            "Train Epoch: 3 [4480/14166 (31.602708803611737%)] Loss:24.769418716430664\n",
            "BCE loss 790.8080444335938, KLD_loss 1.8133537769317627\n",
            "Train Epoch: 3 [4800/14166 (33.86004514672686%)] Loss:24.820640563964844\n",
            "BCE loss 792.3903198242188, KLD_loss 1.8701814413070679\n",
            "Train Epoch: 3 [5120/14166 (36.11738148984199%)] Loss:24.792339324951172\n",
            "BCE loss 791.748046875, KLD_loss 1.6067782640457153\n",
            "Train Epoch: 3 [5440/14166 (38.37471783295711%)] Loss:24.805213928222656\n",
            "BCE loss 792.198486328125, KLD_loss 1.5684462785720825\n",
            "Train Epoch: 3 [5760/14166 (40.63205417607224%)] Loss:24.819673538208008\n",
            "BCE loss 792.7178955078125, KLD_loss 1.5116318464279175\n",
            "Train Epoch: 3 [6080/14166 (42.88939051918736%)] Loss:24.7928466796875\n",
            "BCE loss 791.7280883789062, KLD_loss 1.6429946422576904\n",
            "Train Epoch: 3 [6400/14166 (45.146726862302486%)] Loss:24.773975372314453\n",
            "BCE loss 791.1537475585938, KLD_loss 1.6134390830993652\n",
            "Train Epoch: 3 [6720/14166 (47.40406320541761%)] Loss:24.837745666503906\n",
            "BCE loss 793.4051513671875, KLD_loss 1.4027220010757446\n",
            "Train Epoch: 3 [7040/14166 (49.66139954853273%)] Loss:24.82249641418457\n",
            "BCE loss 792.9230346679688, KLD_loss 1.396877646446228\n",
            "Train Epoch: 3 [7360/14166 (51.918735891647856%)] Loss:24.80168914794922\n",
            "BCE loss 792.1580810546875, KLD_loss 1.4959640502929688\n",
            "Train Epoch: 3 [7680/14166 (54.17607223476298%)] Loss:24.79524040222168\n",
            "BCE loss 791.7100830078125, KLD_loss 1.7376450300216675\n",
            "Train Epoch: 3 [8000/14166 (56.433408577878104%)] Loss:24.80439567565918\n",
            "BCE loss 791.9757690429688, KLD_loss 1.7649202346801758\n",
            "Train Epoch: 3 [8320/14166 (58.690744920993225%)] Loss:24.773672103881836\n",
            "BCE loss 790.8604736328125, KLD_loss 1.8970175981521606\n",
            "Train Epoch: 3 [8640/14166 (60.94808126410835%)] Loss:24.825349807739258\n",
            "BCE loss 793.1114501953125, KLD_loss 1.299695372581482\n",
            "Train Epoch: 3 [8960/14166 (63.205417607223474%)] Loss:24.76479148864746\n",
            "BCE loss 790.7603149414062, KLD_loss 1.713038444519043\n",
            "Train Epoch: 3 [9280/14166 (65.4627539503386%)] Loss:24.79897689819336\n",
            "BCE loss 791.943603515625, KLD_loss 1.623664379119873\n",
            "Train Epoch: 3 [9600/14166 (67.72009029345372%)] Loss:24.803068161010742\n",
            "BCE loss 791.956298828125, KLD_loss 1.7418551445007324\n",
            "Train Epoch: 3 [9920/14166 (69.97742663656885%)] Loss:24.817108154296875\n",
            "BCE loss 792.57177734375, KLD_loss 1.575646996498108\n",
            "Train Epoch: 3 [10240/14166 (72.23476297968398%)] Loss:24.81410789489746\n",
            "BCE loss 792.3818969726562, KLD_loss 1.6695177555084229\n",
            "Train Epoch: 3 [10560/14166 (74.49209932279909%)] Loss:24.78165626525879\n",
            "BCE loss 791.4356689453125, KLD_loss 1.5772939920425415\n",
            "Train Epoch: 3 [10880/14166 (76.74943566591422%)] Loss:24.828195571899414\n",
            "BCE loss 792.6898193359375, KLD_loss 1.8124111890792847\n",
            "Train Epoch: 3 [11200/14166 (79.00677200902935%)] Loss:24.78861427307129\n",
            "BCE loss 791.3817749023438, KLD_loss 1.8539016246795654\n",
            "Train Epoch: 3 [11520/14166 (81.26410835214448%)] Loss:24.792224884033203\n",
            "BCE loss 791.53759765625, KLD_loss 1.8134574890136719\n",
            "Train Epoch: 3 [11840/14166 (83.52144469525959%)] Loss:24.81485366821289\n",
            "BCE loss 792.43798828125, KLD_loss 1.6372874975204468\n",
            "Train Epoch: 3 [12160/14166 (85.77878103837472%)] Loss:24.80441665649414\n",
            "BCE loss 792.27685546875, KLD_loss 1.464478850364685\n",
            "Train Epoch: 3 [12480/14166 (88.03611738148985%)] Loss:24.769329071044922\n",
            "BCE loss 790.9277954101562, KLD_loss 1.690664529800415\n",
            "Train Epoch: 3 [12800/14166 (90.29345372460497%)] Loss:24.777198791503906\n",
            "BCE loss 791.3294677734375, KLD_loss 1.5409162044525146\n",
            "Train Epoch: 3 [13120/14166 (92.55079006772009%)] Loss:24.771045684814453\n",
            "BCE loss 791.1920776367188, KLD_loss 1.4814066886901855\n",
            "Train Epoch: 3 [13440/14166 (94.80812641083521%)] Loss:24.803482055664062\n",
            "BCE loss 792.038818359375, KLD_loss 1.6726479530334473\n",
            "Train Epoch: 3 [13760/14166 (97.06546275395034%)] Loss:24.763916015625\n",
            "BCE loss 790.4044189453125, KLD_loss 2.0409343242645264\n",
            "Train Epoch: 3 [14080/14166 (99.32279909706546%)] Loss:24.792030334472656\n",
            "BCE loss 791.4974365234375, KLD_loss 1.847503662109375\n",
            "====> Epoch: 3 Average loss: 24.8042\n",
            "====> Test set loss: 24.8036\n",
            "Train Epoch: 4 [0/14166 (0.0%)] Loss:24.81730842590332\n",
            "BCE loss 792.5580444335938, KLD_loss 1.595798373222351\n",
            "Train Epoch: 4 [320/14166 (2.2573363431151243%)] Loss:24.78948211669922\n",
            "BCE loss 791.1541137695312, KLD_loss 2.1092944145202637\n",
            "Train Epoch: 4 [640/14166 (4.514672686230249%)] Loss:24.80037498474121\n",
            "BCE loss 791.892822265625, KLD_loss 1.719191551208496\n",
            "Train Epoch: 4 [960/14166 (6.772009029345372%)] Loss:24.818452835083008\n",
            "BCE loss 792.4921264648438, KLD_loss 1.6983857154846191\n",
            "Train Epoch: 4 [1280/14166 (9.029345372460497%)] Loss:24.774805068969727\n",
            "BCE loss 791.0274047851562, KLD_loss 1.7662887573242188\n",
            "Train Epoch: 4 [1600/14166 (11.286681715575622%)] Loss:24.797941207885742\n",
            "BCE loss 792.0630493164062, KLD_loss 1.4710924625396729\n",
            "Train Epoch: 4 [1920/14166 (13.544018058690744%)] Loss:24.828182220458984\n",
            "BCE loss 793.1776123046875, KLD_loss 1.324181079864502\n",
            "Train Epoch: 4 [2240/14166 (15.801354401805868%)] Loss:24.80525016784668\n",
            "BCE loss 792.2008666992188, KLD_loss 1.5671815872192383\n",
            "Train Epoch: 4 [2560/14166 (18.058690744920995%)] Loss:24.795934677124023\n",
            "BCE loss 791.7819213867188, KLD_loss 1.6879922151565552\n",
            "Train Epoch: 4 [2880/14166 (20.31602708803612%)] Loss:24.809335708618164\n",
            "BCE loss 792.0477294921875, KLD_loss 1.8510175943374634\n",
            "Train Epoch: 4 [3200/14166 (22.573363431151243%)] Loss:24.818517684936523\n",
            "BCE loss 792.5660400390625, KLD_loss 1.626537799835205\n",
            "Train Epoch: 4 [3520/14166 (24.830699774266364%)] Loss:24.767532348632812\n",
            "BCE loss 790.68798828125, KLD_loss 1.8731184005737305\n",
            "Train Epoch: 4 [3840/14166 (27.08803611738149%)] Loss:24.79822540283203\n",
            "BCE loss 792.0131225585938, KLD_loss 1.5300447940826416\n",
            "Train Epoch: 4 [4160/14166 (29.345372460496613%)] Loss:24.796039581298828\n",
            "BCE loss 791.7394409179688, KLD_loss 1.7338197231292725\n",
            "Train Epoch: 4 [4480/14166 (31.602708803611737%)] Loss:24.8176326751709\n",
            "BCE loss 792.6240234375, KLD_loss 1.540254831314087\n",
            "Train Epoch: 4 [4800/14166 (33.86004514672686%)] Loss:24.825164794921875\n",
            "BCE loss 792.805419921875, KLD_loss 1.5998438596725464\n",
            "Train Epoch: 4 [5120/14166 (36.11738148984199%)] Loss:24.844823837280273\n",
            "BCE loss 793.396728515625, KLD_loss 1.6376738548278809\n",
            "Train Epoch: 4 [5440/14166 (38.37471783295711%)] Loss:24.814197540283203\n",
            "BCE loss 792.2471923828125, KLD_loss 1.8071308135986328\n",
            "Train Epoch: 4 [5760/14166 (40.63205417607224%)] Loss:24.79180908203125\n",
            "BCE loss 791.694580078125, KLD_loss 1.6433151960372925\n",
            "Train Epoch: 4 [6080/14166 (42.88939051918736%)] Loss:24.811843872070312\n",
            "BCE loss 792.2763671875, KLD_loss 1.702677607536316\n",
            "Train Epoch: 4 [6400/14166 (45.146726862302486%)] Loss:24.774959564208984\n",
            "BCE loss 790.988037109375, KLD_loss 1.8106493949890137\n",
            "Train Epoch: 4 [6720/14166 (47.40406320541761%)] Loss:24.805204391479492\n",
            "BCE loss 791.7989501953125, KLD_loss 1.967560052871704\n",
            "Train Epoch: 4 [7040/14166 (49.66139954853273%)] Loss:24.792251586914062\n",
            "BCE loss 791.5811767578125, KLD_loss 1.7709074020385742\n",
            "Train Epoch: 4 [7360/14166 (51.918735891647856%)] Loss:24.813875198364258\n",
            "BCE loss 792.409912109375, KLD_loss 1.6341208219528198\n",
            "Train Epoch: 4 [7680/14166 (54.17607223476298%)] Loss:24.78083610534668\n",
            "BCE loss 790.9690551757812, KLD_loss 2.017641305923462\n",
            "Train Epoch: 4 [8000/14166 (56.433408577878104%)] Loss:24.79607582092285\n",
            "BCE loss 791.7612915039062, KLD_loss 1.7131187915802002\n",
            "Train Epoch: 4 [8320/14166 (58.690744920993225%)] Loss:24.77752113342285\n",
            "BCE loss 790.9788818359375, KLD_loss 1.9018001556396484\n",
            "Train Epoch: 4 [8640/14166 (60.94808126410835%)] Loss:24.82941246032715\n",
            "BCE loss 793.0029296875, KLD_loss 1.538288950920105\n",
            "Train Epoch: 4 [8960/14166 (63.205417607223474%)] Loss:24.80277442932129\n",
            "BCE loss 791.960205078125, KLD_loss 1.728513479232788\n",
            "Train Epoch: 4 [9280/14166 (65.4627539503386%)] Loss:24.829158782958984\n",
            "BCE loss 792.738037109375, KLD_loss 1.7950546741485596\n",
            "Train Epoch: 4 [9600/14166 (67.72009029345372%)] Loss:24.787160873413086\n",
            "BCE loss 791.0401000976562, KLD_loss 2.1490635871887207\n",
            "Train Epoch: 4 [9920/14166 (69.97742663656885%)] Loss:24.79250717163086\n",
            "BCE loss 791.8009643554688, KLD_loss 1.5592827796936035\n",
            "Train Epoch: 4 [10240/14166 (72.23476297968398%)] Loss:24.786579132080078\n",
            "BCE loss 791.36328125, KLD_loss 1.8072757720947266\n",
            "Train Epoch: 4 [10560/14166 (74.49209932279909%)] Loss:24.80217933654785\n",
            "BCE loss 792.0972290039062, KLD_loss 1.5724793672561646\n",
            "Train Epoch: 4 [10880/14166 (76.74943566591422%)] Loss:24.75341796875\n",
            "BCE loss 790.2890014648438, KLD_loss 1.8204773664474487\n",
            "Train Epoch: 4 [11200/14166 (79.00677200902935%)] Loss:24.802762985229492\n",
            "BCE loss 791.9515380859375, KLD_loss 1.7368850708007812\n",
            "Train Epoch: 4 [11520/14166 (81.26410835214448%)] Loss:24.79088020324707\n",
            "BCE loss 791.1865234375, KLD_loss 2.121626853942871\n",
            "Train Epoch: 4 [11840/14166 (83.52144469525959%)] Loss:24.8167724609375\n",
            "BCE loss 792.1029052734375, KLD_loss 2.033812999725342\n",
            "Train Epoch: 4 [12160/14166 (85.77878103837472%)] Loss:24.82306480407715\n",
            "BCE loss 792.507080078125, KLD_loss 1.8309247493743896\n",
            "Train Epoch: 4 [12480/14166 (88.03611738148985%)] Loss:24.809234619140625\n",
            "BCE loss 792.2548828125, KLD_loss 1.6406217813491821\n",
            "Train Epoch: 4 [12800/14166 (90.29345372460497%)] Loss:24.83648681640625\n",
            "BCE loss 793.0975952148438, KLD_loss 1.669976830482483\n",
            "Train Epoch: 4 [13120/14166 (92.55079006772009%)] Loss:24.81205177307129\n",
            "BCE loss 792.113037109375, KLD_loss 1.8725898265838623\n",
            "Train Epoch: 4 [13440/14166 (94.80812641083521%)] Loss:24.837244033813477\n",
            "BCE loss 792.9217529296875, KLD_loss 1.8700593709945679\n",
            "Train Epoch: 4 [13760/14166 (97.06546275395034%)] Loss:24.771556854248047\n",
            "BCE loss 790.6807861328125, KLD_loss 2.0090243816375732\n",
            "Train Epoch: 4 [14080/14166 (99.32279909706546%)] Loss:24.785015106201172\n",
            "BCE loss 791.254150390625, KLD_loss 1.866288423538208\n",
            "====> Epoch: 4 Average loss: 24.8006\n",
            "====> Test set loss: 24.7991\n",
            "Train Epoch: 5 [0/14166 (0.0%)] Loss:24.81467056274414\n",
            "BCE loss 792.3023681640625, KLD_loss 1.7670785188674927\n",
            "Train Epoch: 5 [320/14166 (2.2573363431151243%)] Loss:24.811124801635742\n",
            "BCE loss 791.9520263671875, KLD_loss 2.004035472869873\n",
            "Train Epoch: 5 [640/14166 (4.514672686230249%)] Loss:24.825332641601562\n",
            "BCE loss 792.557373046875, KLD_loss 1.853325366973877\n",
            "Train Epoch: 5 [960/14166 (6.772009029345372%)] Loss:24.77203369140625\n",
            "BCE loss 790.7310791015625, KLD_loss 1.9739737510681152\n",
            "Train Epoch: 5 [1280/14166 (9.029345372460497%)] Loss:24.817562103271484\n",
            "BCE loss 792.3040161132812, KLD_loss 1.8579838275909424\n",
            "Train Epoch: 5 [1600/14166 (11.286681715575622%)] Loss:24.83447265625\n",
            "BCE loss 792.9517211914062, KLD_loss 1.751423716545105\n",
            "Train Epoch: 5 [1920/14166 (13.544018058690744%)] Loss:24.79993438720703\n",
            "BCE loss 791.8988037109375, KLD_loss 1.6990935802459717\n",
            "Train Epoch: 5 [2240/14166 (15.801354401805868%)] Loss:24.79709815979004\n",
            "BCE loss 791.6597290039062, KLD_loss 1.8474476337432861\n",
            "Train Epoch: 5 [2560/14166 (18.058690744920995%)] Loss:24.813270568847656\n",
            "BCE loss 792.2823486328125, KLD_loss 1.7423515319824219\n",
            "Train Epoch: 5 [2880/14166 (20.31602708803612%)] Loss:24.779157638549805\n",
            "BCE loss 791.0967407226562, KLD_loss 1.8363146781921387\n",
            "Train Epoch: 5 [3200/14166 (22.573363431151243%)] Loss:24.75821304321289\n",
            "BCE loss 790.4210815429688, KLD_loss 1.8417303562164307\n",
            "Train Epoch: 5 [3520/14166 (24.830699774266364%)] Loss:24.77756690979004\n",
            "BCE loss 791.2840576171875, KLD_loss 1.5980983972549438\n",
            "Train Epoch: 5 [3840/14166 (27.08803611738149%)] Loss:24.759340286254883\n",
            "BCE loss 790.31640625, KLD_loss 1.9824693202972412\n",
            "Train Epoch: 5 [4160/14166 (29.345372460496613%)] Loss:24.767799377441406\n",
            "BCE loss 790.5239868164062, KLD_loss 2.0455493927001953\n",
            "Train Epoch: 5 [4480/14166 (31.602708803611737%)] Loss:24.75952911376953\n",
            "BCE loss 790.25048828125, KLD_loss 2.054450511932373\n",
            "Train Epoch: 5 [4800/14166 (33.86004514672686%)] Loss:24.786619186401367\n",
            "BCE loss 791.439208984375, KLD_loss 1.732590675354004\n",
            "Train Epoch: 5 [5120/14166 (36.11738148984199%)] Loss:24.787504196166992\n",
            "BCE loss 791.4927978515625, KLD_loss 1.70733642578125\n",
            "Train Epoch: 5 [5440/14166 (38.37471783295711%)] Loss:24.774477005004883\n",
            "BCE loss 790.7576904296875, KLD_loss 2.025599241256714\n",
            "Train Epoch: 5 [5760/14166 (40.63205417607224%)] Loss:24.80323028564453\n",
            "BCE loss 792.0487060546875, KLD_loss 1.6546669006347656\n",
            "Train Epoch: 5 [6080/14166 (42.88939051918736%)] Loss:24.783540725708008\n",
            "BCE loss 791.238525390625, KLD_loss 1.834763765335083\n",
            "Train Epoch: 5 [6400/14166 (45.146726862302486%)] Loss:24.794878005981445\n",
            "BCE loss 791.6669921875, KLD_loss 1.769100546836853\n",
            "Train Epoch: 5 [6720/14166 (47.40406320541761%)] Loss:24.79720115661621\n",
            "BCE loss 791.5039672851562, KLD_loss 2.006507396697998\n",
            "Train Epoch: 5 [7040/14166 (49.66139954853273%)] Loss:24.83123016357422\n",
            "BCE loss 792.613037109375, KLD_loss 1.986397624015808\n",
            "Train Epoch: 5 [7360/14166 (51.918735891647856%)] Loss:24.820037841796875\n",
            "BCE loss 792.5235595703125, KLD_loss 1.717721939086914\n",
            "Train Epoch: 5 [7680/14166 (54.17607223476298%)] Loss:24.775041580200195\n",
            "BCE loss 790.8623046875, KLD_loss 1.939026951789856\n",
            "Train Epoch: 5 [8000/14166 (56.433408577878104%)] Loss:24.78400993347168\n",
            "BCE loss 791.2369995117188, KLD_loss 1.8513063192367554\n",
            "Train Epoch: 5 [8320/14166 (58.690744920993225%)] Loss:24.825021743774414\n",
            "BCE loss 792.6221923828125, KLD_loss 1.7785203456878662\n",
            "Train Epoch: 5 [8640/14166 (60.94808126410835%)] Loss:24.80541229248047\n",
            "BCE loss 791.9332275390625, KLD_loss 1.8399136066436768\n",
            "Train Epoch: 5 [8960/14166 (63.205417607223474%)] Loss:24.8088321685791\n",
            "BCE loss 792.0987548828125, KLD_loss 1.7839243412017822\n",
            "Train Epoch: 5 [9280/14166 (65.4627539503386%)] Loss:24.787601470947266\n",
            "BCE loss 791.2593994140625, KLD_loss 1.943882703781128\n",
            "Train Epoch: 5 [9600/14166 (67.72009029345372%)] Loss:24.785091400146484\n",
            "BCE loss 791.3220825195312, KLD_loss 1.8008193969726562\n",
            "Train Epoch: 5 [9920/14166 (69.97742663656885%)] Loss:24.792587280273438\n",
            "BCE loss 791.606201171875, KLD_loss 1.7565876245498657\n",
            "Train Epoch: 5 [10240/14166 (72.23476297968398%)] Loss:24.77949333190918\n",
            "BCE loss 791.2080078125, KLD_loss 1.7357807159423828\n",
            "Train Epoch: 5 [10560/14166 (74.49209932279909%)] Loss:24.775083541870117\n",
            "BCE loss 790.9695434570312, KLD_loss 1.8331352472305298\n",
            "Train Epoch: 5 [10880/14166 (76.74943566591422%)] Loss:24.82352638244629\n",
            "BCE loss 792.4534301757812, KLD_loss 1.8993868827819824\n",
            "Train Epoch: 5 [11200/14166 (79.00677200902935%)] Loss:24.833885192871094\n",
            "BCE loss 793.03076171875, KLD_loss 1.653531789779663\n",
            "Train Epoch: 5 [11520/14166 (81.26410835214448%)] Loss:24.778507232666016\n",
            "BCE loss 790.9752807617188, KLD_loss 1.9369374513626099\n",
            "Train Epoch: 5 [11840/14166 (83.52144469525959%)] Loss:24.778480529785156\n",
            "BCE loss 790.7168579101562, KLD_loss 2.1945314407348633\n",
            "Train Epoch: 5 [12160/14166 (85.77878103837472%)] Loss:24.75963592529297\n",
            "BCE loss 790.3632202148438, KLD_loss 1.9451074600219727\n",
            "Train Epoch: 5 [12480/14166 (88.03611738148985%)] Loss:24.785335540771484\n",
            "BCE loss 791.4346923828125, KLD_loss 1.6960551738739014\n",
            "Train Epoch: 5 [12800/14166 (90.29345372460497%)] Loss:24.79543113708496\n",
            "BCE loss 791.6600952148438, KLD_loss 1.79371178150177\n",
            "Train Epoch: 5 [13120/14166 (92.55079006772009%)] Loss:24.811100006103516\n",
            "BCE loss 792.1328125, KLD_loss 1.8223695755004883\n",
            "Train Epoch: 5 [13440/14166 (94.80812641083521%)] Loss:24.81951904296875\n",
            "BCE loss 792.3656616210938, KLD_loss 1.858978033065796\n",
            "Train Epoch: 5 [13760/14166 (97.06546275395034%)] Loss:24.80731964111328\n",
            "BCE loss 791.8402099609375, KLD_loss 1.993959903717041\n",
            "Train Epoch: 5 [14080/14166 (99.32279909706546%)] Loss:24.792774200439453\n",
            "BCE loss 791.443115234375, KLD_loss 1.9256397485733032\n",
            "====> Epoch: 5 Average loss: 24.7984\n",
            "====> Test set loss: 24.7966\n",
            "Train Epoch: 6 [0/14166 (0.0%)] Loss:24.80295753479004\n",
            "BCE loss 791.8668212890625, KLD_loss 1.8278350830078125\n",
            "Train Epoch: 6 [320/14166 (2.2573363431151243%)] Loss:24.772933959960938\n",
            "BCE loss 790.8585815429688, KLD_loss 1.8752700090408325\n",
            "Train Epoch: 6 [640/14166 (4.514672686230249%)] Loss:24.838783264160156\n",
            "BCE loss 793.1141357421875, KLD_loss 1.7269277572631836\n",
            "Train Epoch: 6 [960/14166 (6.772009029345372%)] Loss:24.794757843017578\n",
            "BCE loss 791.5601806640625, KLD_loss 1.872074007987976\n",
            "Train Epoch: 6 [1280/14166 (9.029345372460497%)] Loss:24.806697845458984\n",
            "BCE loss 792.0398559570312, KLD_loss 1.774470567703247\n",
            "Train Epoch: 6 [1600/14166 (11.286681715575622%)] Loss:24.816755294799805\n",
            "BCE loss 792.3156127929688, KLD_loss 1.820535659790039\n",
            "Train Epoch: 6 [1920/14166 (13.544018058690744%)] Loss:24.811660766601562\n",
            "BCE loss 792.173095703125, KLD_loss 1.800095558166504\n",
            "Train Epoch: 6 [2240/14166 (15.801354401805868%)] Loss:24.78660011291504\n",
            "BCE loss 791.3429565429688, KLD_loss 1.8282699584960938\n",
            "Train Epoch: 6 [2560/14166 (18.058690744920995%)] Loss:24.77617835998535\n",
            "BCE loss 790.9625854492188, KLD_loss 1.8751057386398315\n",
            "Train Epoch: 6 [2880/14166 (20.31602708803612%)] Loss:24.788291931152344\n",
            "BCE loss 791.4063720703125, KLD_loss 1.8190159797668457\n",
            "Train Epoch: 6 [3200/14166 (22.573363431151243%)] Loss:24.779436111450195\n",
            "BCE loss 790.96484375, KLD_loss 1.9771209955215454\n",
            "Train Epoch: 6 [3520/14166 (24.830699774266364%)] Loss:24.791259765625\n",
            "BCE loss 791.446533203125, KLD_loss 1.8737382888793945\n",
            "Train Epoch: 6 [3840/14166 (27.08803611738149%)] Loss:24.773283004760742\n",
            "BCE loss 790.7327880859375, KLD_loss 2.0122315883636475\n",
            "Train Epoch: 6 [4160/14166 (29.345372460496613%)] Loss:24.819622039794922\n",
            "BCE loss 792.49951171875, KLD_loss 1.7283848524093628\n",
            "Train Epoch: 6 [4480/14166 (31.602708803611737%)] Loss:24.810951232910156\n",
            "BCE loss 792.3131103515625, KLD_loss 1.6373076438903809\n",
            "Train Epoch: 6 [4800/14166 (33.86004514672686%)] Loss:24.837398529052734\n",
            "BCE loss 792.936767578125, KLD_loss 1.8599662780761719\n",
            "Train Epoch: 6 [5120/14166 (36.11738148984199%)] Loss:24.794042587280273\n",
            "BCE loss 791.5062866210938, KLD_loss 1.9030812978744507\n",
            "Train Epoch: 6 [5440/14166 (38.37471783295711%)] Loss:24.789575576782227\n",
            "BCE loss 791.267333984375, KLD_loss 1.9990888833999634\n",
            "Train Epoch: 6 [5760/14166 (40.63205417607224%)] Loss:24.784744262695312\n",
            "BCE loss 791.218017578125, KLD_loss 1.8937524557113647\n",
            "Train Epoch: 6 [6080/14166 (42.88939051918736%)] Loss:24.779033660888672\n",
            "BCE loss 791.0345458984375, KLD_loss 1.8945249319076538\n",
            "Train Epoch: 6 [6400/14166 (45.146726862302486%)] Loss:24.809961318969727\n",
            "BCE loss 792.166748046875, KLD_loss 1.7519631385803223\n",
            "Train Epoch: 6 [6720/14166 (47.40406320541761%)] Loss:24.793312072753906\n",
            "BCE loss 791.637451171875, KLD_loss 1.7485153675079346\n",
            "Train Epoch: 6 [7040/14166 (49.66139954853273%)] Loss:24.77847671508789\n",
            "BCE loss 791.0231323242188, KLD_loss 1.8880929946899414\n",
            "Train Epoch: 6 [7360/14166 (51.918735891647856%)] Loss:24.793581008911133\n",
            "BCE loss 791.4967041015625, KLD_loss 1.8979216814041138\n",
            "Train Epoch: 6 [7680/14166 (54.17607223476298%)] Loss:24.788366317749023\n",
            "BCE loss 791.5879516601562, KLD_loss 1.6397652626037598\n",
            "Train Epoch: 6 [8000/14166 (56.433408577878104%)] Loss:24.77178192138672\n",
            "BCE loss 790.6243896484375, KLD_loss 2.072610378265381\n",
            "Train Epoch: 6 [8320/14166 (58.690744920993225%)] Loss:24.803321838378906\n",
            "BCE loss 791.8533935546875, KLD_loss 1.852875828742981\n",
            "Train Epoch: 6 [8640/14166 (60.94808126410835%)] Loss:24.81413459777832\n",
            "BCE loss 792.2986450195312, KLD_loss 1.7536171674728394\n",
            "Train Epoch: 6 [8960/14166 (63.205417607223474%)] Loss:24.828067779541016\n",
            "BCE loss 792.8350830078125, KLD_loss 1.6630536317825317\n",
            "Train Epoch: 6 [9280/14166 (65.4627539503386%)] Loss:24.79587173461914\n",
            "BCE loss 791.6033325195312, KLD_loss 1.8645448684692383\n",
            "Train Epoch: 6 [9600/14166 (67.72009029345372%)] Loss:24.794288635253906\n",
            "BCE loss 791.515625, KLD_loss 1.9016008377075195\n",
            "Train Epoch: 6 [9920/14166 (69.97742663656885%)] Loss:24.781967163085938\n",
            "BCE loss 790.9578857421875, KLD_loss 2.065042018890381\n",
            "Train Epoch: 6 [10240/14166 (72.23476297968398%)] Loss:24.786943435668945\n",
            "BCE loss 791.2510986328125, KLD_loss 1.931105136871338\n",
            "Train Epoch: 6 [10560/14166 (74.49209932279909%)] Loss:24.825437545776367\n",
            "BCE loss 792.7025756835938, KLD_loss 1.7114325761795044\n",
            "Train Epoch: 6 [10880/14166 (76.74943566591422%)] Loss:24.797286987304688\n",
            "BCE loss 791.592041015625, KLD_loss 1.9211244583129883\n",
            "Train Epoch: 6 [11200/14166 (79.00677200902935%)] Loss:24.791065216064453\n",
            "BCE loss 791.3262939453125, KLD_loss 1.9877692461013794\n",
            "Train Epoch: 6 [11520/14166 (81.26410835214448%)] Loss:24.786483764648438\n",
            "BCE loss 791.202880859375, KLD_loss 1.9646546840667725\n",
            "Train Epoch: 6 [11840/14166 (83.52144469525959%)] Loss:24.833171844482422\n",
            "BCE loss 792.8575439453125, KLD_loss 1.8038816452026367\n",
            "Train Epoch: 6 [12160/14166 (85.77878103837472%)] Loss:24.800453186035156\n",
            "BCE loss 791.7799072265625, KLD_loss 1.8346000909805298\n",
            "Train Epoch: 6 [12480/14166 (88.03611738148985%)] Loss:24.807449340820312\n",
            "BCE loss 791.9619750976562, KLD_loss 1.876412034034729\n",
            "Train Epoch: 6 [12800/14166 (90.29345372460497%)] Loss:24.79039192199707\n",
            "BCE loss 791.0958862304688, KLD_loss 2.196671485900879\n",
            "Train Epoch: 6 [13120/14166 (92.55079006772009%)] Loss:24.796831130981445\n",
            "BCE loss 791.486083984375, KLD_loss 2.012545585632324\n",
            "Train Epoch: 6 [13440/14166 (94.80812641083521%)] Loss:24.775680541992188\n",
            "BCE loss 790.7960815429688, KLD_loss 2.0257153511047363\n",
            "Train Epoch: 6 [13760/14166 (97.06546275395034%)] Loss:24.77557945251465\n",
            "BCE loss 790.8741455078125, KLD_loss 1.9444366693496704\n",
            "Train Epoch: 6 [14080/14166 (99.32279909706546%)] Loss:24.790307998657227\n",
            "BCE loss 791.2453002929688, KLD_loss 2.0445995330810547\n",
            "====> Epoch: 6 Average loss: 24.7962\n",
            "====> Test set loss: 24.7952\n",
            "Train Epoch: 7 [0/14166 (0.0%)] Loss:24.787952423095703\n",
            "BCE loss 791.2088012695312, KLD_loss 2.0056731700897217\n",
            "Train Epoch: 7 [320/14166 (2.2573363431151243%)] Loss:24.7921085357666\n",
            "BCE loss 791.4864501953125, KLD_loss 1.8610011339187622\n",
            "Train Epoch: 7 [640/14166 (4.514672686230249%)] Loss:24.769948959350586\n",
            "BCE loss 790.7185668945312, KLD_loss 1.9196991920471191\n",
            "Train Epoch: 7 [960/14166 (6.772009029345372%)] Loss:24.782108306884766\n",
            "BCE loss 791.138916015625, KLD_loss 1.8885310888290405\n",
            "Train Epoch: 7 [1280/14166 (9.029345372460497%)] Loss:24.83807945251465\n",
            "BCE loss 793.1558227539062, KLD_loss 1.6627070903778076\n",
            "Train Epoch: 7 [1600/14166 (11.286681715575622%)] Loss:24.765758514404297\n",
            "BCE loss 790.778564453125, KLD_loss 1.7256715297698975\n",
            "Train Epoch: 7 [1920/14166 (13.544018058690744%)] Loss:24.791751861572266\n",
            "BCE loss 791.4609985351562, KLD_loss 1.8750697374343872\n",
            "Train Epoch: 7 [2240/14166 (15.801354401805868%)] Loss:24.783002853393555\n",
            "BCE loss 791.053466796875, KLD_loss 2.002570867538452\n",
            "Train Epoch: 7 [2560/14166 (18.058690744920995%)] Loss:24.786151885986328\n",
            "BCE loss 791.1707153320312, KLD_loss 1.9861834049224854\n",
            "Train Epoch: 7 [2880/14166 (20.31602708803612%)] Loss:24.799272537231445\n",
            "BCE loss 791.596435546875, KLD_loss 1.980275273323059\n",
            "Train Epoch: 7 [3200/14166 (22.573363431151243%)] Loss:24.778499603271484\n",
            "BCE loss 790.75, KLD_loss 2.1619150638580322\n",
            "Train Epoch: 7 [3520/14166 (24.830699774266364%)] Loss:24.789016723632812\n",
            "BCE loss 791.3870849609375, KLD_loss 1.8614686727523804\n",
            "Train Epoch: 7 [3840/14166 (27.08803611738149%)] Loss:24.813962936401367\n",
            "BCE loss 792.2628784179688, KLD_loss 1.7839252948760986\n",
            "Train Epoch: 7 [4160/14166 (29.345372460496613%)] Loss:24.79326629638672\n",
            "BCE loss 791.4640502929688, KLD_loss 1.9204307794570923\n",
            "Train Epoch: 7 [4480/14166 (31.602708803611737%)] Loss:24.82843589782715\n",
            "BCE loss 792.6441040039062, KLD_loss 1.8658429384231567\n",
            "Train Epoch: 7 [4800/14166 (33.86004514672686%)] Loss:24.840877532958984\n",
            "BCE loss 793.2528686523438, KLD_loss 1.6552469730377197\n",
            "Train Epoch: 7 [5120/14166 (36.11738148984199%)] Loss:24.7785701751709\n",
            "BCE loss 790.8112182617188, KLD_loss 2.103076696395874\n",
            "Train Epoch: 7 [5440/14166 (38.37471783295711%)] Loss:24.80422592163086\n",
            "BCE loss 791.8627319335938, KLD_loss 1.8725084066390991\n",
            "Train Epoch: 7 [5760/14166 (40.63205417607224%)] Loss:24.7713623046875\n",
            "BCE loss 790.6180419921875, KLD_loss 2.065525531768799\n",
            "Train Epoch: 7 [6080/14166 (42.88939051918736%)] Loss:24.790197372436523\n",
            "BCE loss 791.2815551757812, KLD_loss 2.004701614379883\n",
            "Train Epoch: 7 [6400/14166 (45.146726862302486%)] Loss:24.813669204711914\n",
            "BCE loss 792.0479125976562, KLD_loss 1.9895070791244507\n",
            "Train Epoch: 7 [6720/14166 (47.40406320541761%)] Loss:24.804779052734375\n",
            "BCE loss 791.6760864257812, KLD_loss 2.076824426651001\n",
            "Train Epoch: 7 [7040/14166 (49.66139954853273%)] Loss:24.797943115234375\n",
            "BCE loss 791.3353881835938, KLD_loss 2.1987502574920654\n",
            "Train Epoch: 7 [7360/14166 (51.918735891647856%)] Loss:24.79680633544922\n",
            "BCE loss 791.644775390625, KLD_loss 1.8530793190002441\n",
            "Train Epoch: 7 [7680/14166 (54.17607223476298%)] Loss:24.78435516357422\n",
            "BCE loss 791.0550537109375, KLD_loss 2.044394016265869\n",
            "Train Epoch: 7 [8000/14166 (56.433408577878104%)] Loss:24.77473258972168\n",
            "BCE loss 790.7201538085938, KLD_loss 2.071293592453003\n",
            "Train Epoch: 7 [8320/14166 (58.690744920993225%)] Loss:24.803604125976562\n",
            "BCE loss 791.7542724609375, KLD_loss 1.9610559940338135\n",
            "Train Epoch: 7 [8640/14166 (60.94808126410835%)] Loss:24.81708526611328\n",
            "BCE loss 792.252685546875, KLD_loss 1.894071340560913\n",
            "Train Epoch: 7 [8960/14166 (63.205417607223474%)] Loss:24.794944763183594\n",
            "BCE loss 791.4973754882812, KLD_loss 1.9408732652664185\n",
            "Train Epoch: 7 [9280/14166 (65.4627539503386%)] Loss:24.814546585083008\n",
            "BCE loss 792.1361083984375, KLD_loss 1.9293298721313477\n",
            "Train Epoch: 7 [9600/14166 (67.72009029345372%)] Loss:24.835779190063477\n",
            "BCE loss 793.1290283203125, KLD_loss 1.615856647491455\n",
            "Train Epoch: 7 [9920/14166 (69.97742663656885%)] Loss:24.754980087280273\n",
            "BCE loss 790.1700439453125, KLD_loss 1.9893054962158203\n",
            "Train Epoch: 7 [10240/14166 (72.23476297968398%)] Loss:24.82183837890625\n",
            "BCE loss 792.4326782226562, KLD_loss 1.866103172302246\n",
            "Train Epoch: 7 [10560/14166 (74.49209932279909%)] Loss:24.806730270385742\n",
            "BCE loss 791.9835815429688, KLD_loss 1.831810712814331\n",
            "Train Epoch: 7 [10880/14166 (76.74943566591422%)] Loss:24.802526473999023\n",
            "BCE loss 791.536376953125, KLD_loss 2.1444613933563232\n",
            "Train Epoch: 7 [11200/14166 (79.00677200902935%)] Loss:24.805158615112305\n",
            "BCE loss 791.8616333007812, KLD_loss 1.9034830331802368\n",
            "Train Epoch: 7 [11520/14166 (81.26410835214448%)] Loss:24.7840576171875\n",
            "BCE loss 791.0013427734375, KLD_loss 2.0885603427886963\n",
            "Train Epoch: 7 [11840/14166 (83.52144469525959%)] Loss:24.8060302734375\n",
            "BCE loss 791.8412475585938, KLD_loss 1.9517208337783813\n",
            "Train Epoch: 7 [12160/14166 (85.77878103837472%)] Loss:24.789073944091797\n",
            "BCE loss 791.2010498046875, KLD_loss 2.0493500232696533\n",
            "Train Epoch: 7 [12480/14166 (88.03611738148985%)] Loss:24.77189064025879\n",
            "BCE loss 790.6964111328125, KLD_loss 2.0040674209594727\n",
            "Train Epoch: 7 [12800/14166 (90.29345372460497%)] Loss:24.816577911376953\n",
            "BCE loss 792.25146484375, KLD_loss 1.8790621757507324\n",
            "Train Epoch: 7 [13120/14166 (92.55079006772009%)] Loss:24.800840377807617\n",
            "BCE loss 791.7377319335938, KLD_loss 1.889197826385498\n",
            "Train Epoch: 7 [13440/14166 (94.80812641083521%)] Loss:24.766921997070312\n",
            "BCE loss 790.5526123046875, KLD_loss 1.988849401473999\n",
            "Train Epoch: 7 [13760/14166 (97.06546275395034%)] Loss:24.7963809967041\n",
            "BCE loss 791.5330810546875, KLD_loss 1.9511213302612305\n",
            "Train Epoch: 7 [14080/14166 (99.32279909706546%)] Loss:24.812294006347656\n",
            "BCE loss 792.37548828125, KLD_loss 1.6178715229034424\n",
            "====> Epoch: 7 Average loss: 24.7941\n",
            "====> Test set loss: 24.7943\n",
            "Train Epoch: 8 [0/14166 (0.0%)] Loss:24.81310272216797\n",
            "BCE loss 792.0389404296875, KLD_loss 1.980337142944336\n",
            "Train Epoch: 8 [320/14166 (2.2573363431151243%)] Loss:24.77965545654297\n",
            "BCE loss 791.0816650390625, KLD_loss 1.8672157526016235\n",
            "Train Epoch: 8 [640/14166 (4.514672686230249%)] Loss:24.81415367126465\n",
            "BCE loss 792.1331176757812, KLD_loss 1.9198004007339478\n",
            "Train Epoch: 8 [960/14166 (6.772009029345372%)] Loss:24.812742233276367\n",
            "BCE loss 792.16845703125, KLD_loss 1.8392720222473145\n",
            "Train Epoch: 8 [1280/14166 (9.029345372460497%)] Loss:24.79667854309082\n",
            "BCE loss 791.6128540039062, KLD_loss 1.8808799982070923\n",
            "Train Epoch: 8 [1600/14166 (11.286681715575622%)] Loss:24.807994842529297\n",
            "BCE loss 792.231201171875, KLD_loss 1.6245769262313843\n",
            "Train Epoch: 8 [1920/14166 (13.544018058690744%)] Loss:24.79986572265625\n",
            "BCE loss 791.5391845703125, KLD_loss 2.056516170501709\n",
            "Train Epoch: 8 [2240/14166 (15.801354401805868%)] Loss:24.796524047851562\n",
            "BCE loss 791.4005737304688, KLD_loss 2.0881993770599365\n",
            "Train Epoch: 8 [2560/14166 (18.058690744920995%)] Loss:24.793472290039062\n",
            "BCE loss 791.43310546875, KLD_loss 1.957943081855774\n",
            "Train Epoch: 8 [2880/14166 (20.31602708803612%)] Loss:24.78144645690918\n",
            "BCE loss 791.0484619140625, KLD_loss 1.9577760696411133\n",
            "Train Epoch: 8 [3200/14166 (22.573363431151243%)] Loss:24.796178817749023\n",
            "BCE loss 791.5654296875, KLD_loss 1.9122215509414673\n",
            "Train Epoch: 8 [3520/14166 (24.830699774266364%)] Loss:24.81853675842285\n",
            "BCE loss 792.33154296875, KLD_loss 1.8616232872009277\n",
            "Train Epoch: 8 [3840/14166 (27.08803611738149%)] Loss:24.77952766418457\n",
            "BCE loss 790.8472900390625, KLD_loss 2.097560167312622\n",
            "Train Epoch: 8 [4160/14166 (29.345372460496613%)] Loss:24.79722023010254\n",
            "BCE loss 791.562744140625, KLD_loss 1.9482530355453491\n",
            "Train Epoch: 8 [4480/14166 (31.602708803611737%)] Loss:24.788028717041016\n",
            "BCE loss 791.3068237304688, KLD_loss 1.9101295471191406\n",
            "Train Epoch: 8 [4800/14166 (33.86004514672686%)] Loss:24.764244079589844\n",
            "BCE loss 790.1923217773438, KLD_loss 2.2634544372558594\n",
            "Train Epoch: 8 [5120/14166 (36.11738148984199%)] Loss:24.80696678161621\n",
            "BCE loss 791.73876953125, KLD_loss 2.084141254425049\n",
            "Train Epoch: 8 [5440/14166 (38.37471783295711%)] Loss:24.79986000061035\n",
            "BCE loss 791.4871826171875, KLD_loss 2.1083555221557617\n",
            "Train Epoch: 8 [5760/14166 (40.63205417607224%)] Loss:24.82975196838379\n",
            "BCE loss 792.662353515625, KLD_loss 1.8897455930709839\n",
            "Train Epoch: 8 [6080/14166 (42.88939051918736%)] Loss:24.78659439086914\n",
            "BCE loss 791.1212158203125, KLD_loss 2.0498125553131104\n",
            "Train Epoch: 8 [6400/14166 (45.146726862302486%)] Loss:24.775209426879883\n",
            "BCE loss 790.8467407226562, KLD_loss 1.9599263668060303\n",
            "Train Epoch: 8 [6720/14166 (47.40406320541761%)] Loss:24.770782470703125\n",
            "BCE loss 790.5700073242188, KLD_loss 2.0950052738189697\n",
            "Train Epoch: 8 [7040/14166 (49.66139954853273%)] Loss:24.79441261291504\n",
            "BCE loss 791.3375244140625, KLD_loss 2.0837061405181885\n",
            "Train Epoch: 8 [7360/14166 (51.918735891647856%)] Loss:24.794017791748047\n",
            "BCE loss 791.3397827148438, KLD_loss 2.068805456161499\n",
            "Train Epoch: 8 [7680/14166 (54.17607223476298%)] Loss:24.78492546081543\n",
            "BCE loss 791.1011962890625, KLD_loss 2.016420364379883\n",
            "Train Epoch: 8 [8000/14166 (56.433408577878104%)] Loss:24.81049156188965\n",
            "BCE loss 792.1079711914062, KLD_loss 1.8277835845947266\n",
            "Train Epoch: 8 [8320/14166 (58.690744920993225%)] Loss:24.812044143676758\n",
            "BCE loss 791.8919677734375, KLD_loss 2.0934078693389893\n",
            "Train Epoch: 8 [8640/14166 (60.94808126410835%)] Loss:24.764083862304688\n",
            "BCE loss 790.519287109375, KLD_loss 1.9313926696777344\n",
            "Train Epoch: 8 [8960/14166 (63.205417607223474%)] Loss:24.814023971557617\n",
            "BCE loss 792.2744140625, KLD_loss 1.7743196487426758\n",
            "Train Epoch: 8 [9280/14166 (65.4627539503386%)] Loss:24.79549217224121\n",
            "BCE loss 791.6429443359375, KLD_loss 1.812805414199829\n",
            "Train Epoch: 8 [9600/14166 (67.72009029345372%)] Loss:24.83148193359375\n",
            "BCE loss 792.6793823242188, KLD_loss 1.9280427694320679\n",
            "Train Epoch: 8 [9920/14166 (69.97742663656885%)] Loss:24.80629539489746\n",
            "BCE loss 791.7972412109375, KLD_loss 2.0042643547058105\n",
            "Train Epoch: 8 [10240/14166 (72.23476297968398%)] Loss:24.80845832824707\n",
            "BCE loss 791.9552001953125, KLD_loss 1.9154620170593262\n",
            "Train Epoch: 8 [10560/14166 (74.49209932279909%)] Loss:24.815963745117188\n",
            "BCE loss 792.0928955078125, KLD_loss 2.0179896354675293\n",
            "Train Epoch: 8 [10880/14166 (76.74943566591422%)] Loss:24.76801109313965\n",
            "BCE loss 790.549072265625, KLD_loss 2.02732515335083\n",
            "Train Epoch: 8 [11200/14166 (79.00677200902935%)] Loss:24.777978897094727\n",
            "BCE loss 790.9288940429688, KLD_loss 1.966426968574524\n",
            "Train Epoch: 8 [11520/14166 (81.26410835214448%)] Loss:24.795923233032227\n",
            "BCE loss 791.5078735351562, KLD_loss 1.961706280708313\n",
            "Train Epoch: 8 [11840/14166 (83.52144469525959%)] Loss:24.79827308654785\n",
            "BCE loss 791.8031005859375, KLD_loss 1.7415995597839355\n",
            "Train Epoch: 8 [12160/14166 (85.77878103837472%)] Loss:24.80902099609375\n",
            "BCE loss 792.0665283203125, KLD_loss 1.8221447467803955\n",
            "Train Epoch: 8 [12480/14166 (88.03611738148985%)] Loss:24.815826416015625\n",
            "BCE loss 792.3170166015625, KLD_loss 1.789456844329834\n",
            "Train Epoch: 8 [12800/14166 (90.29345372460497%)] Loss:24.796695709228516\n",
            "BCE loss 791.3717041015625, KLD_loss 2.12253999710083\n",
            "Train Epoch: 8 [13120/14166 (92.55079006772009%)] Loss:24.796951293945312\n",
            "BCE loss 791.6898193359375, KLD_loss 1.8127003908157349\n",
            "Train Epoch: 8 [13440/14166 (94.80812641083521%)] Loss:24.77565574645996\n",
            "BCE loss 790.486328125, KLD_loss 2.334634780883789\n",
            "Train Epoch: 8 [13760/14166 (97.06546275395034%)] Loss:24.793193817138672\n",
            "BCE loss 791.3583984375, KLD_loss 2.0238282680511475\n",
            "Train Epoch: 8 [14080/14166 (99.32279909706546%)] Loss:24.808120727539062\n",
            "BCE loss 791.9483032226562, KLD_loss 1.911567211151123\n",
            "====> Epoch: 8 Average loss: 24.7932\n",
            "====> Test set loss: 24.7934\n",
            "Train Epoch: 9 [0/14166 (0.0%)] Loss:24.804689407348633\n",
            "BCE loss 791.7105712890625, KLD_loss 2.0394887924194336\n",
            "Train Epoch: 9 [320/14166 (2.2573363431151243%)] Loss:24.80560302734375\n",
            "BCE loss 791.8397216796875, KLD_loss 1.939542531967163\n",
            "Train Epoch: 9 [640/14166 (4.514672686230249%)] Loss:24.806137084960938\n",
            "BCE loss 791.8714599609375, KLD_loss 1.924898624420166\n",
            "Train Epoch: 9 [960/14166 (6.772009029345372%)] Loss:24.793926239013672\n",
            "BCE loss 791.6495971679688, KLD_loss 1.7561132907867432\n",
            "Train Epoch: 9 [1280/14166 (9.029345372460497%)] Loss:24.813167572021484\n",
            "BCE loss 791.8984985351562, KLD_loss 2.1228713989257812\n",
            "Train Epoch: 9 [1600/14166 (11.286681715575622%)] Loss:24.783361434936523\n",
            "BCE loss 791.2462158203125, KLD_loss 1.8213615417480469\n",
            "Train Epoch: 9 [1920/14166 (13.544018058690744%)] Loss:24.82618522644043\n",
            "BCE loss 792.5662841796875, KLD_loss 1.8716371059417725\n",
            "Train Epoch: 9 [2240/14166 (15.801354401805868%)] Loss:24.767623901367188\n",
            "BCE loss 790.384521484375, KLD_loss 2.179331064224243\n",
            "Train Epoch: 9 [2560/14166 (18.058690744920995%)] Loss:24.779584884643555\n",
            "BCE loss 790.7921752929688, KLD_loss 2.154528856277466\n",
            "Train Epoch: 9 [2880/14166 (20.31602708803612%)] Loss:24.791805267333984\n",
            "BCE loss 791.5333251953125, KLD_loss 1.8044493198394775\n",
            "Train Epoch: 9 [3200/14166 (22.573363431151243%)] Loss:24.813432693481445\n",
            "BCE loss 792.0895385742188, KLD_loss 1.9403387308120728\n",
            "Train Epoch: 9 [3520/14166 (24.830699774266364%)] Loss:24.811838150024414\n",
            "BCE loss 792.27880859375, KLD_loss 1.6999692916870117\n",
            "Train Epoch: 9 [3840/14166 (27.08803611738149%)] Loss:24.7684268951416\n",
            "BCE loss 790.4840698242188, KLD_loss 2.105543375015259\n",
            "Train Epoch: 9 [4160/14166 (29.345372460496613%)] Loss:24.775115966796875\n",
            "BCE loss 790.927490234375, KLD_loss 1.8762681484222412\n",
            "Train Epoch: 9 [4480/14166 (31.602708803611737%)] Loss:24.801856994628906\n",
            "BCE loss 791.6082763671875, KLD_loss 2.0512022972106934\n",
            "Train Epoch: 9 [4800/14166 (33.86004514672686%)] Loss:24.80910301208496\n",
            "BCE loss 792.0067749023438, KLD_loss 1.8845757246017456\n",
            "Train Epoch: 9 [5120/14166 (36.11738148984199%)] Loss:24.812889099121094\n",
            "BCE loss 791.92724609375, KLD_loss 2.085294246673584\n",
            "Train Epoch: 9 [5440/14166 (38.37471783295711%)] Loss:24.818599700927734\n",
            "BCE loss 792.477294921875, KLD_loss 1.717795968055725\n",
            "Train Epoch: 9 [5760/14166 (40.63205417607224%)] Loss:24.752065658569336\n",
            "BCE loss 789.9236450195312, KLD_loss 2.1425089836120605\n",
            "Train Epoch: 9 [6080/14166 (42.88939051918736%)] Loss:24.768592834472656\n",
            "BCE loss 790.4400024414062, KLD_loss 2.1549129486083984\n",
            "Train Epoch: 9 [6400/14166 (45.146726862302486%)] Loss:24.789026260375977\n",
            "BCE loss 791.338623046875, KLD_loss 1.910177230834961\n",
            "Train Epoch: 9 [6720/14166 (47.40406320541761%)] Loss:24.79773712158203\n",
            "BCE loss 791.2279052734375, KLD_loss 2.2996878623962402\n",
            "Train Epoch: 9 [7040/14166 (49.66139954853273%)] Loss:24.79078483581543\n",
            "BCE loss 791.1730346679688, KLD_loss 2.1320841312408447\n",
            "Train Epoch: 9 [7360/14166 (51.918735891647856%)] Loss:24.783905029296875\n",
            "BCE loss 791.045654296875, KLD_loss 2.0392823219299316\n",
            "Train Epoch: 9 [7680/14166 (54.17607223476298%)] Loss:24.810518264770508\n",
            "BCE loss 791.9961547851562, KLD_loss 1.9403585195541382\n",
            "Train Epoch: 9 [8000/14166 (56.433408577878104%)] Loss:24.765480041503906\n",
            "BCE loss 790.323974609375, KLD_loss 2.1714720726013184\n",
            "Train Epoch: 9 [8320/14166 (58.690744920993225%)] Loss:24.754039764404297\n",
            "BCE loss 789.8959350585938, KLD_loss 2.233311891555786\n",
            "Train Epoch: 9 [8640/14166 (60.94808126410835%)] Loss:24.795442581176758\n",
            "BCE loss 791.605224609375, KLD_loss 1.848974347114563\n",
            "Train Epoch: 9 [8960/14166 (63.205417607223474%)] Loss:24.783613204956055\n",
            "BCE loss 790.796875, KLD_loss 2.27880859375\n",
            "Train Epoch: 9 [9280/14166 (65.4627539503386%)] Loss:24.768308639526367\n",
            "BCE loss 790.4479370117188, KLD_loss 2.1379470825195312\n",
            "Train Epoch: 9 [9600/14166 (67.72009029345372%)] Loss:24.83828353881836\n",
            "BCE loss 793.0321044921875, KLD_loss 1.792905330657959\n",
            "Train Epoch: 9 [9920/14166 (69.97742663656885%)] Loss:24.787382125854492\n",
            "BCE loss 791.1844482421875, KLD_loss 2.0117573738098145\n",
            "Train Epoch: 9 [10240/14166 (72.23476297968398%)] Loss:24.784412384033203\n",
            "BCE loss 790.9828491210938, KLD_loss 2.1183202266693115\n",
            "Train Epoch: 9 [10560/14166 (74.49209932279909%)] Loss:24.759685516357422\n",
            "BCE loss 790.3133544921875, KLD_loss 1.9965739250183105\n",
            "Train Epoch: 9 [10880/14166 (76.74943566591422%)] Loss:24.796741485595703\n",
            "BCE loss 791.5029907226562, KLD_loss 1.9927399158477783\n",
            "Train Epoch: 9 [11200/14166 (79.00677200902935%)] Loss:24.772621154785156\n",
            "BCE loss 790.4595947265625, KLD_loss 2.2642858028411865\n",
            "Train Epoch: 9 [11520/14166 (81.26410835214448%)] Loss:24.81760025024414\n",
            "BCE loss 792.2138671875, KLD_loss 1.9492864608764648\n",
            "Train Epoch: 9 [11840/14166 (83.52144469525959%)] Loss:24.765567779541016\n",
            "BCE loss 790.4052124023438, KLD_loss 2.0929388999938965\n",
            "Train Epoch: 9 [12160/14166 (85.77878103837472%)] Loss:24.75190544128418\n",
            "BCE loss 789.8394775390625, KLD_loss 2.2214417457580566\n",
            "Train Epoch: 9 [12480/14166 (88.03611738148985%)] Loss:24.77589225769043\n",
            "BCE loss 790.725830078125, KLD_loss 2.1026971340179443\n",
            "Train Epoch: 9 [12800/14166 (90.29345372460497%)] Loss:24.802738189697266\n",
            "BCE loss 791.7035522460938, KLD_loss 1.9839715957641602\n",
            "Train Epoch: 9 [13120/14166 (92.55079006772009%)] Loss:24.763534545898438\n",
            "BCE loss 790.2569580078125, KLD_loss 2.1761178970336914\n",
            "Train Epoch: 9 [13440/14166 (94.80812641083521%)] Loss:24.76996421813965\n",
            "BCE loss 790.5390625, KLD_loss 2.0997843742370605\n",
            "Train Epoch: 9 [13760/14166 (97.06546275395034%)] Loss:24.803327560424805\n",
            "BCE loss 791.7945556640625, KLD_loss 1.9119480848312378\n",
            "Train Epoch: 9 [14080/14166 (99.32279909706546%)] Loss:24.830829620361328\n",
            "BCE loss 792.5105590820312, KLD_loss 2.0759904384613037\n",
            "====> Epoch: 9 Average loss: 24.7916\n",
            "====> Test set loss: 24.7934\n",
            "Train Epoch: 10 [0/14166 (0.0%)] Loss:24.800426483154297\n",
            "BCE loss 791.7476806640625, KLD_loss 1.8659316301345825\n",
            "Train Epoch: 10 [320/14166 (2.2573363431151243%)] Loss:24.75548553466797\n",
            "BCE loss 789.96337890625, KLD_loss 2.212170362472534\n",
            "Train Epoch: 10 [640/14166 (4.514672686230249%)] Loss:24.809961318969727\n",
            "BCE loss 792.1849975585938, KLD_loss 1.7338335514068604\n",
            "Train Epoch: 10 [960/14166 (6.772009029345372%)] Loss:24.79709243774414\n",
            "BCE loss 791.5146484375, KLD_loss 1.9923113584518433\n",
            "Train Epoch: 10 [1280/14166 (9.029345372460497%)] Loss:24.776670455932617\n",
            "BCE loss 790.8327026367188, KLD_loss 2.020815372467041\n",
            "Train Epoch: 10 [1600/14166 (11.286681715575622%)] Loss:24.816804885864258\n",
            "BCE loss 792.1436157226562, KLD_loss 1.99419367313385\n",
            "Train Epoch: 10 [1920/14166 (13.544018058690744%)] Loss:24.840167999267578\n",
            "BCE loss 792.9764404296875, KLD_loss 1.9089516401290894\n",
            "Train Epoch: 10 [2240/14166 (15.801354401805868%)] Loss:24.787023544311523\n",
            "BCE loss 791.1358032226562, KLD_loss 2.049018621444702\n",
            "Train Epoch: 10 [2560/14166 (18.058690744920995%)] Loss:24.78744888305664\n",
            "BCE loss 791.34912109375, KLD_loss 1.8491889238357544\n",
            "Train Epoch: 10 [2880/14166 (20.31602708803612%)] Loss:24.768112182617188\n",
            "BCE loss 790.3800659179688, KLD_loss 2.1994707584381104\n",
            "Train Epoch: 10 [3200/14166 (22.573363431151243%)] Loss:24.781890869140625\n",
            "BCE loss 791.147705078125, KLD_loss 1.8728718757629395\n",
            "Train Epoch: 10 [3520/14166 (24.830699774266364%)] Loss:24.816741943359375\n",
            "BCE loss 792.115966796875, KLD_loss 2.0197913646698\n",
            "Train Epoch: 10 [3840/14166 (27.08803611738149%)] Loss:24.803064346313477\n",
            "BCE loss 791.705322265625, KLD_loss 1.9927358627319336\n",
            "Train Epoch: 10 [4160/14166 (29.345372460496613%)] Loss:24.786231994628906\n",
            "BCE loss 791.0502319335938, KLD_loss 2.1091504096984863\n",
            "Train Epoch: 10 [4480/14166 (31.602708803611737%)] Loss:24.760913848876953\n",
            "BCE loss 790.1030883789062, KLD_loss 2.2461678981781006\n",
            "Train Epoch: 10 [4800/14166 (33.86004514672686%)] Loss:24.797313690185547\n",
            "BCE loss 791.3038330078125, KLD_loss 2.2101778984069824\n",
            "Train Epoch: 10 [5120/14166 (36.11738148984199%)] Loss:24.7950382232666\n",
            "BCE loss 791.3800048828125, KLD_loss 2.061243772506714\n",
            "Train Epoch: 10 [5440/14166 (38.37471783295711%)] Loss:24.789642333984375\n",
            "BCE loss 791.3471069335938, KLD_loss 1.9214552640914917\n",
            "Train Epoch: 10 [5760/14166 (40.63205417607224%)] Loss:24.78047752380371\n",
            "BCE loss 790.9926147460938, KLD_loss 1.9827089309692383\n",
            "Train Epoch: 10 [6080/14166 (42.88939051918736%)] Loss:24.723974227905273\n",
            "BCE loss 788.958984375, KLD_loss 2.208172082901001\n",
            "Train Epoch: 10 [6400/14166 (45.146726862302486%)] Loss:24.80573844909668\n",
            "BCE loss 791.73974609375, KLD_loss 2.043891191482544\n",
            "Train Epoch: 10 [6720/14166 (47.40406320541761%)] Loss:24.794002532958984\n",
            "BCE loss 791.2424926757812, KLD_loss 2.1655526161193848\n",
            "Train Epoch: 10 [7040/14166 (49.66139954853273%)] Loss:24.790367126464844\n",
            "BCE loss 791.3317260742188, KLD_loss 1.9600707292556763\n",
            "Train Epoch: 10 [7360/14166 (51.918735891647856%)] Loss:24.767623901367188\n",
            "BCE loss 790.8522338867188, KLD_loss 1.7117621898651123\n",
            "Train Epoch: 10 [7680/14166 (54.17607223476298%)] Loss:24.81472396850586\n",
            "BCE loss 792.2716064453125, KLD_loss 1.7996059656143188\n",
            "Train Epoch: 10 [8000/14166 (56.433408577878104%)] Loss:24.801151275634766\n",
            "BCE loss 791.5449829101562, KLD_loss 2.0918846130371094\n",
            "Train Epoch: 10 [8320/14166 (58.690744920993225%)] Loss:24.763492584228516\n",
            "BCE loss 790.358642578125, KLD_loss 2.0730984210968018\n",
            "Train Epoch: 10 [8640/14166 (60.94808126410835%)] Loss:24.79378890991211\n",
            "BCE loss 791.4126586914062, KLD_loss 1.98854660987854\n",
            "Train Epoch: 10 [8960/14166 (63.205417607223474%)] Loss:24.788066864013672\n",
            "BCE loss 791.0346069335938, KLD_loss 2.1834819316864014\n",
            "Train Epoch: 10 [9280/14166 (65.4627539503386%)] Loss:24.7990665435791\n",
            "BCE loss 791.3849487304688, KLD_loss 2.185157299041748\n",
            "Train Epoch: 10 [9600/14166 (67.72009029345372%)] Loss:24.801403045654297\n",
            "BCE loss 791.7054443359375, KLD_loss 1.9394632577896118\n",
            "Train Epoch: 10 [9920/14166 (69.97742663656885%)] Loss:24.78113555908203\n",
            "BCE loss 791.0208740234375, KLD_loss 1.9755072593688965\n",
            "Train Epoch: 10 [10240/14166 (72.23476297968398%)] Loss:24.765628814697266\n",
            "BCE loss 790.0540771484375, KLD_loss 2.4460558891296387\n",
            "Train Epoch: 10 [10560/14166 (74.49209932279909%)] Loss:24.810211181640625\n",
            "BCE loss 791.819091796875, KLD_loss 2.107706308364868\n",
            "Train Epoch: 10 [10880/14166 (76.74943566591422%)] Loss:24.80533218383789\n",
            "BCE loss 791.772216796875, KLD_loss 1.9984155893325806\n",
            "Train Epoch: 10 [11200/14166 (79.00677200902935%)] Loss:24.784936904907227\n",
            "BCE loss 791.067626953125, KLD_loss 2.0504038333892822\n",
            "Train Epoch: 10 [11520/14166 (81.26410835214448%)] Loss:24.799415588378906\n",
            "BCE loss 791.697021484375, KLD_loss 1.8842799663543701\n",
            "Train Epoch: 10 [11840/14166 (83.52144469525959%)] Loss:24.76827621459961\n",
            "BCE loss 790.4501953125, KLD_loss 2.134666681289673\n",
            "Train Epoch: 10 [12160/14166 (85.77878103837472%)] Loss:24.763681411743164\n",
            "BCE loss 790.3306274414062, KLD_loss 2.1071338653564453\n",
            "Train Epoch: 10 [12480/14166 (88.03611738148985%)] Loss:24.76837158203125\n",
            "BCE loss 790.573974609375, KLD_loss 2.013950824737549\n",
            "Train Epoch: 10 [12800/14166 (90.29345372460497%)] Loss:24.761672973632812\n",
            "BCE loss 790.2151489257812, KLD_loss 2.1583034992218018\n",
            "Train Epoch: 10 [13120/14166 (92.55079006772009%)] Loss:24.786293029785156\n",
            "BCE loss 791.05908203125, KLD_loss 2.1021945476531982\n",
            "Train Epoch: 10 [13440/14166 (94.80812641083521%)] Loss:24.783374786376953\n",
            "BCE loss 790.8471069335938, KLD_loss 2.220950126647949\n",
            "Train Epoch: 10 [13760/14166 (97.06546275395034%)] Loss:24.780624389648438\n",
            "BCE loss 790.8701782226562, KLD_loss 2.1098146438598633\n",
            "Train Epoch: 10 [14080/14166 (99.32279909706546%)] Loss:24.767440795898438\n",
            "BCE loss 790.60009765625, KLD_loss 1.9579960107803345\n",
            "====> Epoch: 10 Average loss: 24.7904\n",
            "====> Test set loss: 24.7915\n"
          ]
        }
      ]
    }
  ]
}