{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JosephThompson607/dir_vae/blob/main/sepsis_vae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "phRvmO49rUUT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we prepare the data for training"
      ],
      "metadata": {
        "id": "0SiJSH5EMXTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: Read this from the cloud\n",
        "patients = pd.read_csv(\"/content/unique_patient_dem.csv\")\n",
        "\n",
        "patients.drop(columns=['subject_id'], inplace=True)\n",
        "numeric_cols = patients.select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_cols = patients.select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "# Reorder DataFrame\n",
        "patients = patients[numeric_cols + categorical_cols]\n",
        "#1 hot encoding\n",
        "df_encoded = pd.get_dummies(patients, columns=categorical_cols)\n",
        "\n",
        "#If cuda is available, device is cuda, otherwise cpu\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "scaler = StandardScaler()\n",
        "#Scaling numeric columns\n",
        "df_encoded[numeric_cols] = scaler.fit_transform(df_encoded[numeric_cols])\n",
        "features = df_encoded.astype('float32').values\n",
        "# print(features.columns)\n",
        "# print(features.dtypes)\n",
        "# Get indices for slicing\n",
        "num_indices = list(range(len(numeric_cols)))\n",
        "n_numeric = len(numeric_cols)\n",
        "cat_indices = list(range(len(numeric_cols), len(features)))\n",
        "tensor = torch.tensor(features, dtype=torch.float32)\n",
        "\n",
        "X_train, X_test = train_test_split(tensor, test_size=0.2, random_state=42)\n",
        "\n",
        "train_dataset = TensorDataset(X_train)  # or (X_train, y_train)\n",
        "test_dataset = TensorDataset(X_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "input_size = X_train[0].shape[0] #input size is the number of features going into the network\n",
        "print(input_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4j5hmBSfrZhl",
        "outputId": "dababc2d-49a7-473a-d4ad-03a7f1f544e1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we define the model and related functions"
      ],
      "metadata": {
        "id": "HeWHLkFqMPmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ngf = 64\n",
        "ndf = 64\n",
        "nc = 1\n",
        "\n",
        "def prior(K, alpha):\n",
        "    \"\"\"\n",
        "    Prior for the model.\n",
        "    :K: number of categories\n",
        "    :alpha: Hyper param of Dir\n",
        "    :return: mean and variance tensors\n",
        "    \"\"\"\n",
        "    # Approximate to normal distribution using Laplace approximation\n",
        "    a = torch.Tensor(1, K).float().fill_(alpha)\n",
        "    mean = a.log().t() - a.log().mean(1)\n",
        "    var = ((1 - 2.0 / K) * a.reciprocal()).t() + (1.0 / K ** 2) * a.reciprocal().sum(1)\n",
        "    return mean.t(), var.t() # Parameters of prior distribution after approximation\n",
        "\n",
        "class Dir_VAE(nn.Module):\n",
        "    def __init__(self, input_size,n_numeric, latent_size=10, hidden_dim = 200):\n",
        "        self.num_numeric_cols = n_numeric\n",
        "        self.latent_size = latent_size\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.input_size = input_size\n",
        "        super(Dir_VAE, self).__init__()\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "          nn.Linear(self.input_size, self.hidden_dim),\n",
        "          nn.ReLU(),\n",
        "          # nn.Linear(self.hidden_dim, self.hidden_dim),\n",
        "          # nn.ReLU(),\n",
        "          # nn.Linear(self.hidden_dim, self.hidden_dim),\n",
        "          # nn.ReLU()\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "          nn.Linear(self.latent_size, self.hidden_dim),\n",
        "          nn.ReLU(),\n",
        "          # nn.Linear(self.hidden_dim, self.hidden_dim),\n",
        "          # nn.ReLU(),\n",
        "          # nn.Linear(self.hidden_dim, self.hidden_dim),\n",
        "          # nn.ReLU(),\n",
        "          nn.Linear(self.hidden_dim, self.self.hidden_dim),\n",
        "          nn.ReLU(),\n",
        "          # nn.Unflatten(dim=1, unflattened_size=(1, 28, 28)) # This was for image data\n",
        "        )\n",
        "        #self.fc1 = nn.Linear(self.hidden_dim, 512)\n",
        "        self.fc21 = nn.Linear(self.hidden_dim, self.latent_size)\n",
        "        self.fc22 = nn.Linear(self.hidden_dim, self.latent_size)\n",
        "\n",
        "        self.fc3 = nn.Linear(self.hidden_dim, self.input_size)\n",
        "        #self.fc4 = nn.Linear(512, self.hidden_dim)\n",
        "\n",
        "        self.lrelu = nn.LeakyReLU()\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Dir prior\n",
        "        self.prior_mean, self.prior_var = map(nn.Parameter, prior(self.latent_size, 0.3)) # 0.3 is a hyper param of Dirichlet distribution\n",
        "        self.prior_logvar = nn.Parameter(self.prior_var.log())\n",
        "        self.prior_mean.requires_grad = False\n",
        "        self.prior_var.requires_grad = False\n",
        "        self.prior_logvar.requires_grad = False\n",
        "\n",
        "\n",
        "    def encode(self, x):\n",
        "        encoding = self.encoder(x);\n",
        "        #h1 = self.fc1(encoding)\n",
        "        return self.fc21(encoding), self.fc22(encoding)\n",
        "\n",
        "    def decode(self, gauss_z):\n",
        "        dir_z = F.softmax(gauss_z,dim=1) #Reduntant, already done in forward\n",
        "        # This variable (z) can be treated as a variable that follows a Dirichlet distribution (a variable that can be interpreted as a probability that the sum is 1)\n",
        "        # Use the Softmax function to satisfy the simplex constraint\n",
        "        x_out = self.decoder(dir_z)\n",
        "        # Apply sigmoid to categorical output only\n",
        "        x_out[:, self.num_numeric_cols:] = torch.sigmoid(x_out[:, self.num_numeric_cols:])\n",
        "        return x_out\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps*std\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        gauss_z = self.reparameterize(mu, logvar)\n",
        "        # gause_z is a variable that follows a multivariate normal distribution\n",
        "        # Inputting gause_z into softmax func yields a random variable that follows a Dirichlet distribution (Softmax func are used in decoder)\n",
        "        dir_z = F.softmax(gauss_z,dim=1) # This variable follows a Dirichlet distribution\n",
        "        return self.decode(gauss_z), mu, logvar, gauss_z, dir_z\n",
        "\n",
        "    def reconstruction_loss(self, x_true, x_recon):\n",
        "        # Slice the tensors\n",
        "        x_true_num = x_true[:, :self.num_numeric_cols]\n",
        "        x_true_cat = x_true[:, self.num_numeric_cols:]\n",
        "\n",
        "        x_recon_num = x_recon[:, :self.num_numeric_cols]\n",
        "        x_recon_cat = x_recon[:, self.num_numeric_cols:]\n",
        "\n",
        "        # Compute losses\n",
        "        num_loss = F.mse_loss(x_recon_num, x_true_num)\n",
        "        cat_loss = F.cross_entropy(x_recon_cat, x_true_cat)\n",
        "\n",
        "        return num_loss + cat_loss\n",
        "\n",
        "    # Reconstruction + KL divergence losses summed over all elements and batch\n",
        "    def loss_function(self, recon_x, x, mu, logvar):\n",
        "        # Apply sigmoid to the input data x to ensure values are between 0 and 1\n",
        "        recon_loss = self.reconstruction_loss(x, recon_x, )\n",
        "        # ディリクレ事前分布と変分事後分布とのKLを計算\n",
        "        # Calculating KL with Dirichlet prior and variational posterior distributions\n",
        "        # Original paper:\"Autoencodeing variational inference for topic model\"-https://arxiv.org/pdf/1703.01488\n",
        "        prior_mean = self.prior_mean.expand_as(mu)\n",
        "        prior_var = self.prior_var.expand_as(logvar)\n",
        "        prior_logvar = self.prior_logvar.expand_as(logvar)\n",
        "        var_division = logvar.exp() / prior_var # Σ_0 / Σ_1\n",
        "        diff = mu - prior_mean # μ_１ - μ_0\n",
        "        diff_term = diff *diff / prior_var # (μ_1 - μ_0)(μ_1 - μ_0)/Σ_1\n",
        "        logvar_division = prior_logvar - logvar # log|Σ_1| - log|Σ_0| = log(|Σ_1|/|Σ_2|)\n",
        "        # KL\n",
        "        KLD = 0.5 * ((var_division + diff_term + logvar_division).sum(1) - self.latent_size)\n",
        "        self.last_KLD = torch.mean(KLD) #Used for reporting\n",
        "        self.last_BCE = recon_loss\n",
        "        return recon_loss + KLD"
      ],
      "metadata": {
        "id": "jFoNCRlVMNud"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below are the training and test loops\n"
      ],
      "metadata": {
        "id": "EQ-82vOxMqkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = Dir_VAE(input_size, n_numeric, latent_size=2, hidden_dim=20).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data,) in enumerate(train_loader): # Unpack only one element\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar, gauss_z, dir_z = model(data)\n",
        "\n",
        "        loss = model.loss_function(recon_batch, data, mu, logvar, )\n",
        "        loss = loss.mean()\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 10 == 0:\n",
        "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader)}%)] \\\n",
        "            Loss:{loss.item() / len(data)}\\\n",
        "            R_loss {model.last_BCE}, KLD_loss {model.last_KLD}')\n",
        "\n",
        "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
        "          epoch, train_loss / len(train_loader.dataset)))\n",
        "\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for i, (data,) in enumerate(test_loader): # Unpack only one element\n",
        "            data = data.to(device)\n",
        "            recon_batch, mu, logvar, gauss_z, dir_z = model(data)\n",
        "            loss = model.loss_function(recon_batch, data, mu, logvar)\n",
        "            test_loss += loss.mean()\n",
        "            test_loss.item()\n",
        "\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('====> Test set loss: {:.4f}'.format(test_loss))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    for epoch in range(1, 10 + 1):\n",
        "        train(epoch)\n",
        "        test(epoch)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgOw9CXb2p_l",
        "outputId": "f672ae7c-7ad2-4d77-d236-acb2fd4b28b7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/14166 (0.0%)]             Loss:0.259329617023468            R_loss 8.190876007080078, KLD_loss 0.107672318816185\n",
            "Train Epoch: 1 [320/14166 (2.2573363431151243%)]             Loss:0.25180375576019287            R_loss 7.964198112487793, KLD_loss 0.0935220867395401\n",
            "Train Epoch: 1 [640/14166 (4.514672686230249%)]             Loss:0.2416931390762329            R_loss 7.65521764755249, KLD_loss 0.07896298915147781\n",
            "Train Epoch: 1 [960/14166 (6.772009029345372%)]             Loss:0.25052160024642944            R_loss 7.954164981842041, KLD_loss 0.06252578645944595\n",
            "Train Epoch: 1 [1280/14166 (9.029345372460497%)]             Loss:0.2502533793449402            R_loss 7.958432197570801, KLD_loss 0.049675844609737396\n",
            "Train Epoch: 1 [1600/14166 (11.286681715575622%)]             Loss:0.24673624336719513            R_loss 7.8589067459106445, KLD_loss 0.03665335103869438\n",
            "Train Epoch: 1 [1920/14166 (13.544018058690744%)]             Loss:0.24811135232448578            R_loss 7.911916255950928, KLD_loss 0.027647409588098526\n",
            "Train Epoch: 1 [2240/14166 (15.801354401805868%)]             Loss:0.24248658120632172            R_loss 7.740255832672119, KLD_loss 0.019314724951982498\n",
            "Train Epoch: 1 [2560/14166 (18.058690744920995%)]             Loss:0.2433272898197174            R_loss 7.775555610656738, KLD_loss 0.010917864739894867\n",
            "Train Epoch: 1 [2880/14166 (20.31602708803612%)]             Loss:0.23302628099918365            R_loss 7.4512434005737305, KLD_loss 0.0055976323783397675\n",
            "Train Epoch: 1 [3200/14166 (22.573363431151243%)]             Loss:0.23128539323806763            R_loss 7.396831512451172, KLD_loss 0.0043003372848033905\n",
            "Train Epoch: 1 [3520/14166 (24.830699774266364%)]             Loss:0.2320510596036911            R_loss 7.4220662117004395, KLD_loss 0.003568008542060852\n",
            "Train Epoch: 1 [3840/14166 (27.08803611738149%)]             Loss:0.2265799641609192            R_loss 7.247747898101807, KLD_loss 0.0028107017278671265\n",
            "Train Epoch: 1 [4160/14166 (29.345372460496613%)]             Loss:0.22276164591312408            R_loss 7.126242637634277, KLD_loss 0.0021304525434970856\n",
            "Train Epoch: 1 [4480/14166 (31.602708803611737%)]             Loss:0.22717654705047607            R_loss 7.267728805541992, KLD_loss 0.0019207261502742767\n",
            "Train Epoch: 1 [4800/14166 (33.86004514672686%)]             Loss:0.23041439056396484            R_loss 7.371563911437988, KLD_loss 0.0016962960362434387\n",
            "Train Epoch: 1 [5120/14166 (36.11738148984199%)]             Loss:0.24020430445671082            R_loss 7.684123992919922, KLD_loss 0.002413801848888397\n",
            "Train Epoch: 1 [5440/14166 (38.37471783295711%)]             Loss:0.2361275553703308            R_loss 7.55455207824707, KLD_loss 0.0015295036137104034\n",
            "Train Epoch: 1 [5760/14166 (40.63205417607224%)]             Loss:0.22378146648406982            R_loss 7.159816741943359, KLD_loss 0.001189589500427246\n",
            "Train Epoch: 1 [6080/14166 (42.88939051918736%)]             Loss:0.22646775841712952            R_loss 7.24569034576416, KLD_loss 0.0012772493064403534\n",
            "Train Epoch: 1 [6400/14166 (45.146726862302486%)]             Loss:0.23190844058990479            R_loss 7.419487476348877, KLD_loss 0.0015826337039470673\n",
            "Train Epoch: 1 [6720/14166 (47.40406320541761%)]             Loss:0.2385655790567398            R_loss 7.632527828216553, KLD_loss 0.0015705376863479614\n",
            "Train Epoch: 1 [7040/14166 (49.66139954853273%)]             Loss:0.21725741028785706            R_loss 6.951128005981445, KLD_loss 0.0011091232299804688\n",
            "Train Epoch: 1 [7360/14166 (51.918735891647856%)]             Loss:0.21537230908870697            R_loss 6.891032695770264, KLD_loss 0.0008812248706817627\n",
            "Train Epoch: 1 [7680/14166 (54.17607223476298%)]             Loss:0.21797768771648407            R_loss 6.974236011505127, KLD_loss 0.0010496750473976135\n",
            "Train Epoch: 1 [8000/14166 (56.433408577878104%)]             Loss:0.21503987908363342            R_loss 6.880618572235107, KLD_loss 0.0006576701998710632\n",
            "Train Epoch: 1 [8320/14166 (58.690744920993225%)]             Loss:0.2172376960515976            R_loss 6.950922966003418, KLD_loss 0.0006829835474491119\n",
            "Train Epoch: 1 [8640/14166 (60.94808126410835%)]             Loss:0.21348480880260468            R_loss 6.830998420715332, KLD_loss 0.0005154870450496674\n",
            "Train Epoch: 1 [8960/14166 (63.205417607223474%)]             Loss:0.20935095846652985            R_loss 6.69880485534668, KLD_loss 0.0004255026578903198\n",
            "Train Epoch: 1 [9280/14166 (65.4627539503386%)]             Loss:0.2103089541196823            R_loss 6.7294602394104, KLD_loss 0.00042600929737091064\n",
            "Train Epoch: 1 [9600/14166 (67.72009029345372%)]             Loss:0.23934201896190643            R_loss 7.657614707946777, KLD_loss 0.0013298802077770233\n",
            "Train Epoch: 1 [9920/14166 (69.97742663656885%)]             Loss:0.20932435989379883            R_loss 6.6979475021362305, KLD_loss 0.00043195486068725586\n",
            "Train Epoch: 1 [10240/14166 (72.23476297968398%)]             Loss:0.2139752209186554            R_loss 6.8466997146606445, KLD_loss 0.000507548451423645\n",
            "Train Epoch: 1 [10560/14166 (74.49209932279909%)]             Loss:0.22149470448493958            R_loss 7.087058067321777, KLD_loss 0.000772479921579361\n",
            "Train Epoch: 1 [10880/14166 (76.74943566591422%)]             Loss:0.20940017700195312            R_loss 6.700387954711914, KLD_loss 0.0004178658127784729\n",
            "Train Epoch: 1 [11200/14166 (79.00677200902935%)]             Loss:0.20872285962104797            R_loss 6.678722381591797, KLD_loss 0.00040917471051216125\n",
            "Train Epoch: 1 [11520/14166 (81.26410835214448%)]             Loss:0.2146434485912323            R_loss 6.86817741394043, KLD_loss 0.0004131235182285309\n",
            "Train Epoch: 1 [11840/14166 (83.52144469525959%)]             Loss:0.21497714519500732            R_loss 6.87888240814209, KLD_loss 0.0003864653408527374\n",
            "Train Epoch: 1 [12160/14166 (85.77878103837472%)]             Loss:0.21298202872276306            R_loss 6.815014839172363, KLD_loss 0.000409882515668869\n",
            "Train Epoch: 1 [12480/14166 (88.03611738148985%)]             Loss:0.20791897177696228            R_loss 6.652961254119873, KLD_loss 0.000445682555437088\n",
            "Train Epoch: 1 [12800/14166 (90.29345372460497%)]             Loss:0.21737071871757507            R_loss 6.955253601074219, KLD_loss 0.0006090924143791199\n",
            "Train Epoch: 1 [13120/14166 (92.55079006772009%)]             Loss:0.21077148616313934            R_loss 6.744201183319092, KLD_loss 0.0004864931106567383\n",
            "Train Epoch: 1 [13440/14166 (94.80812641083521%)]             Loss:0.2046215981245041            R_loss 6.547713756561279, KLD_loss 0.000177040696144104\n",
            "Train Epoch: 1 [13760/14166 (97.06546275395034%)]             Loss:0.20735201239585876            R_loss 6.63505220413208, KLD_loss 0.00021227821707725525\n",
            "Train Epoch: 1 [14080/14166 (99.32279909706546%)]             Loss:0.22089989483356476            R_loss 7.068500995635986, KLD_loss 0.0002955906093120575\n",
            "====> Epoch: 1 Average loss: 0.2272\n",
            "====> Test set loss: 0.2130\n",
            "Train Epoch: 2 [0/14166 (0.0%)]             Loss:0.2115277796983719            R_loss 6.7685699462890625, KLD_loss 0.0003185346722602844\n",
            "Train Epoch: 2 [320/14166 (2.2573363431151243%)]             Loss:0.21062541007995605            R_loss 6.739760398864746, KLD_loss 0.000252310186624527\n",
            "Train Epoch: 2 [640/14166 (4.514672686230249%)]             Loss:0.2183879315853119            R_loss 6.988089561462402, KLD_loss 0.0003248937427997589\n",
            "Train Epoch: 2 [960/14166 (6.772009029345372%)]             Loss:0.21141420304775238            R_loss 6.765134811401367, KLD_loss 0.00011959299445152283\n",
            "Train Epoch: 2 [1280/14166 (9.029345372460497%)]             Loss:0.20916838943958282            R_loss 6.693272113800049, KLD_loss 0.00011671707034111023\n",
            "Train Epoch: 2 [1600/14166 (11.286681715575622%)]             Loss:0.22603337466716766            R_loss 7.232741355895996, KLD_loss 0.00032706931233406067\n",
            "Train Epoch: 2 [1920/14166 (13.544018058690744%)]             Loss:0.20156988501548767            R_loss 6.450128555297852, KLD_loss 0.00010742247104644775\n",
            "Train Epoch: 2 [2240/14166 (15.801354401805868%)]             Loss:0.21253222227096558            R_loss 6.8008575439453125, KLD_loss 0.00017385557293891907\n",
            "Train Epoch: 2 [2560/14166 (18.058690744920995%)]             Loss:0.2059018462896347            R_loss 6.588756561279297, KLD_loss 0.00010226666927337646\n",
            "Train Epoch: 2 [2880/14166 (20.31602708803612%)]             Loss:0.2010459303855896            R_loss 6.433279991149902, KLD_loss 0.0001903139054775238\n",
            "Train Epoch: 2 [3200/14166 (22.573363431151243%)]             Loss:0.20698902010917664            R_loss 6.623316764831543, KLD_loss 0.0003314986824989319\n",
            "Train Epoch: 2 [3520/14166 (24.830699774266364%)]             Loss:0.2120603322982788            R_loss 6.785360336303711, KLD_loss 0.0005708001554012299\n",
            "Train Epoch: 2 [3840/14166 (27.08803611738149%)]             Loss:0.20274004340171814            R_loss 6.487316131591797, KLD_loss 0.00036549195647239685\n",
            "Train Epoch: 2 [4160/14166 (29.345372460496613%)]             Loss:0.2088678777217865            R_loss 6.683038711547852, KLD_loss 0.0007332153618335724\n",
            "Train Epoch: 2 [4480/14166 (31.602708803611737%)]             Loss:0.2164851725101471            R_loss 6.926477909088135, KLD_loss 0.0010477863252162933\n",
            "Train Epoch: 2 [4800/14166 (33.86004514672686%)]             Loss:0.21628603339195251            R_loss 6.920079231262207, KLD_loss 0.0010734908282756805\n",
            "Train Epoch: 2 [5120/14166 (36.11738148984199%)]             Loss:0.20597662031650543            R_loss 6.590203762054443, KLD_loss 0.001048453152179718\n",
            "Train Epoch: 2 [5440/14166 (38.37471783295711%)]             Loss:0.21105355024337769            R_loss 6.752068519592285, KLD_loss 0.0016450658440589905\n",
            "Train Epoch: 2 [5760/14166 (40.63205417607224%)]             Loss:0.21812964975833893            R_loss 6.978568077087402, KLD_loss 0.0015805736184120178\n",
            "Train Epoch: 2 [6080/14166 (42.88939051918736%)]             Loss:0.21435293555259705            R_loss 6.8576459884643555, KLD_loss 0.0016482025384902954\n",
            "Train Epoch: 2 [6400/14166 (45.146726862302486%)]             Loss:0.19687160849571228            R_loss 6.298810005187988, KLD_loss 0.0010815635323524475\n",
            "Train Epoch: 2 [6720/14166 (47.40406320541761%)]             Loss:0.20048774778842926            R_loss 6.413989067077637, KLD_loss 0.0016186945140361786\n",
            "Train Epoch: 2 [7040/14166 (49.66139954853273%)]             Loss:0.20830722153186798            R_loss 6.662710666656494, KLD_loss 0.0031207874417304993\n",
            "Train Epoch: 2 [7360/14166 (51.918735891647856%)]             Loss:0.21689607203006744            R_loss 6.936580181121826, KLD_loss 0.004094582051038742\n",
            "Train Epoch: 2 [7680/14166 (54.17607223476298%)]             Loss:0.2122599333524704            R_loss 6.788716793060303, KLD_loss 0.0036007389426231384\n",
            "Train Epoch: 2 [8000/14166 (56.433408577878104%)]             Loss:0.20405414700508118            R_loss 6.526960849761963, KLD_loss 0.0027717873454093933\n",
            "Train Epoch: 2 [8320/14166 (58.690744920993225%)]             Loss:0.19730058312416077            R_loss 6.31061315536499, KLD_loss 0.003005053848028183\n",
            "Train Epoch: 2 [8640/14166 (60.94808126410835%)]             Loss:0.20096230506896973            R_loss 6.427002906799316, KLD_loss 0.0037907175719738007\n",
            "Train Epoch: 2 [8960/14166 (63.205417607223474%)]             Loss:0.22182364761829376            R_loss 7.091019630432129, KLD_loss 0.007337335497140884\n",
            "Train Epoch: 2 [9280/14166 (65.4627539503386%)]             Loss:0.21729303896427155            R_loss 6.947444915771484, KLD_loss 0.005932316184043884\n",
            "Train Epoch: 2 [9600/14166 (67.72009029345372%)]             Loss:0.22319044172763824            R_loss 7.1321210861206055, KLD_loss 0.009972933679819107\n",
            "Train Epoch: 2 [9920/14166 (69.97742663656885%)]             Loss:0.21324092149734497            R_loss 6.815608978271484, KLD_loss 0.008100293576717377\n",
            "Train Epoch: 2 [10240/14166 (72.23476297968398%)]             Loss:0.2021571397781372            R_loss 6.462831497192383, KLD_loss 0.0061969757080078125\n",
            "Train Epoch: 2 [10560/14166 (74.49209932279909%)]             Loss:0.21002297103405            R_loss 6.713916778564453, KLD_loss 0.0068178921937942505\n",
            "Train Epoch: 2 [10880/14166 (76.74943566591422%)]             Loss:0.21404574811458588            R_loss 6.842072486877441, KLD_loss 0.0073919035494327545\n",
            "Train Epoch: 2 [11200/14166 (79.00677200902935%)]             Loss:0.19663310050964355            R_loss 6.288332462310791, KLD_loss 0.003926947712898254\n",
            "Train Epoch: 2 [11520/14166 (81.26410835214448%)]             Loss:0.22838233411312103            R_loss 7.29681921005249, KLD_loss 0.011415351182222366\n",
            "Train Epoch: 2 [11840/14166 (83.52144469525959%)]             Loss:0.20227940380573273            R_loss 6.467740535736084, KLD_loss 0.005200188606977463\n",
            "Train Epoch: 2 [12160/14166 (85.77878103837472%)]             Loss:0.2253527045249939            R_loss 7.198915958404541, KLD_loss 0.012370429933071136\n",
            "Train Epoch: 2 [12480/14166 (88.03611738148985%)]             Loss:0.22921010851860046            R_loss 7.319217205047607, KLD_loss 0.015505991876125336\n",
            "Train Epoch: 2 [12800/14166 (90.29345372460497%)]             Loss:0.2069137543439865            R_loss 6.611392498016357, KLD_loss 0.009847857058048248\n",
            "Train Epoch: 2 [13120/14166 (92.55079006772009%)]             Loss:0.22369170188903809            R_loss 7.138492584228516, KLD_loss 0.019641943275928497\n",
            "Train Epoch: 2 [13440/14166 (94.80812641083521%)]             Loss:0.2134155035018921            R_loss 6.814578056335449, KLD_loss 0.014717835932970047\n",
            "Train Epoch: 2 [13760/14166 (97.06546275395034%)]             Loss:0.20609411597251892            R_loss 6.580674171447754, KLD_loss 0.01433742418885231\n",
            "Train Epoch: 2 [14080/14166 (99.32279909706546%)]             Loss:0.20968510210514069            R_loss 6.696124076843262, KLD_loss 0.013799376785755157\n",
            "====> Epoch: 2 Average loss: 0.2120\n",
            "====> Test set loss: 0.2110\n",
            "Train Epoch: 3 [0/14166 (0.0%)]             Loss:0.20942986011505127            R_loss 6.689497470855713, KLD_loss 0.012258395552635193\n",
            "Train Epoch: 3 [320/14166 (2.2573363431151243%)]             Loss:0.20613983273506165            R_loss 6.581442832946777, KLD_loss 0.01503162831068039\n",
            "Train Epoch: 3 [640/14166 (4.514672686230249%)]             Loss:0.2033591866493225            R_loss 6.490622520446777, KLD_loss 0.01687103509902954\n",
            "Train Epoch: 3 [960/14166 (6.772009029345372%)]             Loss:0.21517068147659302            R_loss 6.858258247375488, KLD_loss 0.027203340083360672\n",
            "Train Epoch: 3 [1280/14166 (9.029345372460497%)]             Loss:0.20671910047531128            R_loss 6.593577861785889, KLD_loss 0.02143324539065361\n",
            "Train Epoch: 3 [1600/14166 (11.286681715575622%)]             Loss:0.21039031445980072            R_loss 6.708539962768555, KLD_loss 0.02394985407590866\n",
            "Train Epoch: 3 [1920/14166 (13.544018058690744%)]             Loss:0.20718541741371155            R_loss 6.601700782775879, KLD_loss 0.02823275700211525\n",
            "Train Epoch: 3 [2240/14166 (15.801354401805868%)]             Loss:0.20018264651298523            R_loss 6.383961200714111, KLD_loss 0.021883167326450348\n",
            "Train Epoch: 3 [2560/14166 (18.058690744920995%)]             Loss:0.20631472766399384            R_loss 6.578799247741699, KLD_loss 0.023271657526493073\n",
            "Train Epoch: 3 [2880/14166 (20.31602708803612%)]             Loss:0.21080394089221954            R_loss 6.714438438415527, KLD_loss 0.031287696212530136\n",
            "Train Epoch: 3 [3200/14166 (22.573363431151243%)]             Loss:0.20429982244968414            R_loss 6.507176399230957, KLD_loss 0.030417803674936295\n",
            "Train Epoch: 3 [3520/14166 (24.830699774266364%)]             Loss:0.2007288634777069            R_loss 6.395791530609131, KLD_loss 0.027532249689102173\n",
            "Train Epoch: 3 [3840/14166 (27.08803611738149%)]             Loss:0.21022231876850128            R_loss 6.686352729797363, KLD_loss 0.04076140746474266\n",
            "Train Epoch: 3 [4160/14166 (29.345372460496613%)]             Loss:0.22286218404769897            R_loss 7.063851833343506, KLD_loss 0.06773778796195984\n",
            "Train Epoch: 3 [4480/14166 (31.602708803611737%)]             Loss:0.21148572862148285            R_loss 6.717482566833496, KLD_loss 0.05006048455834389\n",
            "Train Epoch: 3 [4800/14166 (33.86004514672686%)]             Loss:0.2163381725549698            R_loss 6.857553958892822, KLD_loss 0.06526756286621094\n",
            "Train Epoch: 3 [5120/14166 (36.11738148984199%)]             Loss:0.21431908011436462            R_loss 6.801642417907715, KLD_loss 0.05656825006008148\n",
            "Train Epoch: 3 [5440/14166 (38.37471783295711%)]             Loss:0.2085389941930771            R_loss 6.6186323165893555, KLD_loss 0.05461524426937103\n",
            "Train Epoch: 3 [5760/14166 (40.63205417607224%)]             Loss:0.20559556782245636            R_loss 6.525825023651123, KLD_loss 0.05323290079832077\n",
            "Train Epoch: 3 [6080/14166 (42.88939051918736%)]             Loss:0.20352667570114136            R_loss 6.4521565437316895, KLD_loss 0.06069726124405861\n",
            "Train Epoch: 3 [6400/14166 (45.146726862302486%)]             Loss:0.20636822283267975            R_loss 6.52433443069458, KLD_loss 0.07944905757904053\n",
            "Train Epoch: 3 [6720/14166 (47.40406320541761%)]             Loss:0.21893587708473206            R_loss 6.903471946716309, KLD_loss 0.1024755910038948\n",
            "Train Epoch: 3 [7040/14166 (49.66139954853273%)]             Loss:0.20856766402721405            R_loss 6.596965312957764, KLD_loss 0.0772000402212143\n",
            "Train Epoch: 3 [7360/14166 (51.918735891647856%)]             Loss:0.2126777172088623            R_loss 6.6957106590271, KLD_loss 0.10997651517391205\n",
            "Train Epoch: 3 [7680/14166 (54.17607223476298%)]             Loss:0.221806600689888            R_loss 6.967403888702393, KLD_loss 0.13040763139724731\n",
            "Train Epoch: 3 [8000/14166 (56.433408577878104%)]             Loss:0.2075941562652588            R_loss 6.54963493347168, KLD_loss 0.09337781369686127\n",
            "Train Epoch: 3 [8320/14166 (58.690744920993225%)]             Loss:0.20156057178974152            R_loss 6.364965438842773, KLD_loss 0.08497319370508194\n",
            "Train Epoch: 3 [8640/14166 (60.94808126410835%)]             Loss:0.2110292911529541            R_loss 6.6328864097595215, KLD_loss 0.12005063146352768\n",
            "Train Epoch: 3 [8960/14166 (63.205417607223474%)]             Loss:0.22053343057632446            R_loss 6.905473709106445, KLD_loss 0.15159621834754944\n",
            "Train Epoch: 3 [9280/14166 (65.4627539503386%)]             Loss:0.20854315161705017            R_loss 6.563877582550049, KLD_loss 0.10950365662574768\n",
            "Train Epoch: 3 [9600/14166 (67.72009029345372%)]             Loss:0.21694159507751465            R_loss 6.808772563934326, KLD_loss 0.13335874676704407\n",
            "Train Epoch: 3 [9920/14166 (69.97742663656885%)]             Loss:0.20319056510925293            R_loss 6.374222278594971, KLD_loss 0.12787562608718872\n",
            "Train Epoch: 3 [10240/14166 (72.23476297968398%)]             Loss:0.20845668017864227            R_loss 6.540409088134766, KLD_loss 0.13020434975624084\n",
            "Train Epoch: 3 [10560/14166 (74.49209932279909%)]             Loss:0.2281326949596405            R_loss 7.1037774085998535, KLD_loss 0.1964685171842575\n",
            "Train Epoch: 3 [10880/14166 (76.74943566591422%)]             Loss:0.21423374116420746            R_loss 6.651777267456055, KLD_loss 0.20370268821716309\n",
            "Train Epoch: 3 [11200/14166 (79.00677200902935%)]             Loss:0.20729690790176392            R_loss 6.510490894317627, KLD_loss 0.12301043421030045\n",
            "Train Epoch: 3 [11520/14166 (81.26410835214448%)]             Loss:0.2089187204837799            R_loss 6.550011157989502, KLD_loss 0.1353878676891327\n",
            "Train Epoch: 3 [11840/14166 (83.52144469525959%)]             Loss:0.20890185236930847            R_loss 6.5324931144714355, KLD_loss 0.1523660123348236\n",
            "Train Epoch: 3 [12160/14166 (85.77878103837472%)]             Loss:0.2105330377817154            R_loss 6.592787265777588, KLD_loss 0.14426937699317932\n",
            "Train Epoch: 3 [12480/14166 (88.03611738148985%)]             Loss:0.20484638214111328            R_loss 6.434474945068359, KLD_loss 0.12060905247926712\n",
            "Train Epoch: 3 [12800/14166 (90.29345372460497%)]             Loss:0.2006223350763321            R_loss 6.284354209899902, KLD_loss 0.13556042313575745\n",
            "Train Epoch: 3 [13120/14166 (92.55079006772009%)]             Loss:0.2073041796684265            R_loss 6.483716011047363, KLD_loss 0.1500171720981598\n",
            "Train Epoch: 3 [13440/14166 (94.80812641083521%)]             Loss:0.20323708653450012            R_loss 6.376285552978516, KLD_loss 0.12730130553245544\n",
            "Train Epoch: 3 [13760/14166 (97.06546275395034%)]             Loss:0.20574618875980377            R_loss 6.424746036529541, KLD_loss 0.15913192927837372\n",
            "Train Epoch: 3 [14080/14166 (99.32279909706546%)]             Loss:0.20701436698436737            R_loss 6.434189796447754, KLD_loss 0.19026987254619598\n",
            "====> Epoch: 3 Average loss: 0.2101\n",
            "====> Test set loss: 0.2089\n",
            "Train Epoch: 4 [0/14166 (0.0%)]             Loss:0.21365275979042053            R_loss 6.612303256988525, KLD_loss 0.22458478808403015\n",
            "Train Epoch: 4 [320/14166 (2.2573363431151243%)]             Loss:0.21102505922317505            R_loss 6.567024230957031, KLD_loss 0.1857779324054718\n",
            "Train Epoch: 4 [640/14166 (4.514672686230249%)]             Loss:0.20729586482048035            R_loss 6.487579822540283, KLD_loss 0.14588850736618042\n",
            "Train Epoch: 4 [960/14166 (6.772009029345372%)]             Loss:0.1979263722896576            R_loss 6.2045488357543945, KLD_loss 0.12909501791000366\n",
            "Train Epoch: 4 [1280/14166 (9.029345372460497%)]             Loss:0.19761839509010315            R_loss 6.163829326629639, KLD_loss 0.159959077835083\n",
            "Train Epoch: 4 [1600/14166 (11.286681715575622%)]             Loss:0.2132796198129654            R_loss 6.603725910186768, KLD_loss 0.22122225165367126\n",
            "Train Epoch: 4 [1920/14166 (13.544018058690744%)]             Loss:0.2045164406299591            R_loss 6.34517240524292, KLD_loss 0.1993539035320282\n",
            "Train Epoch: 4 [2240/14166 (15.801354401805868%)]             Loss:0.2117658406496048            R_loss 6.527564525604248, KLD_loss 0.24894261360168457\n",
            "Train Epoch: 4 [2560/14166 (18.058690744920995%)]             Loss:0.20087383687496185            R_loss 6.2336931228637695, KLD_loss 0.1942697912454605\n",
            "Train Epoch: 4 [2880/14166 (20.31602708803612%)]             Loss:0.2211594134569168            R_loss 6.814902305603027, KLD_loss 0.26219919323921204\n",
            "Train Epoch: 4 [3200/14166 (22.573363431151243%)]             Loss:0.2110125869512558            R_loss 6.495145797729492, KLD_loss 0.25725695490837097\n",
            "Train Epoch: 4 [3520/14166 (24.830699774266364%)]             Loss:0.20329970121383667            R_loss 6.288293361663818, KLD_loss 0.21729731559753418\n",
            "Train Epoch: 4 [3840/14166 (27.08803611738149%)]             Loss:0.22181104123592377            R_loss 6.84313440322876, KLD_loss 0.254818856716156\n",
            "Train Epoch: 4 [4160/14166 (29.345372460496613%)]             Loss:0.2029285877943039            R_loss 6.255433559417725, KLD_loss 0.23828133940696716\n",
            "Train Epoch: 4 [4480/14166 (31.602708803611737%)]             Loss:0.19913029670715332            R_loss 6.166652202606201, KLD_loss 0.20551717281341553\n",
            "Train Epoch: 4 [4800/14166 (33.86004514672686%)]             Loss:0.20553942024707794            R_loss 6.340522289276123, KLD_loss 0.2367391586303711\n",
            "Train Epoch: 4 [5120/14166 (36.11738148984199%)]             Loss:0.2052261233329773            R_loss 6.37785530090332, KLD_loss 0.1893807351589203\n",
            "Train Epoch: 4 [5440/14166 (38.37471783295711%)]             Loss:0.19778195023536682            R_loss 6.1229681968688965, KLD_loss 0.20605377852916718\n",
            "Train Epoch: 4 [5760/14166 (40.63205417607224%)]             Loss:0.20851169526576996            R_loss 6.447305202484131, KLD_loss 0.2250688076019287\n",
            "Train Epoch: 4 [6080/14166 (42.88939051918736%)]             Loss:0.1956237107515335            R_loss 6.071661949157715, KLD_loss 0.18829643726348877\n",
            "Train Epoch: 4 [6400/14166 (45.146726862302486%)]             Loss:0.20439669489860535            R_loss 6.397943019866943, KLD_loss 0.14275112748146057\n",
            "Train Epoch: 4 [6720/14166 (47.40406320541761%)]             Loss:0.206256702542305            R_loss 6.395136833190918, KLD_loss 0.20507772266864777\n",
            "Train Epoch: 4 [7040/14166 (49.66139954853273%)]             Loss:0.20548100769519806            R_loss 6.300307750701904, KLD_loss 0.27508413791656494\n",
            "Train Epoch: 4 [7360/14166 (51.918735891647856%)]             Loss:0.20833396911621094            R_loss 6.439145088195801, KLD_loss 0.22754204273223877\n",
            "Train Epoch: 4 [7680/14166 (54.17607223476298%)]             Loss:0.21113330125808716            R_loss 6.469842910766602, KLD_loss 0.2864229381084442\n",
            "Train Epoch: 4 [8000/14166 (56.433408577878104%)]             Loss:0.21186678111553192            R_loss 6.48037052154541, KLD_loss 0.29936641454696655\n",
            "Train Epoch: 4 [8320/14166 (58.690744920993225%)]             Loss:0.21704719960689545            R_loss 6.678659915924072, KLD_loss 0.26685065031051636\n",
            "Train Epoch: 4 [8640/14166 (60.94808126410835%)]             Loss:0.19887606799602509            R_loss 6.149372100830078, KLD_loss 0.2146618664264679\n",
            "Train Epoch: 4 [8960/14166 (63.205417607223474%)]             Loss:0.2128092497587204            R_loss 6.583545684814453, KLD_loss 0.22635036706924438\n",
            "Train Epoch: 4 [9280/14166 (65.4627539503386%)]             Loss:0.21119627356529236            R_loss 6.432346343994141, KLD_loss 0.32593366503715515\n",
            "Train Epoch: 4 [9600/14166 (67.72009029345372%)]             Loss:0.20076628029346466            R_loss 6.21557092666626, KLD_loss 0.20894955098628998\n",
            "Train Epoch: 4 [9920/14166 (69.97742663656885%)]             Loss:0.2071666568517685            R_loss 6.4493088722229, KLD_loss 0.18002426624298096\n",
            "Train Epoch: 4 [10240/14166 (72.23476297968398%)]             Loss:0.21701818704605103            R_loss 6.615609169006348, KLD_loss 0.3289721608161926\n",
            "Train Epoch: 4 [10560/14166 (74.49209932279909%)]             Loss:0.21194660663604736            R_loss 6.502846717834473, KLD_loss 0.27944424748420715\n",
            "Train Epoch: 4 [10880/14166 (76.74943566591422%)]             Loss:0.20553627610206604            R_loss 6.404469013214111, KLD_loss 0.17269176244735718\n",
            "Train Epoch: 4 [11200/14166 (79.00677200902935%)]             Loss:0.20601677894592285            R_loss 6.353767395019531, KLD_loss 0.23876941204071045\n",
            "Train Epoch: 4 [11520/14166 (81.26410835214448%)]             Loss:0.21498610079288483            R_loss 6.534174919128418, KLD_loss 0.34537988901138306\n",
            "Train Epoch: 4 [11840/14166 (83.52144469525959%)]             Loss:0.20401623845100403            R_loss 6.309396743774414, KLD_loss 0.2191232442855835\n",
            "Train Epoch: 4 [12160/14166 (85.77878103837472%)]             Loss:0.20828285813331604            R_loss 6.424652576446533, KLD_loss 0.24039918184280396\n",
            "Train Epoch: 4 [12480/14166 (88.03611738148985%)]             Loss:0.219248965382576            R_loss 6.718435287475586, KLD_loss 0.29753145575523376\n",
            "Train Epoch: 4 [12800/14166 (90.29345372460497%)]             Loss:0.20421022176742554            R_loss 6.271624565124512, KLD_loss 0.26310208439826965\n",
            "Train Epoch: 4 [13120/14166 (92.55079006772009%)]             Loss:0.19920939207077026            R_loss 6.152510643005371, KLD_loss 0.22219005227088928\n",
            "Train Epoch: 4 [13440/14166 (94.80812641083521%)]             Loss:0.21423044800758362            R_loss 6.628666400909424, KLD_loss 0.22670777142047882\n",
            "Train Epoch: 4 [13760/14166 (97.06546275395034%)]             Loss:0.20825403928756714            R_loss 6.45478630065918, KLD_loss 0.20934301614761353\n",
            "Train Epoch: 4 [14080/14166 (99.32279909706546%)]             Loss:0.20515458285808563            R_loss 6.325830459594727, KLD_loss 0.23911604285240173\n",
            "====> Epoch: 4 Average loss: 0.2081\n",
            "====> Test set loss: 0.2087\n",
            "Train Epoch: 5 [0/14166 (0.0%)]             Loss:0.20842470228672028            R_loss 6.404853343963623, KLD_loss 0.26473739743232727\n",
            "Train Epoch: 5 [320/14166 (2.2573363431151243%)]             Loss:0.203543022274971            R_loss 6.223997116088867, KLD_loss 0.2893797755241394\n",
            "Train Epoch: 5 [640/14166 (4.514672686230249%)]             Loss:0.20346887409687042            R_loss 6.273133754730225, KLD_loss 0.2378704696893692\n",
            "Train Epoch: 5 [960/14166 (6.772009029345372%)]             Loss:0.2032739520072937            R_loss 6.271148681640625, KLD_loss 0.23361827433109283\n",
            "Train Epoch: 5 [1280/14166 (9.029345372460497%)]             Loss:0.20942333340644836            R_loss 6.392274856567383, KLD_loss 0.30927175283432007\n",
            "Train Epoch: 5 [1600/14166 (11.286681715575622%)]             Loss:0.20156942307949066            R_loss 6.2202467918396, KLD_loss 0.22997497022151947\n",
            "Train Epoch: 5 [1920/14166 (13.544018058690744%)]             Loss:0.20764002203941345            R_loss 6.339017868041992, KLD_loss 0.3054629862308502\n",
            "Train Epoch: 5 [2240/14166 (15.801354401805868%)]             Loss:0.22536678612232208            R_loss 6.915198802947998, KLD_loss 0.29653841257095337\n",
            "Train Epoch: 5 [2560/14166 (18.058690744920995%)]             Loss:0.20874235033988953            R_loss 6.427786827087402, KLD_loss 0.2519676685333252\n",
            "Train Epoch: 5 [2880/14166 (20.31602708803612%)]             Loss:0.1993914097547531            R_loss 6.115827560424805, KLD_loss 0.2646975815296173\n",
            "Train Epoch: 5 [3200/14166 (22.573363431151243%)]             Loss:0.2007284015417099            R_loss 6.230258941650391, KLD_loss 0.19305026531219482\n",
            "Train Epoch: 5 [3520/14166 (24.830699774266364%)]             Loss:0.20151986181735992            R_loss 6.197717666625977, KLD_loss 0.25091803073883057\n",
            "Train Epoch: 5 [3840/14166 (27.08803611738149%)]             Loss:0.2166564017534256            R_loss 6.645637512207031, KLD_loss 0.2873676121234894\n",
            "Train Epoch: 5 [4160/14166 (29.345372460496613%)]             Loss:0.2131493091583252            R_loss 6.540928363800049, KLD_loss 0.27984994649887085\n",
            "Train Epoch: 5 [4480/14166 (31.602708803611737%)]             Loss:0.20688435435295105            R_loss 6.357515811920166, KLD_loss 0.26278334856033325\n",
            "Train Epoch: 5 [4800/14166 (33.86004514672686%)]             Loss:0.20568576455116272            R_loss 6.32470178604126, KLD_loss 0.2572430670261383\n",
            "Train Epoch: 5 [5120/14166 (36.11738148984199%)]             Loss:0.21547091007232666            R_loss 6.617539405822754, KLD_loss 0.2775294780731201\n",
            "Train Epoch: 5 [5440/14166 (38.37471783295711%)]             Loss:0.1981213092803955            R_loss 6.117892265319824, KLD_loss 0.22198927402496338\n",
            "Train Epoch: 5 [5760/14166 (40.63205417607224%)]             Loss:0.22607207298278809            R_loss 6.849946022033691, KLD_loss 0.38436025381088257\n",
            "Train Epoch: 5 [6080/14166 (42.88939051918736%)]             Loss:0.22213365137577057            R_loss 6.71457576751709, KLD_loss 0.3937006890773773\n",
            "Train Epoch: 5 [6400/14166 (45.146726862302486%)]             Loss:0.20876626670360565            R_loss 6.397350788116455, KLD_loss 0.28316959738731384\n",
            "Train Epoch: 5 [6720/14166 (47.40406320541761%)]             Loss:0.215951606631279            R_loss 6.633857727050781, KLD_loss 0.2765932083129883\n",
            "Train Epoch: 5 [7040/14166 (49.66139954853273%)]             Loss:0.2083377242088318            R_loss 6.424854278564453, KLD_loss 0.24195222556591034\n",
            "Train Epoch: 5 [7360/14166 (51.918735891647856%)]             Loss:0.21046927571296692            R_loss 6.474126815795898, KLD_loss 0.2608896493911743\n",
            "Train Epoch: 5 [7680/14166 (54.17607223476298%)]             Loss:0.20240972936153412            R_loss 6.283999919891357, KLD_loss 0.1931115984916687\n",
            "Train Epoch: 5 [8000/14166 (56.433408577878104%)]             Loss:0.20169180631637573            R_loss 6.17841911315918, KLD_loss 0.2757185995578766\n",
            "Train Epoch: 5 [8320/14166 (58.690744920993225%)]             Loss:0.20872342586517334            R_loss 6.40317440032959, KLD_loss 0.27597564458847046\n",
            "Train Epoch: 5 [8640/14166 (60.94808126410835%)]             Loss:0.20366433262825012            R_loss 6.291871547698975, KLD_loss 0.2253870666027069\n",
            "Train Epoch: 5 [8960/14166 (63.205417607223474%)]             Loss:0.2132904976606369            R_loss 6.523200988769531, KLD_loss 0.3020946681499481\n",
            "Train Epoch: 5 [9280/14166 (65.4627539503386%)]             Loss:0.20714953541755676            R_loss 6.362329006195068, KLD_loss 0.2664563059806824\n",
            "Train Epoch: 5 [9600/14166 (67.72009029345372%)]             Loss:0.21140168607234955            R_loss 6.494349956512451, KLD_loss 0.2705039978027344\n",
            "Train Epoch: 5 [9920/14166 (69.97742663656885%)]             Loss:0.2132674753665924            R_loss 6.530951499938965, KLD_loss 0.29360800981521606\n",
            "Train Epoch: 5 [10240/14166 (72.23476297968398%)]             Loss:0.20064572989940643            R_loss 6.203256607055664, KLD_loss 0.2174070030450821\n",
            "Train Epoch: 5 [10560/14166 (74.49209932279909%)]             Loss:0.20322254300117493            R_loss 6.226078510284424, KLD_loss 0.27704310417175293\n",
            "Train Epoch: 5 [10880/14166 (76.74943566591422%)]             Loss:0.2156071662902832            R_loss 6.5927042961120605, KLD_loss 0.3067251443862915\n",
            "Train Epoch: 5 [11200/14166 (79.00677200902935%)]             Loss:0.21254117786884308            R_loss 6.463010311126709, KLD_loss 0.33830755949020386\n",
            "Train Epoch: 5 [11520/14166 (81.26410835214448%)]             Loss:0.2144816368818283            R_loss 6.490646839141846, KLD_loss 0.3727654218673706\n",
            "Train Epoch: 5 [11840/14166 (83.52144469525959%)]             Loss:0.2217070609331131            R_loss 6.705266952514648, KLD_loss 0.38935813307762146\n",
            "Train Epoch: 5 [12160/14166 (85.77878103837472%)]             Loss:0.21576836705207825            R_loss 6.6353349685668945, KLD_loss 0.26925310492515564\n",
            "Train Epoch: 5 [12480/14166 (88.03611738148985%)]             Loss:0.20402869582176208            R_loss 6.363254547119141, KLD_loss 0.16566377878189087\n",
            "Train Epoch: 5 [12800/14166 (90.29345372460497%)]             Loss:0.20479881763458252            R_loss 6.324484348297119, KLD_loss 0.22907719016075134\n",
            "Train Epoch: 5 [13120/14166 (92.55079006772009%)]             Loss:0.20990006625652313            R_loss 6.435824871063232, KLD_loss 0.280976802110672\n",
            "Train Epoch: 5 [13440/14166 (94.80812641083521%)]             Loss:0.20405912399291992            R_loss 6.266228675842285, KLD_loss 0.2636629343032837\n",
            "Train Epoch: 5 [13760/14166 (97.06546275395034%)]             Loss:0.2057805061340332            R_loss 6.3238067626953125, KLD_loss 0.2611692249774933\n",
            "Train Epoch: 5 [14080/14166 (99.32279909706546%)]             Loss:0.20115457475185394            R_loss 6.219815254211426, KLD_loss 0.21713069081306458\n",
            "====> Epoch: 5 Average loss: 0.2081\n",
            "====> Test set loss: 0.2084\n",
            "Train Epoch: 6 [0/14166 (0.0%)]             Loss:0.20091377198696136            R_loss 6.24888277053833, KLD_loss 0.1803579032421112\n",
            "Train Epoch: 6 [320/14166 (2.2573363431151243%)]             Loss:0.20107969641685486            R_loss 6.207789421081543, KLD_loss 0.226761132478714\n",
            "Train Epoch: 6 [640/14166 (4.514672686230249%)]             Loss:0.21543927490711212            R_loss 6.614189147949219, KLD_loss 0.27986782789230347\n",
            "Train Epoch: 6 [960/14166 (6.772009029345372%)]             Loss:0.20640000700950623            R_loss 6.338844299316406, KLD_loss 0.2659565806388855\n",
            "Train Epoch: 6 [1280/14166 (9.029345372460497%)]             Loss:0.1963493674993515            R_loss 6.1293044090271, KLD_loss 0.15387526154518127\n",
            "Train Epoch: 6 [1600/14166 (11.286681715575622%)]             Loss:0.21189871430397034            R_loss 6.501064777374268, KLD_loss 0.2796943187713623\n",
            "Train Epoch: 6 [1920/14166 (13.544018058690744%)]             Loss:0.19844019412994385            R_loss 6.14127779006958, KLD_loss 0.2088080495595932\n",
            "Train Epoch: 6 [2240/14166 (15.801354401805868%)]             Loss:0.20135189592838287            R_loss 6.1431684494018555, KLD_loss 0.3000919818878174\n",
            "Train Epoch: 6 [2560/14166 (18.058690744920995%)]             Loss:0.20148736238479614            R_loss 6.225709438323975, KLD_loss 0.22188615798950195\n",
            "Train Epoch: 6 [2880/14166 (20.31602708803612%)]             Loss:0.20509709417819977            R_loss 6.275033950805664, KLD_loss 0.2880730628967285\n",
            "Train Epoch: 6 [3200/14166 (22.573363431151243%)]             Loss:0.20521870255470276            R_loss 6.304647445678711, KLD_loss 0.26235127449035645\n",
            "Train Epoch: 6 [3520/14166 (24.830699774266364%)]             Loss:0.20514428615570068            R_loss 6.2866668701171875, KLD_loss 0.2779504656791687\n",
            "Train Epoch: 6 [3840/14166 (27.08803611738149%)]             Loss:0.2019767463207245            R_loss 6.239905834197998, KLD_loss 0.2233501523733139\n",
            "Train Epoch: 6 [4160/14166 (29.345372460496613%)]             Loss:0.20738843083381653            R_loss 6.3689799308776855, KLD_loss 0.2674502432346344\n",
            "Train Epoch: 6 [4480/14166 (31.602708803611737%)]             Loss:0.21120066940784454            R_loss 6.46090030670166, KLD_loss 0.29752078652381897\n",
            "Train Epoch: 6 [4800/14166 (33.86004514672686%)]             Loss:0.20439429581165314            R_loss 6.242492198944092, KLD_loss 0.29812508821487427\n",
            "Train Epoch: 6 [5120/14166 (36.11738148984199%)]             Loss:0.21252118051052094            R_loss 6.541727066040039, KLD_loss 0.2589506506919861\n",
            "Train Epoch: 6 [5440/14166 (38.37471783295711%)]             Loss:0.2017570286989212            R_loss 6.2230353355407715, KLD_loss 0.23318934440612793\n",
            "Train Epoch: 6 [5760/14166 (40.63205417607224%)]             Loss:0.2176654189825058            R_loss 6.711489677429199, KLD_loss 0.25380417704582214\n",
            "Train Epoch: 6 [6080/14166 (42.88939051918736%)]             Loss:0.20712614059448242            R_loss 6.389919281005859, KLD_loss 0.2381172776222229\n",
            "Train Epoch: 6 [6400/14166 (45.146726862302486%)]             Loss:0.20932817459106445            R_loss 6.397552013397217, KLD_loss 0.30094972252845764\n",
            "Train Epoch: 6 [6720/14166 (47.40406320541761%)]             Loss:0.20024336874485016            R_loss 6.179678916931152, KLD_loss 0.22810931503772736\n",
            "Train Epoch: 6 [7040/14166 (49.66139954853273%)]             Loss:0.21418212354183197            R_loss 6.551039218902588, KLD_loss 0.3027886748313904\n",
            "Train Epoch: 6 [7360/14166 (51.918735891647856%)]             Loss:0.19633476436138153            R_loss 6.070072174072266, KLD_loss 0.21264028549194336\n",
            "Train Epoch: 6 [7680/14166 (54.17607223476298%)]             Loss:0.21971160173416138            R_loss 6.732340335845947, KLD_loss 0.2984311282634735\n",
            "Train Epoch: 6 [8000/14166 (56.433408577878104%)]             Loss:0.21147505939006805            R_loss 6.462567329406738, KLD_loss 0.3046344518661499\n",
            "Train Epoch: 6 [8320/14166 (58.690744920993225%)]             Loss:0.21917054057121277            R_loss 6.686348915100098, KLD_loss 0.32710859179496765\n",
            "Train Epoch: 6 [8640/14166 (60.94808126410835%)]             Loss:0.20416924357414246            R_loss 6.308961391448975, KLD_loss 0.22445444762706757\n",
            "Train Epoch: 6 [8960/14166 (63.205417607223474%)]             Loss:0.2047221064567566            R_loss 6.343226432800293, KLD_loss 0.20788080990314484\n",
            "Train Epoch: 6 [9280/14166 (65.4627539503386%)]             Loss:0.21499396860599518            R_loss 6.5570526123046875, KLD_loss 0.32275480031967163\n",
            "Train Epoch: 6 [9600/14166 (67.72009029345372%)]             Loss:0.20668479800224304            R_loss 6.387256622314453, KLD_loss 0.22665658593177795\n",
            "Train Epoch: 6 [9920/14166 (69.97742663656885%)]             Loss:0.2108321338891983            R_loss 6.457074165344238, KLD_loss 0.2895543575286865\n",
            "Train Epoch: 6 [10240/14166 (72.23476297968398%)]             Loss:0.20738178491592407            R_loss 6.316744804382324, KLD_loss 0.31947213411331177\n",
            "Train Epoch: 6 [10560/14166 (74.49209932279909%)]             Loss:0.20713716745376587            R_loss 6.411897659301758, KLD_loss 0.21649107336997986\n",
            "Train Epoch: 6 [10880/14166 (76.74943566591422%)]             Loss:0.21416501700878143            R_loss 6.48261833190918, KLD_loss 0.3706621825695038\n",
            "Train Epoch: 6 [11200/14166 (79.00677200902935%)]             Loss:0.19789673388004303            R_loss 6.11778450012207, KLD_loss 0.21491116285324097\n",
            "Train Epoch: 6 [11520/14166 (81.26410835214448%)]             Loss:0.2117440551519394            R_loss 6.451770305633545, KLD_loss 0.32403960824012756\n",
            "Train Epoch: 6 [11840/14166 (83.52144469525959%)]             Loss:0.20179349184036255            R_loss 6.24379301071167, KLD_loss 0.21359890699386597\n",
            "Train Epoch: 6 [12160/14166 (85.77878103837472%)]             Loss:0.20776844024658203            R_loss 6.4075751304626465, KLD_loss 0.24101516604423523\n",
            "Train Epoch: 6 [12480/14166 (88.03611738148985%)]             Loss:0.2035418450832367            R_loss 6.27065372467041, KLD_loss 0.24268481135368347\n",
            "Train Epoch: 6 [12800/14166 (90.29345372460497%)]             Loss:0.20270758867263794            R_loss 6.227565288543701, KLD_loss 0.2590777277946472\n",
            "Train Epoch: 6 [13120/14166 (92.55079006772009%)]             Loss:0.21342390775680542            R_loss 6.542374134063721, KLD_loss 0.28719115257263184\n",
            "Train Epoch: 6 [13440/14166 (94.80812641083521%)]             Loss:0.21201205253601074            R_loss 6.471066474914551, KLD_loss 0.3133193552494049\n",
            "Train Epoch: 6 [13760/14166 (97.06546275395034%)]             Loss:0.21562232077121735            R_loss 6.551426410675049, KLD_loss 0.3484882414340973\n",
            "Train Epoch: 6 [14080/14166 (99.32279909706546%)]             Loss:0.20545968413352966            R_loss 6.305128574371338, KLD_loss 0.26958131790161133\n",
            "====> Epoch: 6 Average loss: 0.2079\n",
            "====> Test set loss: 0.2080\n",
            "Train Epoch: 7 [0/14166 (0.0%)]             Loss:0.20878294110298157            R_loss 6.408641338348389, KLD_loss 0.2724127173423767\n",
            "Train Epoch: 7 [320/14166 (2.2573363431151243%)]             Loss:0.21004563570022583            R_loss 6.474661350250244, KLD_loss 0.2467990517616272\n",
            "Train Epoch: 7 [640/14166 (4.514672686230249%)]             Loss:0.21343345940113068            R_loss 6.485421180725098, KLD_loss 0.3444492518901825\n",
            "Train Epoch: 7 [960/14166 (6.772009029345372%)]             Loss:0.2086845189332962            R_loss 6.405284404754639, KLD_loss 0.27262046933174133\n",
            "Train Epoch: 7 [1280/14166 (9.029345372460497%)]             Loss:0.2100517451763153            R_loss 6.463001251220703, KLD_loss 0.25865522027015686\n",
            "Train Epoch: 7 [1600/14166 (11.286681715575622%)]             Loss:0.2005409598350525            R_loss 6.165322303771973, KLD_loss 0.25198912620544434\n",
            "Train Epoch: 7 [1920/14166 (13.544018058690744%)]             Loss:0.19919513165950775            R_loss 6.168920040130615, KLD_loss 0.20532411336898804\n",
            "Train Epoch: 7 [2240/14166 (15.801354401805868%)]             Loss:0.2234126180410385            R_loss 6.810397624969482, KLD_loss 0.33880630135536194\n",
            "Train Epoch: 7 [2560/14166 (18.058690744920995%)]             Loss:0.20301620662212372            R_loss 6.222498893737793, KLD_loss 0.27401989698410034\n",
            "Train Epoch: 7 [2880/14166 (20.31602708803612%)]             Loss:0.21440079808235168            R_loss 6.583406925201416, KLD_loss 0.2774185240268707\n",
            "Train Epoch: 7 [3200/14166 (22.573363431151243%)]             Loss:0.21014493703842163            R_loss 6.4298505783081055, KLD_loss 0.29478687047958374\n",
            "Train Epoch: 7 [3520/14166 (24.830699774266364%)]             Loss:0.208073690533638            R_loss 6.380867004394531, KLD_loss 0.27749112248420715\n",
            "Train Epoch: 7 [3840/14166 (27.08803611738149%)]             Loss:0.2149340659379959            R_loss 6.578824043273926, KLD_loss 0.2990662455558777\n",
            "Train Epoch: 7 [4160/14166 (29.345372460496613%)]             Loss:0.20539695024490356            R_loss 6.3544230461120605, KLD_loss 0.21827976405620575\n",
            "Train Epoch: 7 [4480/14166 (31.602708803611737%)]             Loss:0.2127617597579956            R_loss 6.519956588745117, KLD_loss 0.2884194850921631\n",
            "Train Epoch: 7 [4800/14166 (33.86004514672686%)]             Loss:0.20839861035346985            R_loss 6.427228927612305, KLD_loss 0.24152688682079315\n",
            "Train Epoch: 7 [5120/14166 (36.11738148984199%)]             Loss:0.20589376986026764            R_loss 6.302603244781494, KLD_loss 0.28599774837493896\n",
            "Train Epoch: 7 [5440/14166 (38.37471783295711%)]             Loss:0.20872022211551666            R_loss 6.342070579528809, KLD_loss 0.3369766175746918\n",
            "Train Epoch: 7 [5760/14166 (40.63205417607224%)]             Loss:0.2043728232383728            R_loss 6.241971015930176, KLD_loss 0.2979598641395569\n",
            "Train Epoch: 7 [6080/14166 (42.88939051918736%)]             Loss:0.2128005027770996            R_loss 6.4591474533081055, KLD_loss 0.35046881437301636\n",
            "Train Epoch: 7 [6400/14166 (45.146726862302486%)]             Loss:0.21011735498905182            R_loss 6.493309020996094, KLD_loss 0.2304462343454361\n",
            "Train Epoch: 7 [6720/14166 (47.40406320541761%)]             Loss:0.21726879477500916            R_loss 6.671688079833984, KLD_loss 0.2809136211872101\n",
            "Train Epoch: 7 [7040/14166 (49.66139954853273%)]             Loss:0.20164062082767487            R_loss 6.226428031921387, KLD_loss 0.22607240080833435\n",
            "Train Epoch: 7 [7360/14166 (51.918735891647856%)]             Loss:0.22019225358963013            R_loss 6.7280192375183105, KLD_loss 0.31813254952430725\n",
            "Train Epoch: 7 [7680/14166 (54.17607223476298%)]             Loss:0.20648357272148132            R_loss 6.382153511047363, KLD_loss 0.2253204882144928\n",
            "Train Epoch: 7 [8000/14166 (56.433408577878104%)]             Loss:0.1963014006614685            R_loss 6.080014228820801, KLD_loss 0.20163045823574066\n",
            "Train Epoch: 7 [8320/14166 (58.690744920993225%)]             Loss:0.2051258236169815            R_loss 6.287290096282959, KLD_loss 0.27673599123954773\n",
            "Train Epoch: 7 [8640/14166 (60.94808126410835%)]             Loss:0.212581068277359            R_loss 6.536326885223389, KLD_loss 0.2662672996520996\n",
            "Train Epoch: 7 [8960/14166 (63.205417607223474%)]             Loss:0.21034479141235352            R_loss 6.44727087020874, KLD_loss 0.2837624251842499\n",
            "Train Epoch: 7 [9280/14166 (65.4627539503386%)]             Loss:0.2091243714094162            R_loss 6.417977809906006, KLD_loss 0.2740016579627991\n",
            "Train Epoch: 7 [9600/14166 (67.72009029345372%)]             Loss:0.20546948909759521            R_loss 6.308744430541992, KLD_loss 0.26627886295318604\n",
            "Train Epoch: 7 [9920/14166 (69.97742663656885%)]             Loss:0.20702092349529266            R_loss 6.3534088134765625, KLD_loss 0.27126023173332214\n",
            "Train Epoch: 7 [10240/14166 (72.23476297968398%)]             Loss:0.2105099856853485            R_loss 6.400007724761963, KLD_loss 0.33631157875061035\n",
            "Train Epoch: 7 [10560/14166 (74.49209932279909%)]             Loss:0.2098180204629898            R_loss 6.421321392059326, KLD_loss 0.29285523295402527\n",
            "Train Epoch: 7 [10880/14166 (76.74943566591422%)]             Loss:0.1957835853099823            R_loss 6.030029773712158, KLD_loss 0.2350456416606903\n",
            "Train Epoch: 7 [11200/14166 (79.00677200902935%)]             Loss:0.20726744830608368            R_loss 6.413952350616455, KLD_loss 0.21860578656196594\n",
            "Train Epoch: 7 [11520/14166 (81.26410835214448%)]             Loss:0.20321445167064667            R_loss 6.27688455581665, KLD_loss 0.22597718238830566\n",
            "Train Epoch: 7 [11840/14166 (83.52144469525959%)]             Loss:0.21036465466022491            R_loss 6.4876933097839355, KLD_loss 0.24397504329681396\n",
            "Train Epoch: 7 [12160/14166 (85.77878103837472%)]             Loss:0.2061365246772766            R_loss 6.341306686401367, KLD_loss 0.25506260991096497\n",
            "Train Epoch: 7 [12480/14166 (88.03611738148985%)]             Loss:0.2060239613056183            R_loss 6.373847961425781, KLD_loss 0.21891866624355316\n",
            "Train Epoch: 7 [12800/14166 (90.29345372460497%)]             Loss:0.196027010679245            R_loss 6.069692611694336, KLD_loss 0.20317140221595764\n",
            "Train Epoch: 7 [13120/14166 (92.55079006772009%)]             Loss:0.20403769612312317            R_loss 6.31150484085083, KLD_loss 0.2177019715309143\n",
            "Train Epoch: 7 [13440/14166 (94.80812641083521%)]             Loss:0.21955259144306183            R_loss 6.730142116546631, KLD_loss 0.29554083943367004\n",
            "Train Epoch: 7 [13760/14166 (97.06546275395034%)]             Loss:0.21558044850826263            R_loss 6.518024444580078, KLD_loss 0.3805503249168396\n",
            "Train Epoch: 7 [14080/14166 (99.32279909706546%)]             Loss:0.21158617734909058            R_loss 6.542506694793701, KLD_loss 0.2282511442899704\n",
            "====> Epoch: 7 Average loss: 0.2080\n",
            "====> Test set loss: 0.2079\n",
            "Train Epoch: 8 [0/14166 (0.0%)]             Loss:0.20038443803787231            R_loss 6.159132480621338, KLD_loss 0.25316983461380005\n",
            "Train Epoch: 8 [320/14166 (2.2573363431151243%)]             Loss:0.19367675483226776            R_loss 5.997378826141357, KLD_loss 0.20027759671211243\n",
            "Train Epoch: 8 [640/14166 (4.514672686230249%)]             Loss:0.21431811153888702            R_loss 6.549342632293701, KLD_loss 0.3088369071483612\n",
            "Train Epoch: 8 [960/14166 (6.772009029345372%)]             Loss:0.204361230134964            R_loss 6.2979326248168945, KLD_loss 0.2416270673274994\n",
            "Train Epoch: 8 [1280/14166 (9.029345372460497%)]             Loss:0.20720121264457703            R_loss 6.435388088226318, KLD_loss 0.19505071640014648\n",
            "Train Epoch: 8 [1600/14166 (11.286681715575622%)]             Loss:0.2047441005706787            R_loss 6.321769714355469, KLD_loss 0.23004114627838135\n",
            "Train Epoch: 8 [1920/14166 (13.544018058690744%)]             Loss:0.1994786560535431            R_loss 6.148543357849121, KLD_loss 0.23477351665496826\n",
            "Train Epoch: 8 [2240/14166 (15.801354401805868%)]             Loss:0.21475568413734436            R_loss 6.57853889465332, KLD_loss 0.29364365339279175\n",
            "Train Epoch: 8 [2560/14166 (18.058690744920995%)]             Loss:0.2171161025762558            R_loss 6.678942680358887, KLD_loss 0.26877233386039734\n",
            "Train Epoch: 8 [2880/14166 (20.31602708803612%)]             Loss:0.19775649905204773            R_loss 6.170424461364746, KLD_loss 0.15778367221355438\n",
            "Train Epoch: 8 [3200/14166 (22.573363431151243%)]             Loss:0.20409269630908966            R_loss 6.314417839050293, KLD_loss 0.21654853224754333\n",
            "Train Epoch: 8 [3520/14166 (24.830699774266364%)]             Loss:0.2085222750902176            R_loss 6.398758411407471, KLD_loss 0.27395421266555786\n",
            "Train Epoch: 8 [3840/14166 (27.08803611738149%)]             Loss:0.22522735595703125            R_loss 6.813962936401367, KLD_loss 0.39331233501434326\n",
            "Train Epoch: 8 [4160/14166 (29.345372460496613%)]             Loss:0.22211067378520966            R_loss 6.743130207061768, KLD_loss 0.36441174149513245\n",
            "Train Epoch: 8 [4480/14166 (31.602708803611737%)]             Loss:0.21685436367988586            R_loss 6.628066539764404, KLD_loss 0.3112735152244568\n",
            "Train Epoch: 8 [4800/14166 (33.86004514672686%)]             Loss:0.2277739942073822            R_loss 6.885635852813721, KLD_loss 0.4031321406364441\n",
            "Train Epoch: 8 [5120/14166 (36.11738148984199%)]             Loss:0.21860343217849731            R_loss 6.664892196655273, KLD_loss 0.3304174542427063\n",
            "Train Epoch: 8 [5440/14166 (38.37471783295711%)]             Loss:0.20642800629138947            R_loss 6.3550615310668945, KLD_loss 0.25063493847846985\n",
            "Train Epoch: 8 [5760/14166 (40.63205417607224%)]             Loss:0.22769737243652344            R_loss 6.934338569641113, KLD_loss 0.35197821259498596\n",
            "Train Epoch: 8 [6080/14166 (42.88939051918736%)]             Loss:0.2075151652097702            R_loss 6.4258294105529785, KLD_loss 0.2146555781364441\n",
            "Train Epoch: 8 [6400/14166 (45.146726862302486%)]             Loss:0.20776085555553436            R_loss 6.380603790283203, KLD_loss 0.26774391531944275\n",
            "Train Epoch: 8 [6720/14166 (47.40406320541761%)]             Loss:0.20436498522758484            R_loss 6.339518070220947, KLD_loss 0.20016123354434967\n",
            "Train Epoch: 8 [7040/14166 (49.66139954853273%)]             Loss:0.2119094431400299            R_loss 6.506172180175781, KLD_loss 0.274930477142334\n",
            "Train Epoch: 8 [7360/14166 (51.918735891647856%)]             Loss:0.2149982452392578            R_loss 6.578269958496094, KLD_loss 0.3016740679740906\n",
            "Train Epoch: 8 [7680/14166 (54.17607223476298%)]             Loss:0.2098047137260437            R_loss 6.483809471130371, KLD_loss 0.22994130849838257\n",
            "Train Epoch: 8 [8000/14166 (56.433408577878104%)]             Loss:0.22012507915496826            R_loss 6.7092509269714355, KLD_loss 0.3347519040107727\n",
            "Train Epoch: 8 [8320/14166 (58.690744920993225%)]             Loss:0.2132943570613861            R_loss 6.512861728668213, KLD_loss 0.312557190656662\n",
            "Train Epoch: 8 [8640/14166 (60.94808126410835%)]             Loss:0.2145015001296997            R_loss 6.591116905212402, KLD_loss 0.27293139696121216\n",
            "Train Epoch: 8 [8960/14166 (63.205417607223474%)]             Loss:0.1985800564289093            R_loss 6.137082099914551, KLD_loss 0.21747948229312897\n",
            "Train Epoch: 8 [9280/14166 (65.4627539503386%)]             Loss:0.21386155486106873            R_loss 6.58824348449707, KLD_loss 0.25532621145248413\n",
            "Train Epoch: 8 [9600/14166 (67.72009029345372%)]             Loss:0.20704463124275208            R_loss 6.417017936706543, KLD_loss 0.20841047167778015\n",
            "Train Epoch: 8 [9920/14166 (69.97742663656885%)]             Loss:0.20682598650455475            R_loss 6.335992813110352, KLD_loss 0.2824389636516571\n",
            "Train Epoch: 8 [10240/14166 (72.23476297968398%)]             Loss:0.20533151924610138            R_loss 6.286267280578613, KLD_loss 0.2843417227268219\n",
            "Train Epoch: 8 [10560/14166 (74.49209932279909%)]             Loss:0.2060127854347229            R_loss 6.342041015625, KLD_loss 0.25036805868148804\n",
            "Train Epoch: 8 [10880/14166 (76.74943566591422%)]             Loss:0.20309142768383026            R_loss 6.241640567779541, KLD_loss 0.25728508830070496\n",
            "Train Epoch: 8 [11200/14166 (79.00677200902935%)]             Loss:0.20670238137245178            R_loss 6.348604679107666, KLD_loss 0.2658713459968567\n",
            "Train Epoch: 8 [11520/14166 (81.26410835214448%)]             Loss:0.20364147424697876            R_loss 6.270339488983154, KLD_loss 0.24618765711784363\n",
            "Train Epoch: 8 [11840/14166 (83.52144469525959%)]             Loss:0.20590868592262268            R_loss 6.371823787689209, KLD_loss 0.2172538787126541\n",
            "Train Epoch: 8 [12160/14166 (85.77878103837472%)]             Loss:0.20110958814620972            R_loss 6.214725971221924, KLD_loss 0.22078105807304382\n",
            "Train Epoch: 8 [12480/14166 (88.03611738148985%)]             Loss:0.2082875818014145            R_loss 6.385875225067139, KLD_loss 0.2793271839618683\n",
            "Train Epoch: 8 [12800/14166 (90.29345372460497%)]             Loss:0.20757713913917542            R_loss 6.413120746612549, KLD_loss 0.22934795916080475\n",
            "Train Epoch: 8 [13120/14166 (92.55079006772009%)]             Loss:0.2019868791103363            R_loss 6.218425750732422, KLD_loss 0.24515412747859955\n",
            "Train Epoch: 8 [13440/14166 (94.80812641083521%)]             Loss:0.20637337863445282            R_loss 6.312729358673096, KLD_loss 0.29121896624565125\n",
            "Train Epoch: 8 [13760/14166 (97.06546275395034%)]             Loss:0.21371766924858093            R_loss 6.531435012817383, KLD_loss 0.3075300455093384\n",
            "Train Epoch: 8 [14080/14166 (99.32279909706546%)]             Loss:0.20620957016944885            R_loss 6.330838680267334, KLD_loss 0.2678672969341278\n",
            "====> Epoch: 8 Average loss: 0.2083\n",
            "====> Test set loss: 0.2080\n",
            "Train Epoch: 9 [0/14166 (0.0%)]             Loss:0.20540310442447662            R_loss 6.308034896850586, KLD_loss 0.26486486196517944\n",
            "Train Epoch: 9 [320/14166 (2.2573363431151243%)]             Loss:0.20531219244003296            R_loss 6.33901309967041, KLD_loss 0.23097696900367737\n",
            "Train Epoch: 9 [640/14166 (4.514672686230249%)]             Loss:0.20306342840194702            R_loss 6.214052200317383, KLD_loss 0.283977746963501\n",
            "Train Epoch: 9 [960/14166 (6.772009029345372%)]             Loss:0.2065042406320572            R_loss 6.350261211395264, KLD_loss 0.25787514448165894\n",
            "Train Epoch: 9 [1280/14166 (9.029345372460497%)]             Loss:0.21332579851150513            R_loss 6.485593795776367, KLD_loss 0.34083181619644165\n",
            "Train Epoch: 9 [1600/14166 (11.286681715575622%)]             Loss:0.20121140778064728            R_loss 6.159371852874756, KLD_loss 0.279392808675766\n",
            "Train Epoch: 9 [1920/14166 (13.544018058690744%)]             Loss:0.20891238749027252            R_loss 6.421454906463623, KLD_loss 0.26374107599258423\n",
            "Train Epoch: 9 [2240/14166 (15.801354401805868%)]             Loss:0.20171815156936646            R_loss 6.299359321594238, KLD_loss 0.155621737241745\n",
            "Train Epoch: 9 [2560/14166 (18.058690744920995%)]             Loss:0.19846129417419434            R_loss 6.171561241149902, KLD_loss 0.17920026183128357\n",
            "Train Epoch: 9 [2880/14166 (20.31602708803612%)]             Loss:0.21298040449619293            R_loss 6.5901994705200195, KLD_loss 0.22517327964305878\n",
            "Train Epoch: 9 [3200/14166 (22.573363431151243%)]             Loss:0.2118958830833435            R_loss 6.559062957763672, KLD_loss 0.22160604596138\n",
            "Train Epoch: 9 [3520/14166 (24.830699774266364%)]             Loss:0.22896921634674072            R_loss 7.002906799316406, KLD_loss 0.3241082429885864\n",
            "Train Epoch: 9 [3840/14166 (27.08803611738149%)]             Loss:0.19901403784751892            R_loss 6.192721843719482, KLD_loss 0.17572703957557678\n",
            "Train Epoch: 9 [4160/14166 (29.345372460496613%)]             Loss:0.20876967906951904            R_loss 6.372935771942139, KLD_loss 0.3076939880847931\n",
            "Train Epoch: 9 [4480/14166 (31.602708803611737%)]             Loss:0.2066948413848877            R_loss 6.3428473472595215, KLD_loss 0.27138805389404297\n",
            "Train Epoch: 9 [4800/14166 (33.86004514672686%)]             Loss:0.22118434309959412            R_loss 6.759687900543213, KLD_loss 0.3182116150856018\n",
            "Train Epoch: 9 [5120/14166 (36.11738148984199%)]             Loss:0.1999424397945404            R_loss 6.226640224456787, KLD_loss 0.17151787877082825\n",
            "Train Epoch: 9 [5440/14166 (38.37471783295711%)]             Loss:0.2077581137418747            R_loss 6.3870086669921875, KLD_loss 0.261251300573349\n",
            "Train Epoch: 9 [5760/14166 (40.63205417607224%)]             Loss:0.2059253752231598            R_loss 6.309126377105713, KLD_loss 0.2804858386516571\n",
            "Train Epoch: 9 [6080/14166 (42.88939051918736%)]             Loss:0.20452189445495605            R_loss 6.2794270515441895, KLD_loss 0.2652738094329834\n",
            "Train Epoch: 9 [6400/14166 (45.146726862302486%)]             Loss:0.20564286410808563            R_loss 6.2939019203186035, KLD_loss 0.2866692543029785\n",
            "Train Epoch: 9 [6720/14166 (47.40406320541761%)]             Loss:0.21418744325637817            R_loss 6.524673938751221, KLD_loss 0.3293248414993286\n",
            "Train Epoch: 9 [7040/14166 (49.66139954853273%)]             Loss:0.19789338111877441            R_loss 6.088318824768066, KLD_loss 0.2442694902420044\n",
            "Train Epoch: 9 [7360/14166 (51.918735891647856%)]             Loss:0.20075415074825287            R_loss 6.152761459350586, KLD_loss 0.271371990442276\n",
            "Train Epoch: 9 [7680/14166 (54.17607223476298%)]             Loss:0.2023731768131256            R_loss 6.187897205352783, KLD_loss 0.28804469108581543\n",
            "Train Epoch: 9 [8000/14166 (56.433408577878104%)]             Loss:0.20342305302619934            R_loss 6.263896942138672, KLD_loss 0.2456410527229309\n",
            "Train Epoch: 9 [8320/14166 (58.690744920993225%)]             Loss:0.2064513862133026            R_loss 6.368649482727051, KLD_loss 0.23779471218585968\n",
            "Train Epoch: 9 [8640/14166 (60.94808126410835%)]             Loss:0.2013128101825714            R_loss 6.198801040649414, KLD_loss 0.24320852756500244\n",
            "Train Epoch: 9 [8960/14166 (63.205417607223474%)]             Loss:0.21305730938911438            R_loss 6.492260932922363, KLD_loss 0.3255734145641327\n",
            "Train Epoch: 9 [9280/14166 (65.4627539503386%)]             Loss:0.2153274118900299            R_loss 6.618734359741211, KLD_loss 0.2717427611351013\n",
            "Train Epoch: 9 [9600/14166 (67.72009029345372%)]             Loss:0.20380112528800964            R_loss 6.257262706756592, KLD_loss 0.2643735110759735\n",
            "Train Epoch: 9 [9920/14166 (69.97742663656885%)]             Loss:0.20590132474899292            R_loss 6.3260674476623535, KLD_loss 0.262775182723999\n",
            "Train Epoch: 9 [10240/14166 (72.23476297968398%)]             Loss:0.21819905936717987            R_loss 6.686328887939453, KLD_loss 0.29604125022888184\n",
            "Train Epoch: 9 [10560/14166 (74.49209932279909%)]             Loss:0.21351207792758942            R_loss 6.493297576904297, KLD_loss 0.3390892744064331\n",
            "Train Epoch: 9 [10880/14166 (76.74943566591422%)]             Loss:0.21328777074813843            R_loss 6.543640613555908, KLD_loss 0.28156793117523193\n",
            "Train Epoch: 9 [11200/14166 (79.00677200902935%)]             Loss:0.2108633816242218            R_loss 6.511580467224121, KLD_loss 0.23604831099510193\n",
            "Train Epoch: 9 [11520/14166 (81.26410835214448%)]             Loss:0.1939309537410736            R_loss 5.994629859924316, KLD_loss 0.211160346865654\n",
            "Train Epoch: 9 [11840/14166 (83.52144469525959%)]             Loss:0.2023099809885025            R_loss 6.2992024421691895, KLD_loss 0.1747170090675354\n",
            "Train Epoch: 9 [12160/14166 (85.77878103837472%)]             Loss:0.219366654753685            R_loss 6.745429039001465, KLD_loss 0.2743039131164551\n",
            "Train Epoch: 9 [12480/14166 (88.03611738148985%)]             Loss:0.20957517623901367            R_loss 6.435654163360596, KLD_loss 0.27075183391571045\n",
            "Train Epoch: 9 [12800/14166 (90.29345372460497%)]             Loss:0.21176151931285858            R_loss 6.472323417663574, KLD_loss 0.3040452003479004\n",
            "Train Epoch: 9 [13120/14166 (92.55079006772009%)]             Loss:0.2098737359046936            R_loss 6.481921195983887, KLD_loss 0.2340385913848877\n",
            "Train Epoch: 9 [13440/14166 (94.80812641083521%)]             Loss:0.22647793591022491            R_loss 6.892421245574951, KLD_loss 0.3548724353313446\n",
            "Train Epoch: 9 [13760/14166 (97.06546275395034%)]             Loss:0.19999027252197266            R_loss 6.111026763916016, KLD_loss 0.288661390542984\n",
            "Train Epoch: 9 [14080/14166 (99.32279909706546%)]             Loss:0.22260399162769318            R_loss 6.815161228179932, KLD_loss 0.30816650390625\n",
            "====> Epoch: 9 Average loss: 0.2082\n",
            "====> Test set loss: 0.2079\n",
            "Train Epoch: 10 [0/14166 (0.0%)]             Loss:0.20641712844371796            R_loss 6.353946685791016, KLD_loss 0.25140178203582764\n",
            "Train Epoch: 10 [320/14166 (2.2573363431151243%)]             Loss:0.20352350175380707            R_loss 6.263788223266602, KLD_loss 0.24896344542503357\n",
            "Train Epoch: 10 [640/14166 (4.514672686230249%)]             Loss:0.20420044660568237            R_loss 6.293269157409668, KLD_loss 0.24114573001861572\n",
            "Train Epoch: 10 [960/14166 (6.772009029345372%)]             Loss:0.20495563745498657            R_loss 6.26930570602417, KLD_loss 0.2892749011516571\n",
            "Train Epoch: 10 [1280/14166 (9.029345372460497%)]             Loss:0.19719113409519196            R_loss 6.103307723999023, KLD_loss 0.2068084329366684\n",
            "Train Epoch: 10 [1600/14166 (11.286681715575622%)]             Loss:0.20517468452453613            R_loss 6.340751647949219, KLD_loss 0.22483763098716736\n",
            "Train Epoch: 10 [1920/14166 (13.544018058690744%)]             Loss:0.2056853324174881            R_loss 6.357217788696289, KLD_loss 0.22471323609352112\n",
            "Train Epoch: 10 [2240/14166 (15.801354401805868%)]             Loss:0.2058836966753006            R_loss 6.300998687744141, KLD_loss 0.28727930784225464\n",
            "Train Epoch: 10 [2560/14166 (18.058690744920995%)]             Loss:0.2027706503868103            R_loss 6.18564510345459, KLD_loss 0.3030158579349518\n",
            "Train Epoch: 10 [2880/14166 (20.31602708803612%)]             Loss:0.20986615121364594            R_loss 6.42108154296875, KLD_loss 0.29463493824005127\n",
            "Train Epoch: 10 [3200/14166 (22.573363431151243%)]             Loss:0.21036741137504578            R_loss 6.476382255554199, KLD_loss 0.2553756535053253\n",
            "Train Epoch: 10 [3520/14166 (24.830699774266364%)]             Loss:0.21075835824012756            R_loss 6.525471210479736, KLD_loss 0.2187959849834442\n",
            "Train Epoch: 10 [3840/14166 (27.08803611738149%)]             Loss:0.217478945851326            R_loss 6.704526424407959, KLD_loss 0.2547997832298279\n",
            "Train Epoch: 10 [4160/14166 (29.345372460496613%)]             Loss:0.1974506974220276            R_loss 6.159949779510498, KLD_loss 0.15847212076187134\n",
            "Train Epoch: 10 [4480/14166 (31.602708803611737%)]             Loss:0.207831010222435            R_loss 6.42500638961792, KLD_loss 0.22558553516864777\n",
            "Train Epoch: 10 [4800/14166 (33.86004514672686%)]             Loss:0.22241094708442688            R_loss 6.770873069763184, KLD_loss 0.346277117729187\n",
            "Train Epoch: 10 [5120/14166 (36.11738148984199%)]             Loss:0.20738372206687927            R_loss 6.376006603240967, KLD_loss 0.26027244329452515\n",
            "Train Epoch: 10 [5440/14166 (38.37471783295711%)]             Loss:0.20586255192756653            R_loss 6.27884578704834, KLD_loss 0.30875593423843384\n",
            "Train Epoch: 10 [5760/14166 (40.63205417607224%)]             Loss:0.1986006498336792            R_loss 6.153665542602539, KLD_loss 0.2015552520751953\n",
            "Train Epoch: 10 [6080/14166 (42.88939051918736%)]             Loss:0.2054682821035385            R_loss 6.278648853302002, KLD_loss 0.29633620381355286\n",
            "Train Epoch: 10 [6400/14166 (45.146726862302486%)]             Loss:0.20817039906978607            R_loss 6.3878173828125, KLD_loss 0.2736351788043976\n",
            "Train Epoch: 10 [6720/14166 (47.40406320541761%)]             Loss:0.2118573784828186            R_loss 6.537857532501221, KLD_loss 0.24157890677452087\n",
            "Train Epoch: 10 [7040/14166 (49.66139954853273%)]             Loss:0.2112749218940735            R_loss 6.550665855407715, KLD_loss 0.21013151109218597\n",
            "Train Epoch: 10 [7360/14166 (51.918735891647856%)]             Loss:0.19801019132137299            R_loss 6.112000942230225, KLD_loss 0.2243252694606781\n",
            "Train Epoch: 10 [7680/14166 (54.17607223476298%)]             Loss:0.2114235758781433            R_loss 6.496159553527832, KLD_loss 0.26939499378204346\n",
            "Train Epoch: 10 [8000/14166 (56.433408577878104%)]             Loss:0.20679859817028046            R_loss 6.379049301147461, KLD_loss 0.23850569128990173\n",
            "Train Epoch: 10 [8320/14166 (58.690744920993225%)]             Loss:0.20825789868831635            R_loss 6.383049964904785, KLD_loss 0.2812025845050812\n",
            "Train Epoch: 10 [8640/14166 (60.94808126410835%)]             Loss:0.20199307799339294            R_loss 6.2202863693237305, KLD_loss 0.24349217116832733\n",
            "Train Epoch: 10 [8960/14166 (63.205417607223474%)]             Loss:0.21090593934059143            R_loss 6.490853309631348, KLD_loss 0.25813695788383484\n",
            "Train Epoch: 10 [9280/14166 (65.4627539503386%)]             Loss:0.21433047950267792            R_loss 6.5344696044921875, KLD_loss 0.3241058588027954\n",
            "Train Epoch: 10 [9600/14166 (67.72009029345372%)]             Loss:0.22913342714309692            R_loss 7.024901390075684, KLD_loss 0.3073682487010956\n",
            "Train Epoch: 10 [9920/14166 (69.97742663656885%)]             Loss:0.20161113142967224            R_loss 6.230134963989258, KLD_loss 0.22142082452774048\n",
            "Train Epoch: 10 [10240/14166 (72.23476297968398%)]             Loss:0.20633462071418762            R_loss 6.342008590698242, KLD_loss 0.2606995701789856\n",
            "Train Epoch: 10 [10560/14166 (74.49209932279909%)]             Loss:0.2035120129585266            R_loss 6.299735069274902, KLD_loss 0.21264968812465668\n",
            "Train Epoch: 10 [10880/14166 (76.74943566591422%)]             Loss:0.20600517094135284            R_loss 6.334311008453369, KLD_loss 0.2578546404838562\n",
            "Train Epoch: 10 [11200/14166 (79.00677200902935%)]             Loss:0.21070128679275513            R_loss 6.443732261657715, KLD_loss 0.2987087368965149\n",
            "Train Epoch: 10 [11520/14166 (81.26410835214448%)]             Loss:0.21570135653018951            R_loss 6.548330783843994, KLD_loss 0.35411229729652405\n",
            "Train Epoch: 10 [11840/14166 (83.52144469525959%)]             Loss:0.19922176003456116            R_loss 6.163417816162109, KLD_loss 0.21167857944965363\n",
            "Train Epoch: 10 [12160/14166 (85.77878103837472%)]             Loss:0.20496875047683716            R_loss 6.328019618988037, KLD_loss 0.23098085820674896\n",
            "Train Epoch: 10 [12480/14166 (88.03611738148985%)]             Loss:0.21801207959651947            R_loss 6.66031551361084, KLD_loss 0.31607070565223694\n",
            "Train Epoch: 10 [12800/14166 (90.29345372460497%)]             Loss:0.1992257982492447            R_loss 6.142161846160889, KLD_loss 0.23306438326835632\n",
            "Train Epoch: 10 [13120/14166 (92.55079006772009%)]             Loss:0.20309683680534363            R_loss 6.190435886383057, KLD_loss 0.30866286158561707\n",
            "Train Epoch: 10 [13440/14166 (94.80812641083521%)]             Loss:0.20263439416885376            R_loss 6.2365617752075195, KLD_loss 0.24773912131786346\n",
            "Train Epoch: 10 [13760/14166 (97.06546275395034%)]             Loss:0.20631131529808044            R_loss 6.298534393310547, KLD_loss 0.3034273087978363\n",
            "Train Epoch: 10 [14080/14166 (99.32279909706546%)]             Loss:0.21897512674331665            R_loss 6.718365669250488, KLD_loss 0.28883856534957886\n",
            "====> Epoch: 10 Average loss: 0.2078\n",
            "====> Test set loss: 0.2086\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  print(\"testing encoding\")\n",
        "  data = next(iter(test_loader))\n",
        "  print(data[0][0])\n",
        "  recon_batch, mu, logvar, gauss_z, dir_z = model(data[0])\n",
        "  print(recon_batch[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57g2SgMSQjPY",
        "outputId": "63a1dd25-754a-4a25-c12a-68656fbf5524"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "testing encoding\n",
            "tensor([0.6984, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000])\n",
            "tensor([7.0599e-01, 1.1541e-04, 1.5541e-04, 1.0534e-04, 1.4746e-04, 1.0779e-04,\n",
            "        9.4734e-05, 9.8256e-05, 5.2996e-01, 1.8855e-04, 8.8673e-05, 1.2442e-04,\n",
            "        1.6131e-04, 1.0111e-04, 1.5494e-04, 1.8781e-04, 1.5484e-04, 1.7324e-04,\n",
            "        1.4581e-04, 2.0903e-04, 1.5217e-04, 1.3672e-04, 1.3104e-04, 9.8247e-04,\n",
            "        1.4903e-04, 8.4857e-05, 1.5018e-04, 1.4926e-04, 4.4486e-01, 9.9989e-01,\n",
            "        1.3694e-04, 1.0310e-04, 5.1154e-04, 1.4122e-04, 9.9989e-01, 9.9989e-01])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L3NKOc3mHlIP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}